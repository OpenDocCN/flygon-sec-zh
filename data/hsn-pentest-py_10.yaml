- en: Building a Custom Crawler
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建自定义爬虫
- en: 'When we talk of web application scanning, we often come across crawlers that
    are built into the automatic scanning tools we use for web application scanning.
    Tools such as Burp Suite, Acunetix, web inspect, and so on all have wonderful
    crawlers that crawl through web applications and try various attack vectors against
    the crawled URLs. In this chapter, we are going to understand how a crawler works
    and what happens under the hood. The objective of this chapter is to enable the
    user to understand how a crawler collects all the information and forms the attack
    surface for various attacks. The same knowledge can be later used to develop a
    custom tool that may automate web application scanning. In this chapter, we are
    going to create a custom web crawler that will crawl through a website and give
    us a list that contains the following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论Web应用程序扫描时，我们经常会遇到内置在我们用于Web应用程序扫描的自动扫描工具中的爬虫。诸如Burp Suite、Acunetix、Web
    Inspect等工具都有精彩的爬虫，可以浏览Web应用程序并针对爬取的URL尝试各种攻击向量。在本章中，我们将了解爬虫是如何工作的，以及在幕后发生了什么。本章的目标是使用户了解爬虫如何收集所有信息并形成各种攻击的攻击面。相同的知识可以稍后用于开发可能自动化Web应用程序扫描的自定义工具。在本章中，我们将创建一个自定义Web爬虫，它将浏览网站并给出一个包含以下内容的列表：
- en: Web pages
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网页
- en: HTML forms
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HTML表单
- en: All input fields within each form
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个表单中的所有输入字段
- en: 'We will see how we can crawl a web application in two modes:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到如何以两种模式爬取Web应用程序：
- en: Without authentication
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无身份验证
- en: With authentication
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有身份验证
- en: We will have a small GUI developed in the Django (a web application framework
    for Python) that will enable the users to conduct crawling on the test applications.
    It must be noted that the main focus of the chapter is on the workings of the
    crawler, and so we will discuss the crawler code in detail. We will not be focusing
    on the workings of Django web applications. For this, there will be reference
    links provided at the end of the chapter. I will be sharing the whole code base
    in my GitHub repository for readers to download and execute in order to get a
    better understanding of the application.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在Django（Python的Web应用程序框架）中开发一个小型GUI，使用户能够在测试应用程序上进行爬取。必须注意，本章的主要重点是爬虫的工作原理，因此我们将详细讨论爬虫代码。我们不会专注于Django
    Web应用程序的工作原理。为此，本章末尾将提供参考链接。我将在我的GitHub存储库中分享整个代码库，供读者下载和执行，以便更好地理解该应用程序。
- en: Setup and installations
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置和安装
- en: The operating system to be used is Ubuntu 16.04\. The code is tested on this
    version, but readers are free to use any other version.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用的操作系统是Ubuntu 16.04。该代码在此版本上经过测试，但读者可以自由使用任何其他版本。
- en: 'Install the prerequisites required for this chapter by running the following
    commands:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下命令安装本章所需的先决条件：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: It should be noted that the code is tried and tested on Python 2.7\. It is recommended
    for the readers to try the code on the same version of Python, but it should work
    with Python 3 as well. There might be a few syntactic changes with regard to print
    statements.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 应注意，该代码经过Python 2.7的尝试和测试。建议读者在相同版本的Python上尝试该代码，但它也应该适用于Python 3。关于打印语句可能会有一些语法上的变化。
- en: Getting started
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始
- en: A typical Django project follows an MVC-based architecture. The user requests
    first hit the URLs configured in the `Urls.py` file, and from there it is forwarded
    to the appropriate view. The view acts as middleware between the backend core
    logic and the template/HTML that is rendered to user. `views.py` has various methods,
    each of which corresponds to the URL mapper in the `Urls.py` file. On receiving
    the request, the logic written in the `views` class or method prepares the data
    from `models.py` and other core business modules. Once all the data is prepared,
    it is rendered back to the user with the help of templates. Thus, the templates
    form the UI layer of the web project.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的Django项目遵循基于MVC的架构。用户请求首先命中`Urls.py`文件中配置的URL，然后转发到适当的视图。视图充当后端核心逻辑和呈现给用户的模板/HTML之间的中间件。`views.py`有各种方法，每个方法对应于`Urls.py`文件中的URL映射器。在接收请求时，`views`类或方法中编写的逻辑从`models.py`和其他核心业务模块中准备数据。一旦所有数据准备好，它就会通过模板呈现给用户。因此，模板形成了Web项目的UI层。
- en: 'The following diagram represents the Django request-response cycle:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表代表了Django请求-响应循环：
- en: '![](img/d7a2a172-5d51-47c9-ba36-3e0aa1ffbc65.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7a2a172-5d51-47c9-ba36-3e0aa1ffbc65.png)'
- en: Crawler code
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 爬虫代码
- en: As mentioned earlier, we have a user interface that will collect the user parameters
    for the web application that is to be crawled. Thus, the request is forwarded
    to the `views.py` file and from there we will invoke the crawler driver file, `run_crawler.py`,
    which in turn will call `crawler.py`. The `new_scan` view method takes all the
    user parameters, saves them in a database, and assigns a new project ID to the
    crawl project. It then passes on the project ID to the crawler driver, for it
    to reference and pull the relevant project parameters with the help of the ID
    and then pass them on to `crawler.py` to start the scanning.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们有一个用户界面，将收集要爬取的Web应用程序的用户参数。因此，请求被转发到`views.py`文件，然后我们将调用爬虫驱动文件`run_crawler.py`，然后再调用`crawler.py`。`new_scan`视图方法获取所有用户参数，将它们保存在数据库中，并为爬取项目分配一个新的项目ID。然后将项目ID传递给爬虫驱动程序，以便引用并使用ID提取相关项目参数，然后将它们传递给`crawler.py`开始扫描。
- en: Urls.py and Views.py code snippet
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Urls.py和Views.py代码片段
- en: 'The following is the configuration of the `Urls.py` file, which has the mapping
    between the HTTP URL and the `views.py` method mapped to that URL. The path of
    this file is `Chapter8/Xtreme_InjectCrawler/XtremeWebAPP/Urls.py`:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是`Urls.py`文件的配置，其中包含HTTP URL和映射到该URL的`views.py`方法之间的映射关系。该文件的路径是`Chapter8/Xtreme_InjectCrawler/XtremeWebAPP/Urls.py`：
- en: '![](img/af450762-e9b2-4934-8aac-19e965629f14.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/af450762-e9b2-4934-8aac-19e965629f14.png)'
- en: 'The preceding highlighted line represents the mapping between the URL for the
    new crawl project and the `views` method that caters to the request. Thus, we
    will have a method called `new_scan` inside the `views.py` file. The path of the
    file is `Chapter8/Xtreme_InjectCrawler/XtremeWebAPP/xtreme_server/views.py`. The
    method definition is shown here:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 前面突出显示的行表示新爬行项目的URL与满足请求的`views`方法之间的映射。因此，我们将在`views.py`文件中有一个名为`new_scan`的方法。文件的路径是`Chapter8/Xtreme_InjectCrawler/XtremeWebAPP/xtreme_server/views.py`。方法定义如下：
- en: '![](img/825f44cd-a637-4f6f-98e2-ce6e871a44b2.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/825f44cd-a637-4f6f-98e2-ce6e871a44b2.png)'
- en: '![](img/d4b60bba-4872-40cf-b9d2-a2e654f8624e.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d4b60bba-4872-40cf-b9d2-a2e654f8624e.png)'
- en: '![](img/1aa217d1-0194-4339-84c8-a4e3bde1596c.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1aa217d1-0194-4339-84c8-a4e3bde1596c.png)'
- en: Code explanation
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码解释
- en: The `new_scan` method will receive both `HTTP GET` and `POST` requests from
    the user. The `GET` request will be resolved to serve the page where the user
    can enter the project parameters and the `POST` request will post all the parameters
    to the previous code, which can then be further processed. As highlighted by section
    **(1)** of the code, the project parameters are being retrieved from the user
    request and are placed in Python program variables. Section (2) of the code does
    the same. It also takes a few other parameters from the settings provided by the
    user and places them in a Python dictionary called settings. Finally, when all
    the data is collected, it saves all the details in the backend database table
    called `Project`. As can be seen in line 261, the code initializes a class called `Project()`,
    and then from lines 262 to 279, it assigns the parameters obtained from the user
    to the instance variables of the `Project()` class. Finally, at line 280, the
    `project.save()` code is invoked. This places all the instance variables into
    a database table as a single row.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`new_scan`方法将接收用户的`HTTP GET`和`POST`请求。`GET`请求将被解析为提供用户输入项目参数的页面，`POST`请求将把所有参数发布到先前的代码，然后可以进一步处理。正如代码的**（1）**部分所突出显示的那样，项目参数正在从用户请求中检索，并放置在Python程序变量中。代码的**（2）**部分也是如此。它还从用户提供的设置中获取一些其他参数，并将它们放在一个名为settings的Python字典中。最后，当所有数据收集完毕，它将所有细节保存在名为`Project`的后端数据库表中。正如在第261行所示，代码初始化了一个名为`Project()`的类，然后从第262行到279行，它将从用户那里获得的参数分配给`Project()`类的实例变量。最后，在第280行，调用了`project.save()`代码。这将把所有实例变量作为单行放入数据库表中。'
- en: 'Basically, Django follows an ORM model of development. **ORM** stands for **object
    relational mapping**. The model layer of a Django project is a set of classes,
    and when the project is compiled using the `python manage.py syncdb` command,
    these classes actually translate into database tables. We actually do not write
    raw SQL queries in Django to push data to database tables or fetch them. Django
    provides us with a models wrapper that we can access as classes and call various
    methods such as `save()`, `delete()`, `update()`, `filter()`, and `get()` in order
    to perform **create, retrieve, update, and delete** (**CRUD**) operations on our
    database tables. For the current case, let''s take a look at the `models.py` file,
    which contains the `Project` model class:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，Django遵循开发的ORM模型。**ORM**代表**对象关系映射**。Django项目的模型层是一组类，当使用`python manage.py
    syncdb`命令编译项目时，这些类实际上会转换为数据库表。我们实际上不在Django中编写原始的SQL查询来将数据推送到数据库表或提取它们。Django为我们提供了一个模型包装器，我们可以将其作为类访问，并调用各种方法，如`save()`、`delete()`、`update()`、`filter()`和`get()`，以执行对数据库表的**创建、检索、更新和删除**（**CRUD**）操作。对于当前情况，让我们看一下包含`Project`模型类的`models.py`文件：
- en: '![](img/61d66aeb-a4ba-4316-9ec6-3cdee9d63aa4.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61d66aeb-a4ba-4316-9ec6-3cdee9d63aa4.png)'
- en: 'Thus, when the code is compiled or database syncing happens with the `python
    manage.py syncdb` command, a table will be created in the working database called `<project_name>_Project`.
    The schema of the table will be replicated as per the definition of the instance
    variables in the class. Thus, for the preceding case for the projects table, there
    will be 18 columns created. The table will have a primary key of `project_name`, whose
    data type within the Django application is defined as `CharField`, but at the
    backend will be translated to something like `varchar(50)`. The backend database
    in this case is a SQLite database, which is defined in the `settings.py` file
    as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当代码被编译或数据库同步发生时，使用`python manage.py syncdb`命令，一个名为`<project_name>_Project`的表将在工作数据库中创建。表的架构将根据类中实例变量的定义进行复制。因此，对于项目表的前面情况，将创建18个列。表将具有`project_name`的主键，Django应用程序中其数据类型被定义为`CharField`，但在后端将被转换为类似`varchar(50)`的东西。在这种情况下，后端数据库是SQLite数据库，在`settings.py`文件中定义如下：
- en: '![](img/39093f85-29cf-47bf-8a37-82d16ce9e608.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/39093f85-29cf-47bf-8a37-82d16ce9e608.png)'
- en: Sections **(3)** and **(4)** of the code snippet are interesting, as this is
    where the workflow execution actually begins. It can be seen in section **(3)**
    that we are checking for the OS environment. If the OS is Windows, then we are
    invoking the `crawler_driver` code `run_crawler.py` as a subprocess.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段的**（3）**和**（4）**部分很有趣，因为这是工作流执行实际开始的地方。可以在**（3）**部分看到，我们正在检查操作系统环境。如果操作系统是Windows，那么我们将调用`crawler_driver`代码`run_crawler.py`作为子进程。
- en: If the underlying environment is Linux-based, then we are invoking the same
    driver file with the command relevant to the Linux environment. As we might have
    observed previously, we are making use of a subprocess call to invoke this code
    as a separate process. The reason behind having this kind of architecture is so
    that we can use asynchronous processing. The HTTP request sent from the user should
    be responded to quickly with a message to indicate that the crawling has started.
    We can't have the same request held on until the whole crawling operation is complete.
    To accommodate this, we spawn an independent process and offload the crawling
    task to that process, and the HTTP request is immediately returned with an HTTP
    response indicating that the crawling has started. We further map the process
    ID and the project name/ID in the backend database to continuously monitor the
    status of the scan. We return control to the user by redirecting control to the
    details URL which in turn returns the template `details.html`.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果底层环境是基于Linux的，那么我们将使用与Linux环境相关的命令来调用相同的驱动文件。正如我们之前可能观察到的那样，我们使用子进程调用来将此代码作为单独的进程调用。拥有这种类型的架构背后的原因是为了能够使用异步处理。用户发送的HTTP请求应该能够快速得到响应，指示爬取已经开始。我们不能让相同的请求一直保持，直到整个爬取操作完成。为了适应这一点，我们生成一个独立的进程并将爬取任务卸载到该进程中，HTTP请求立即返回一个指示爬取已经开始的HTTP响应。我们进一步将进程ID和后端数据库中的项目名称/ID进行映射，以持续监视扫描的状态。我们通过将控制权重定向到详细URL来将控制权返回给用户，详细URL反过来返回模板`details.html`。
- en: Driver code – run_crawler.py
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 驱动代码 - run_crawler.py
- en: 'The following code is for the `run_crawler.py` file:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是`run_crawler.py`文件的代码：
- en: '![](img/11d5a06c-c6a0-4c34-8966-c4cd4a0c918a.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/11d5a06c-c6a0-4c34-8966-c4cd4a0c918a.png)'
- en: Remember how we invoked this file from our `views.py` code? We invoked it by
    passing a command-line argument that was the name of the project. As highlighted
    in section **(1)**, the preceding code of `run_crawler.py` loads that command-line
    argument into a project_name program variable. In section **(2)**, the code tries
    to read all the parameters from the backend database table project with the `project.objects.get(project_name=project_name)` command.
    As mentioned earlier, Django follows an ORM model and we don't need to write raw
    SQL queries to take data from database tables. The preceding code snippet will
    internally translate to `select * from project where project_name=project_name`.
    Thus, all the project parameters are pulled and passed to local program variables.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得我们如何从`views.py`代码中调用这个文件吗？我们通过传递一个命令行参数来调用它，这个参数是项目的名称。如第**(1)**部分所示，`run_crawler.py`的前面代码将这个命令行参数加载到一个`project_name`程序变量中。在第**(2)**部分，代码尝试从后端数据库表`project`中读取所有参数，使用`project.objects.get(project_name=project_name)`命令。正如之前提到的，Django遵循ORM模型，我们不需要编写原始的SQL查询来从数据库表中获取数据。前面的代码片段将在内部转换为`select
    * from project where project_name=project_name`。因此，所有项目参数都被提取并传递给本地程序变量。
- en: Finally, in section **(3)**, we initialize the `crawler` class and pass all
    the project parameters to it. Once initialized, we invoke the `c.start()` method
    highlighted as section **(4)**. This is where the crawling starts. In the next
    section, we will see the working of our crawler class.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在第**(3)**部分，我们初始化`crawler`类并将所有项目参数传递给它。一旦初始化，我们调用标记为第**(4)**部分的`c.start()`方法。这是爬取开始的地方。在接下来的部分，我们将看到我们的爬虫类的工作方式。
- en: Crawler code – crawler.py
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 爬虫代码 - crawler.py
- en: 'The following code snippet represents the constructor of the `crawler` class.
    It initializes all the relevant instance variables. `logger` is one of the custom
    classes written to log debug messages, so that if any error occurs during the
    execution of the crawler, which will have been spawned as a subprocess and will
    be running in the background, it can be debugged:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段代表了`crawler`类的构造函数。它初始化了所有相关的实例变量。`logger`是一个自定义类，用于记录调试消息，因此如果在爬虫执行过程中发生任何错误，它将作为一个子进程被生成并在后台运行，可以进行调试：
- en: '![](img/55cc89cf-df18-4999-9384-5e457c77afbd.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/55cc89cf-df18-4999-9384-5e457c77afbd.png)'
- en: '![](img/de260b8d-d57a-4452-9f49-d5b450c1253c.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de260b8d-d57a-4452-9f49-d5b450c1253c.png)'
- en: 'Let''s now take a look at the `start()` method of the `crawler`, from where
    the crawling actually begins:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看一下`crawler`的`start()`方法，从这里开始爬取实际上开始：
- en: '![](img/e3b390e3-51b0-4b62-b8ba-505f7ae87594.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3b390e3-51b0-4b62-b8ba-505f7ae87594.png)'
- en: It can be seen in section **(1)**, which will be true for the second iteration
    (`auth=True`), that we make a `HTTP GET` request to whichever URL is supplied
    as the login URL by the user. We are using the `GET` method from the Python `requests`
    library. When we make the `GET` request to the URL, the response content (web
    page) is placed in the `xx` variable.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在第**(1)**部分可以看到，对于第二次迭代（`auth=True`），我们会向用户提供的登录URL发出`HTTP GET`请求。我们使用Python
    `requests`库中的`GET`方法。当我们向URL发出`GET`请求时，响应内容（网页）会被放入`xx`变量中。
- en: Now, as highlighted in section **(2)**, we extract the content of the webpage
    using the `xx.content` command and pass the extracted content to the instance
    of the `Beautifulsoup` module. `Beautifulsoup` is an excellent Python utility
    that makes parsing web pages very simple. From here on, we will represent `Beautifulsoup` with
    an alias, BS.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如第**(2)**部分所示，我们使用`xx.content`命令提取网页内容，并将提取的内容传递给`Beautifulsoup`模块的实例。`Beautifulsoup`是一个非常好用的Python工具，可以使解析网页变得非常简单。从这里开始，我们将用别名BS来表示`Beautifulsoup`。
- en: Section **(3)** uses the `s.findall('form')` method from the BS parsing library.
    The `findall()` method takes the type of the HTML element, which is to be searched
    as a string argument, and it returns a list containing the search matches. If
    a web page contains ten forms, `s.findall('form')` will return a list containing
    the data for the ten forms. It will look as follows: `[<Form1 data>,<Form2 data>,
    <Form3 data> ....<Form10 data>]`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 第三部分使用了BS解析库中的`s.findall('form')`方法。`findall()`方法接受要搜索的HTML元素类型作为字符串参数，并返回一个包含搜索匹配项的列表。如果一个网页包含十个表单，`s.findall('form')`将返回一个包含这十个表单数据的列表。它看起来如下：`[<Form1
    data>,<Form2 data>, <Form3 data> ....<Form10 data>]`。
- en: 'In section **(4)** of the code, we are iterating over the list of forms that
    was returned before. The objective here is to identify the login form among multiple
    input forms that might be present on the web page. We also need to figure out
    the action URL of the login form, as that will be be the place where we will `POST`
    the valid credentials and set a valid session as shown in the following screenshots:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码的第四部分，我们正在遍历之前返回的表单列表。这里的目标是在网页上可能存在的多个输入表单中识别登录表单。我们还需要找出登录表单的操作URL，因为那将是我们`POST`有效凭据并设置有效会话的地方，如下面的截图所示：
- en: '![](img/386c8822-761c-43e9-bfce-9ac845603173.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/386c8822-761c-43e9-bfce-9ac845603173.png)'
- en: '![](img/917dce6b-b79d-4a19-bb25-c60251913528.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/917dce6b-b79d-4a19-bb25-c60251913528.png)'
- en: '![](img/bcad6b00-4d83-4be4-8761-17be78c8848e.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bcad6b00-4d83-4be4-8761-17be78c8848e.png)'
- en: 'Let''s try to break down the preceding incomplete code to understand what has
    happened so far. Before we move on, however, let''s take a look at the user interface
    from where the crawling parameters are taken from the user. This will give us
    a good idea about the prerequisites and will help us to understand the code better.
    The following screen shows a representation of the user input parameters:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试着分解前面的不完整代码，以了解到目前为止发生了什么。然而，在我们继续之前，让我们看一下用户界面，从中获取爬取参数。这将让我们对先决条件有一个很好的了解，并帮助我们更好地理解代码。以下屏幕显示了用户输入参数的表示：
- en: '![](img/dc67b572-3501-45f3-b936-264b5b4bdfa3.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc67b572-3501-45f3-b936-264b5b4bdfa3.png)'
- en: As mentioned earlier, the crawler works in two iterations. In the first iteration,
    it tries to crawl the web application without authentication, and in the second
    iteration, it crawls the application with authentication. The authentication information
    is held in the `self.auth` variable, which by default is initialized to `false`.
    Therefore, the first iteration will always be without authentication.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，爬虫工作分为两次迭代。在第一次迭代中，它尝试在没有身份验证的情况下爬取Web应用程序，在第二次迭代中，它使用身份验证爬取应用程序。身份验证信息保存在`self.auth`变量中，默认情况下初始化为`false`。因此，第一次迭代将始终没有身份验证。
- en: It should be noted that the purpose of the code mentioned before, which falls
    under the `< if self.auth ==True >` section, is to identify the login form from
    the login web page/URL. Once the login form is identified, the code tried to identify
    all the input fields of that form. It then formulates a data payload with legitimate
    user credentials to submit the login form. Once submitted, a valid user session
    will be returned and saved. That session will be used for the second iteration
    of crawling, which is authentication-based.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，前面提到的代码的目的是从登录网页/URL中识别登录表单。一旦识别出登录表单，代码就会尝试识别该表单的所有输入字段。然后，它将制定一个包含合法用户凭据的数据有效载荷，以提交登录表单。提交后，将返回并保存一个有效的用户会话。该会话将用于基于身份验证的第二次爬取迭代。
- en: 'In section **(5)** of the code, we are invoking the `self.process_form_action()`
    method. Before that, we extract the action URL of the form, so we know where the
    data is to be *posted*. It also combines the relative action URL with the base
    URL of the application, so that we end up sending our request to a valid endpoint
    URL. For example, if the form action is pointing to a location called `/login`,
    and the current URL is `http://127.0.0.1/my_app`, this method will carry out the
    following tasks:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码的第五部分，我们正在调用`self.process_form_action()`方法。在此之前，我们提取了表单的操作URL，以便知道数据将被*发布*的位置。它还将相对操作URL与应用程序的基本URL结合起来，这样我们最终会将请求发送到一个有效的端点URL。例如，如果表单操作指向名为`/login`的位置，当前URL为`http://127.0.0.1/my_app`，这个方法将执行以下任务：
- en: Check whether the URL is already added to a list of URLs that the crawler is
    supposed to visit
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查URL是否已经添加到爬虫应该访问的URL列表中
- en: Combine the action URL with the base context URL and return `http://127.0.0.1/my_app/login`
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将操作URL与基本上下文URL组合并返回`http://127.0.0.1/my_app/login`
- en: 'The definition of this method is shown here:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法的定义如下所示：
- en: '![](img/0ca8d816-05a2-43b1-9257-04195a391ff2.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ca8d816-05a2-43b1-9257-04195a391ff2.png)'
- en: 'As can be seen, the first thing that is invoked within this method is another
    method, `self.check_and_add_to_visit`. This method checks whether the URL in question
    has already been added to the list of URLs that the crawler is supposed to crawl.
    If it is added, then `no9` action is done. If not, the crawler adds the URL for
    it to revisit later. There are many other things that this method checks, such
    as whether the URL is in scope, whether the protocol is the one permitted, and
    so on. The definition of this method is shown here:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到，在这个方法中首先调用的是另一个方法`self.check_and_add_to_visit`。这个方法检查所讨论的URL是否已经被添加到爬虫应该爬取的URL列表中。如果已经添加，则执行`no9`操作。如果没有，爬虫将该URL添加到稍后重新访问。这个方法还检查许多其他事情，比如URL是否在范围内，协议是否被允许等等。这个方法的定义如下所示：
- en: '![](img/8eafa6cb-3962-458a-8816-7d6bc272cc1a.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8eafa6cb-3962-458a-8816-7d6bc272cc1a.png)'
- en: 'As can be seen, if `self.already_seen()` under line 158 returns `false`, then
    a row is created in the backend database `Page` table under the current project.
    The row is created again via Django ORM (model abstraction). The `self.already_seen()` method simply
    checks the `Page` table to see whether the URL in question has been visited under
    the current project name and the current authentication mode by the crawler or
    not. This is verified with the visited `Flag`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，如果第158行下的`self.already_seen()`返回`false`，那么在当前项目的后端数据库`Page`表中将创建一行。这一行再次通过Django
    ORM（模型抽象）创建。`self.already_seen()`方法只是检查`Page`表，看看爬虫是否以当前项目名称和当前认证模式访问了问题URL。这是通过访问标志来验证的：
- en: '![](img/7a854a93-c9cc-4b7b-a1d5-974ab26aeae7.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a854a93-c9cc-4b7b-a1d5-974ab26aeae7.png)'
- en: '`Page.objects.filter()` is equivalent to `select * from page where auth_visited=True/False
    and project=''current_project'' and URL=''current_url''`.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`Page.objects.filter()`相当于`select * from page where auth_visited=True/False
    and project=''current_project'' and URL=''current_url''`。'
- en: In section **(6)** of the code, we are passing the content of the current form
    to a newly created instance of the BS parsing module. The reason for this is that
    we will parse and extract all the input fields from the form that we are currently
    processing. Once the input fields are extracted, we will compare the name of each
    input field with the name that is supplied by the user under `username_field` and
    `password_field`. The reason why we do this is that there might be occasions where
    there are multiple forms on the login page such as a search form, a sign up form,
    a feedback form, and a login form. We need to be able to identify which of these
    forms is the login form. As we are asking the user to provide the field name for
    **login username/email** and the field name for **Login-password**, our approach
    will be to extract the input fields from all forms and compare them with what
    the user has supplied. If we get a match for both the fields, we set `flag1` and
    `flag2` to `True`. If we get a match within a form, it is very likely that this
    is our login form. This is the form in which we will place our user supplied login
    credentials under the appropriate fields and then submit the form at the action
    URL, as specified under the action parameter. This logic is handled by sections
    **(7)**, **(8)**, **(9)**, **(10)**, **(11)**, **(12)**, **(13)**, and **(14)**.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码的第**（6）**部分，我们将当前表单的内容传递给一个新创建的BS解析模块的实例。这样做的原因是我们将解析并提取当前处理的表单中的所有输入字段。一旦输入字段被提取，我们将比较每个输入字段的名称与用户在`username_field`和`password_field`下提供的名称。我们这样做的原因是可能会有多个表单在登录页面上，比如搜索表单、注册表单、反馈表单和登录表单。我们需要能够识别这些表单中的哪一个是登录表单。当我们要求用户提供**登录用户名/电子邮件**字段名称和**登录密码**字段名称时，我们的方法是从所有表单中提取输入字段并将它们与用户提供的内容进行比较。如果我们两个字段都匹配，我们将`flag1`和`flag2`设置为`True`。如果我们在一个表单中找到匹配，很可能这就是我们的登录表单。这是我们将在其中将用户提供的登录凭据放在适当字段下并在操作URL下提交表单的表单。这个逻辑由第**（7）**、**（8）**、**（9）**、**（10）**、**（11）**、**（12）**、**（13）**和**（14）**部分处理。
- en: There is another consideration that is important. There might be many occasions
    in which the login web page also has a signup form in it. Let's suppose that the
    user has specified `username` and `user_pass` as the field names for the username
    and password parameters for our code, to submit proper credentials under these
    field names to obtain a valid session. However, the signup form also contains
    another two fields, also called `username` and `user_pass`, and this also contains
    a few additional fields such as **Address**, **Phone**, **Email**, and so on.
    However, as discussed earlier, our code identifies the login form with these supplied
    field names only, and may end up considering the signup form as the login form.
    In order to address this, we are storing all the obtained forms in program lists.
    When all the forms are parsed and stored, we should have two probable candidates
    as login forms. We will compare the content length of both, and the one with a
    shorter length will be taken as the login form. This is because the signup form
    will usually have more fields than a login form. This condition is handled by
    section **(15)** of the code, which enumerates over all the probable forms and
    finally places the smallest one at index 0 of the `payloadforms[]` list and the `actionform[]`
    list.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个重要的考虑因素。登录网页上可能还有注册表单。假设用户已经在我们的代码中指定了`username`和`user_pass`作为用户名和密码参数的字段名称，以便在这些字段名称下提交正确的凭据以获得有效会话。然而，注册表单还包含另外两个字段，也称为`username`和`user_pass`，还包含一些其他字段，如**地址**、**电话**、**电子邮件**等。然而，正如前面讨论的，我们的代码只识别这些提供的字段名称的登录表单，并可能将注册表单视为登录表单。为了解决这个问题，我们将所有获取的表单存储在程序列表中。当所有表单都被解析和存储时，我们应该有两个可能的登录表单候选。我们将比较两者的内容长度，长度较短的将被视为登录表单。这是因为注册表单通常比登录表单有更多的字段。这个条件由代码的第**（15）**部分处理，它枚举了所有可能的表单，并最终将最小的表单放在`payloadforms[]`列表和`actionform[]`列表的索引0处。
- en: Finally, in line 448, we post the supplied user credentials to the valid parsed
    login form. If the credentials are correct, a valid session will be returned and
    placed under a session variable, `ss`. The request is made by invoking the `POST`
    method as follows: `ss.post(action_forms[0],data=payload,cookie=cookie)`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在第448行，我们将提供的用户凭据发布到有效解析的登录表单。如果凭据正确，将返回有效会话并放置在会话变量`ss`下。通过调用`POST`方法进行请求，如下所示：`ss.post(action_forms[0],data=payload,cookie=cookie)`。
- en: 'The user provides the start URL of the web application that is to be crawled.
    Section **(16)** takes that start URL and begins the crawling process. If there
    are multiple start URLs, they should be comma separated. The start URLs are added
    to the `Page()` database table as a URL that the crawler is supposed to visit:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 用户提供要爬取的Web应用程序的起始URL。第**（16）**部分获取该起始URL并开始爬取过程。如果有多个起始URL，它们应该用逗号分隔。起始URL被添加到`Page()`数据库表中，作为爬虫应该访问的URL：
- en: '![](img/c24126aa-1811-425f-8e6f-3eb1393bb9ab.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c24126aa-1811-425f-8e6f-3eb1393bb9ab.png)'
- en: 'In section **(17)**, there is a crawling loop that invokes a `there_are_pages_to_crawl()` method,
    which checks the backend `Page()` database table to see whether there are any
    pages for the current project with the visited flag `set = False`. If there are
    pages in the table that have not been visited by the crawler, this method will
    return `True`. As we just added the start page to the `Page` table in section **(16)**,
    this method will return `True` for the start page. The idea is to make a `GET` request
    on that page and extract all further links, forms, or URLs, and keep on adding
    them to the `Page` table. The loop will continue to execute as long as there are
    unvisited pages. Once the page is completely parsed and all links are extracted,
    the visited flag is `set=True` for that page or URL so that it will not be extracted
    to be crawled again. The definition of this method is shown here:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在第**(17)**节中，有一个爬行循环调用`there_are_pages_to_crawl()`方法，该方法检查后端的`Page()`数据库表，看看当前项目中是否有任何未被访问的页面，visited
    flag`set = False`。如果表中有尚未被爬行器访问的页面，此方法将返回`True`。由于我们刚刚在第**(16)**节将起始页面添加到`Page`表中，因此此方法将对起始页面返回`True`。其思想是对该页面进行`GET`请求，并提取所有进一步的链接、表单或URL，并不断将它们添加到`Page`表中。只要有未被访问的页面，循环将继续执行。一旦页面完全解析并提取了所有链接，visited
    flag就会被设置为`True`，以便不会再提取该页面或URL进行爬行。该方法的定义如下所示：
- en: '![](img/648bf440-d99e-46be-bbbb-429276042251.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/648bf440-d99e-46be-bbbb-429276042251.png)'
- en: 'In section **(18)**, we get the unvisited page from the backend `Page` table by
    invoking the `get_a_page_to_visit()` method, the definition of which is given
    here:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在第**(18)**节中，我们通过调用`get_a_page_to_visit()`方法从后端的`Page`表中获取未访问的页面，该方法的定义在这里给出：
- en: '![](img/4bdcd81c-aafd-4cc8-b4da-c6f87e7c81ce.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bdcd81c-aafd-4cc8-b4da-c6f87e7c81ce.png)'
- en: '![](img/4bdcd81c-aafd-4cc8-b4da-c6f87e7c81ce.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bdcd81c-aafd-4cc8-b4da-c6f87e7c81ce.png)'
- en: In section **(19)**, we make a HTTP `GET` request to this page, along with the
    session cookies, `ss`, as section **(19)** belongs to the iteration that deals
    with `auth=True`. Once a request is made to this page, the response of the page
    is then further processed to extract more links. Before processing the response,
    we check for the response codes produced by the application.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在第**(19)**节中，我们向该页面发出HTTP `GET`请求，同时携带会话cookie `ss`，因为第**(19)**节属于处理`auth=True`的迭代。一旦向该页面发出请求，页面的响应将进一步处理以提取更多链接。在处理响应之前，我们检查应用程序产生的响应代码。
- en: 'There are occasions where certain pages will return a redirection (`3XX` response
    codes) and we need to save the URLs and form content appropriately. Let''s say
    that we made a `GET` request to page X and in response we had three forms. Ideally,
    we will save those forms with the URL marked as X. However, let''s say that upon
    making a `GET` request on page X, we got a 302 redirection to page Y, and the
    response HTML actually belonged to the web page where the redirection was set.
    In that case, we will end up saving the response content of three forms mapped
    with the URL X, which is not correct. Therefore, in sections (20) and (21), we
    are handling these redirections and are mapping the response content with the
    appropriate URL:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，某些页面会返回重定向（`3XX`响应代码），我们需要适当保存URL和表单内容。假设我们向页面X发出了`GET`请求，响应中有三个表单。理想情况下，我们将以X为标记保存这些表单。但是，假设在向页面X发出`GET`请求时，我们得到了一个302重定向到页面Y，并且响应HTML实际上属于设置重定向的网页。在这种情况下，我们最终会保存与URL
    X映射的三个表单的响应内容，这是不正确的。因此，在第(20)和(21)节中，我们处理这些重定向，并将响应内容与适当的URL进行映射：
- en: '![](img/4bdcd81c-aafd-4cc8-b4da-c6f87e7c81ce.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4bdcd81c-aafd-4cc8-b4da-c6f87e7c81ce.png)'
- en: 'Sections (22) and (23) do exactly what the previously mentioned sections (19),
    (20), and (21) do, but (22) and (23) do it for iterations where `authentication
    =False`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 第(22)和(23)节的代码与前面提到的第(19)、(20)和(21)节完全相同，但(22)和(23)节是针对`authentication=False`的迭代进行的：
- en: '![](img/4b6fd524-92c5-423e-96aa-4c230d964ae6.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b6fd524-92c5-423e-96aa-4c230d964ae6.png)'
- en: If any exceptions are encountered while processing the current page, section
    (24) handles those exceptions, marks the visited flag of the current page as `True`,
    and puts an appropriate exception message in the database.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在处理当前页面时遇到任何异常，第(24)节将处理这些异常，将当前页面的visited flag标记为`True`，并在数据库中放置适当的异常消息。
- en: 'If everything works smoothly, then control passes on to section (26), from
    where the processing of the HTML response content obtained from the `GET` request
    on the current page being visited begins. The objective of this processing is
    to do the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，控制将传递到第(26)节，从那里开始处理从当前正在访问的页面上的`GET`请求获取的HTML响应内容。此处理的目标是进行以下操作：
- en: Extract all further links from the HTML response (`a href`, `base` tags, `Frame`
    tags, `iframe` tags)
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从HTML响应中提取所有进一步的链接（`a href`、`base`标签、`Frame`标签、`iframe`标签）
- en: Extract all forms from the HTML response
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从HTML响应中提取所有表单
- en: Extract all form fields from the HTML response
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取HTML响应中的所有表单字段
- en: Section **(26)** of the code extracts all the links and URLs that are present
    under the `base` tag (if any) of the returned HTML response content.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的第**(26)**节提取了返回的HTML响应内容中`base`标签下（如果有的话）存在的所有链接和URL。
- en: Sections **(27)** and **(28)** parses the content with the BS parsing module
    to extract all anchor tags and their `href` locations. Once extracted, they are
    passed to be added to the `Pages` database table for the crawler to visit later.
    It must be noted that the links are added only after checking they don't exist
    already under the current project and current authentication mode.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 第**(27)**和**(28)**节使用BS解析模块解析内容，提取所有锚标签及其`href`位置。一旦提取，它们将被传递以添加到`Pages`数据库表中，以供爬行器以后访问。必须注意的是，只有在检查它们在当前项目和当前身份验证模式下不存在后，才会添加这些链接。
- en: 'Section **(29)** parses the content with the BS parsing module to extract all
    `iframe` tags and their `src` locations. Once extracted, they are passed to be
    added to the `Pages` database table for the crawler to visit later. Section **(30)** does
    the same for frame tags:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 第（29）节使用BS解析模块解析内容，以提取所有`iframe`标签及其`src`位置。一旦提取，它们将被传递以添加到`Pages`数据库表中，以便爬虫以后访问。第（30）节对frame标签执行相同的操作：
- en: '![](img/da96a8bd-6587-478b-a449-c57060376a82.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/da96a8bd-6587-478b-a449-c57060376a82.png)'
- en: Section **(31)** parses the content with the BS parsing module to extract all
    option tags and checks whether they have a link under the `value` attribute. Once
    extracted, they are passed to be added to the `Pages` database table for the crawler
    to visit later.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 第（31）节使用BS解析模块解析内容，以提取所有选项标签，并检查它们是否在`value`属性下有链接。一旦提取，它们将被传递以添加到`Pages`数据库表中，以便爬虫以后访问。
- en: 'Section **(32)** of the code tries to explore all other options to extract
    any missed links from a web page. The following is the code snippet that checks
    for other possibilities:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的第（32）节尝试探索从网页中提取任何遗漏链接的所有其他选项。以下是检查其他可能性的代码片段：
- en: '![](img/7405799b-5114-4547-b285-ed249137023b.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7405799b-5114-4547-b285-ed249137023b.png)'
- en: 'Sections **(33)** and **(34)** extract all the forms from the current HTML
    response. If any forms are identified, various attributes of the form tag, such
    as action or method, are extracted and saved under local variables:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 第（33）和第（34）节从当前HTML响应中提取所有表单。如果识别出任何表单，将提取并保存表单标签的各种属性，例如action或method，保存在本地变量中：
- en: '![](img/917d3776-7194-4557-b226-ef833763f39d.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/917d3776-7194-4557-b226-ef833763f39d.png)'
- en: 'If any HTML form is identified, the next task is to extract all the input fields,
    text areas, select tags, option fields, hidden fields, and submit buttons. This
    is carried out by sections **(35)**, **(36)**, **(37)**, **(38)**, and **(39)**.
    Finally, all the extracted fields are placed under an `input_field_list` variable in
    a comma-separated manner. For example, let''s say a form, `Form1`, is identified
    with the following fields:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果识别出任何HTML表单，下一个任务是提取所有输入字段、文本区域、选择标签、选项字段、隐藏字段和提交按钮。这是由第（35）、（36）、（37）、（38）和（39）节执行的。最后，所有提取的字段以逗号分隔的方式放在`input_field_list`变量下。例如，假设识别出一个名为`Form1`的表单，其中包含以下字段：
- en: '`<input type ="text" name="search">`'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<input type ="text" name="search">`'
- en: '`<input type="hidden" name ="secret">`'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<input type="hidden" name ="secret">`'
- en: '`<input type="submit" name="submit_button>`'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<input type="submit" name="submit_button>`'
- en: 'All of these are extracted as `"Form1" : input_field_list = "search,text,secret,hidden,submit_button,submit"`**.**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '所有这些都被提取为`"Form1" : input_field_list = "search,text,secret,hidden,submit_button,submit"`**.**'
- en: 'Section **(40)** of the code checks whether there are already any forms saved
    in the database table with the exact same content for the current project and
    current `auth_mode`. If no such form exists, the form is saved in the `Form` table,
    again with the help of the Django ORM (`models`) wrapper:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的第（40）节检查数据库表中是否已经保存了具有当前项目和当前`auth_mode`相同内容的任何表单。如果没有这样的表单存在，则使用Django ORM（`models`）包再次将表单保存在`Form`表中：
- en: '![](img/a2ac4a7f-1ce5-4b73-8e65-93295d283408.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a2ac4a7f-1ce5-4b73-8e65-93295d283408.png)'
- en: 'Section (41) of the previous code goes ahead and saves these unique forms in
    a JSON file with the name as the current project name. This file can then be parsed
    with a simple Python program to list various forms and input fields present in
    the web application that we crawled. Additionally, at the end of the code, we
    have a small snippet that places all discovered/crawled pages in a text file that
    we can refer to later. The snippet is shown here:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 先前代码的第（41）节继续并将这些唯一的表单保存在以当前项目名称命名的JSON文件中。然后可以使用简单的Python程序解析此文件，以列出我们爬取的网页应用程序中存在的各种表单和输入字段。此外，在代码的末尾，我们有一个小片段，将所有发现/爬取的页面放在一个文本文件中，以便以后参考。片段如下所示：
- en: '[PRE1]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Section **(42)** of the code updates the visited flag of the web page whose
    content we just parsed and marks that as visited for the current `auth` mode.
    If any exceptions occur during saving, these are handled by section **(43)**,
    which again marks the visited flag as `true`, but additionally adds an exception
    message.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的第（42）节更新了刚刚解析内容的网页的访问标志，并标记为当前`auth`模式的已访问。如果在保存期间发生任何异常，这些异常将由第（43）节处理，该节再次将访问标志标记为`true`，但另外添加异常消息。
- en: After sections **(42)** and **(43)**, the control goes back again to section
    **(17)** of the code. The next page that is yet to be visited by the crawler is
    taken from the database and all the operations are repeated. This continues until
    all web pages have been visited by the crawler.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在第（42）和第（43）节之后，控制再次回到代码的第（17）节。爬虫尚未访问的下一页从数据库中获取，并重复所有操作。这将持续到爬虫访问了所有网页为止。
- en: Finally, we check whether the current iteration is with or without authentication
    in section (44). If it was without authentication, then the `start()` method of
    the crawler is invoked with the `auth` flag set to `True`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们检查当前迭代是否在第（44）节中进行身份验证。如果没有进行身份验证，则调用爬虫的`start()`方法，并将`auth`标志设置为`True`。
- en: After both the iterations are successfully finished, the web application is
    assumed to be crawled completely and the project status is marked as **Finished** by
    section (45) of the code.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 成功完成两次迭代后，假定网页应用程序已完全爬取，并且代码的第（45）节将项目状态标记为**已完成**。
- en: Execution of code
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码的执行
- en: 'The first step we need to do is to convert the model classes into database
    tables. This can be done by executing the `syncdb()` command as shown here:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一步是将模型类转换为数据库表。可以通过执行`syncdb()`命令来完成，如下所示：
- en: '![](img/1842a8e9-b78d-44e2-bd63-7ef34f41cdff.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1842a8e9-b78d-44e2-bd63-7ef34f41cdff.png)'
- en: 'Once the database tables are created, let''s start the Django server as shown
    here:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 创建数据库表后，让我们启动Django服务器，如下所示：
- en: '![](img/067049a1-d5ed-40c7-a280-d8fe41064921.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/067049a1-d5ed-40c7-a280-d8fe41064921.png)'
- en: 'We will be testing our crawler against the famous DVWA application to see what
    it discovers. We need to start the Apache server and serve DVWA locally. The Apache
    server can be started by running the following command:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将测试我们的爬虫针对著名的DVWA应用程序，以查看它发现了什么。我们需要启动Apache服务器并在本地提供DVWA。可以通过运行以下命令启动Apache服务器：
- en: '[PRE2]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, let''s browse the Crawler interface and supply the scan parameters as
    follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们浏览爬虫界面，并提供以下扫描参数：
- en: '![](img/326f3517-bc7e-443c-ad1d-1e96fe5377f0.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/326f3517-bc7e-443c-ad1d-1e96fe5377f0.png)'
- en: '![](img/ee6968ff-bcbe-4af0-8085-2e1d9c3246ea.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee6968ff-bcbe-4af0-8085-2e1d9c3246ea.png)'
- en: 'Click on the **Start Crawling** button:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**开始爬取**按钮：
- en: '![](img/5f087f8b-e2a5-4e9c-bc82-e4bf95fbe61e.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f087f8b-e2a5-4e9c-bc82-e4bf95fbe61e.png)'
- en: 'Let''s now browse the `results` folder of the app, which is at the `<Xtreme_InjectCrawler/results>` path, to
    see the URLs and forms discovered as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们浏览应用程序的`results`文件夹，位于`<Xtreme_InjectCrawler/results>`路径，以查看发现的URL和表单如下：
- en: '![](img/01f854c9-3497-4f8c-8024-b2ea90fdbbd2.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01f854c9-3497-4f8c-8024-b2ea90fdbbd2.png)'
- en: 'Let''s open the JSON file first to see the contents:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 首先打开JSON文件查看内容：
- en: '![](img/8c68dfc8-4be7-4ffe-a668-8a77a2e187fc.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8c68dfc8-4be7-4ffe-a668-8a77a2e187fc.png)'
- en: 'Now, let''s open the `Pages_Dvwa_test` file to see the discovered URLs as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们打开`Pages_Dvwa_test`文件，查看发现的URL如下：
- en: '![](img/c77b7d22-a667-4a17-a36c-f38777f48b2a.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c77b7d22-a667-4a17-a36c-f38777f48b2a.png)'
- en: 'It can therefore be verified that the crawler has successfully crawled the
    application and identified the links shown in the previous screenshot:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，可以验证爬虫已成功爬取了应用程序，并识别了前一个截图中显示的链接：
- en: '![](img/1c9bb08e-c24f-4593-a6f1-7f5507a77805.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1c9bb08e-c24f-4593-a6f1-7f5507a77805.png)'
- en: Summary
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we saw how we can write a custom crawler from scratch. This
    task is made easier using Python's modules, such as requests, BeautifulSoup, and
    so on. Feel free to download the whole code base and test the crawler with various
    other websites in order to examine its coverage. There may be occasions in which
    the crawler does not give 100% coverage. Take a look and see for yourself the
    limitations of the crawler and how it can be improved.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了如何从头开始编写自定义爬虫。使用Python的模块，如requests，BeautifulSoup等，可以更轻松地完成这项任务。随意下载整个代码库，并测试爬虫与其他各种网站，以检查其覆盖范围。爬虫可能无法达到100%的覆盖率。看看爬虫的局限性以及如何改进它。
- en: Questions
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: How can the crawler be improved to cover JavaScript and Ajax calls?
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何改进爬虫以涵盖JavaScript和Ajax调用？
- en: How can we use the crawler results to automate web application testing?
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何使用爬虫结果来自动化Web应用程序测试？
- en: Further reading
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Penetration Testing Automation Using Python and Kali Linux*: [https://www.dataquest.io/blog/web-scraping-tutorial-python/](https://www.dataquest.io/blog/web-scraping-tutorial-python/)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python和Kali Linux进行渗透测试自动化：[https://www.dataquest.io/blog/web-scraping-tutorial-python/](https://www.dataquest.io/blog/web-scraping-tutorial-python/)
- en: '*Requests: HTTP for Humans*: [http://docs.python-requests.org/en/master/](http://docs.python-requests.org/en/master/)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Requests: 人类使用的HTTP*：[http://docs.python-requests.org/en/master/](http://docs.python-requests.org/en/master/)'
- en: '*Django project*: [https://www.djangoproject.com/](https://www.djangoproject.com/)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Django项目*：[https://www.djangoproject.com/](https://www.djangoproject.com/)'
- en: '*Penetration Testing Automation Using Python and Kali Linux*: [https://scrapy.org/](https://scrapy.org/)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Python和Kali Linux进行渗透测试自动化：[https://scrapy.org/](https://scrapy.org/)
