- en: Web Crawling with Scrapy – Mapping the Application
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scrapy进行Web爬行-映射应用程序
- en: 'In [Chapter 2](part0029.html#RL0A0-5a228e2885234f4ba832bb786a6d0c80), *Interacting
    with Web Applications*, we learned how to interact with a web application programmatically
    using Python and the requests library. In this chapter, we will cover the following
    topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](part0029.html#RL0A0-5a228e2885234f4ba832bb786a6d0c80)，*与Web应用程序交互*中，我们学习了如何使用Python和requests库以编程方式与Web应用程序进行交互。在本章中，我们将涵盖以下主题：
- en: Web application mapping
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Web应用程序映射
- en: Creating our own crawler/spider with Scrapy
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Scrapy创建我们自己的爬虫
- en: Making our crawler recursive
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使我们的爬虫递归
- en: Scraping interesting stuff
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抓取有趣的东西
- en: Web application mapping
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Web应用程序映射
- en: Remember in [Chapter 1](part0019.html#I3QM0-5a228e2885234f4ba832bb786a6d0c80),
    *Introduction to Web Application Penetration Testing*, that we learned about the
    penetration testing process. In that process, the second phase was mapping.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得在[第1章](part0019.html#I3QM0-5a228e2885234f4ba832bb786a6d0c80)，*Web应用程序渗透测试简介*中，我们学习了渗透测试过程。在该过程中，第二阶段是映射。
- en: In the mapping phase, we need to build a map or catalog of the application resources
    and functionalities. As a security tester, we aim to identify all the components
    and entry points in the app. The main components that we are interested in are
    the resources that take parameters as input, the forms, and the directories.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在映射阶段，我们需要构建应用程序资源和功能的地图或目录。作为安全测试人员，我们的目标是识别应用程序中的所有组件和入口点。我们感兴趣的主要组件是以输入参数为输入的资源、表单和目录。
- en: The mapping is mainly performed with a crawler. Crawlers are also known as spiders,
    and usually, they perform scraping tasks, which means that they will also extract
    interesting data from the application such as emails, forms, comments, hidden
    fields, and more.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 映射主要是通过爬虫来执行的。爬虫也被称为蜘蛛，通常执行抓取任务，这意味着它们还将从应用程序中提取有趣的数据，如电子邮件、表单、评论、隐藏字段等。
- en: 'In order to perform application mapping, we have the following options:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行应用程序映射，我们有以下选项：
- en: The first technique is crawling. The idea is to request the first page, pass
    all the content, extract all the links in scope, and repeat this with the links
    that have been discovered until the entire application is covered. Then, we can
    use an HTTP proxy to identify all the resources and links that may be missed by
    a crawler. Basically, most of the URLs that are generated dynamically in the browser
    with JavaScript will be missed by the crawler, as the crawler does not interpret
    JS.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一种技术是爬行。其思想是请求第一页，传递所有内容，提取范围内的所有链接，并重复这个过程，直到整个应用程序都被覆盖。然后，我们可以使用HTTP代理来识别爬虫可能错过的所有资源和链接。基本上，浏览器中使用JavaScript动态生成的大多数URL将被爬虫忽略，因为爬虫不解释JS。
- en: Another technique is to discover resources that are not linked anywhere in the
    application by using dictionary attacks. We'll build our own BruteForcer in the
    next section.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种技术是通过使用字典攻击来发现应用程序中未链接到任何地方的资源。我们将在下一节中构建我们自己的BruteForcer。
- en: 'Here, we have an example of how the Burp proxy creates application mapping
    using the proxy and the spider functionalities:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有Burp代理使用代理和蜘蛛功能创建应用程序映射的示例：
- en: '![](img/00023.jpeg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00023.jpeg)'
- en: We can see the directories, the static pages, and the pages that accept parameters,
    with the different parameters and the different values.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到目录、静态页面和接受参数的页面，以及不同的参数和不同的值。
- en: All the interesting parts will be used for handling vulnerabilities using different
    techniques such as SQL injection, cross-site scripting, XML injection, and LDAP
    injection. Basically, the aim of mapping is to cover all the applications in order
    to identify the interesting resources for the vulnerability identification phase.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所有有趣的部分将用于使用不同的技术处理漏洞，如SQL注入，跨站脚本，XML注入和LDAP注入。基本上，映射的目的是覆盖所有应用程序，以识别漏洞识别阶段的有趣资源。
- en: In the next section, we'll start developing our own crawler. Let's get ready!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将开始开发我们自己的爬虫。准备好了吗！
- en: Creating our own crawler/spider with Scrapy
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scrapy创建我们自己的爬虫
- en: In this section, we'll create our first Scrapy project. We'll define our objective,
    create our spider, and finally, run it and see the results.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建我们的第一个Scrapy项目。我们将定义我们的目标，创建我们的爬虫，最后运行它并查看结果。
- en: Starting with Scrapy
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用Scrapy
- en: 'First, we need to define what we want to accomplish. In this case, we want
    to create a crawler that will extract all the book titles from [https://www.packtpub.com/](https://www.packtpub.com/).
    In order to do so, we need to analyze our target. If we go to the [https://www.packtpub.com/](https://www.packtpub.com/)
    website and right-click on a book title and select Inspect, we will see the source
    code of that element. We can see, in this case, that the book title has this format:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要定义我们想要实现的目标。在这种情况下，我们想要创建一个爬虫，它将从[https://www.packtpub.com/](https://www.packtpub.com/)提取所有的书名。为了做到这一点，我们需要分析我们的目标。如果我们去[https://www.packtpub.com/](https://www.packtpub.com/)网站，右键单击书名并选择检查，我们将看到该元素的源代码。在这种情况下，我们可以看到书名的格式是这样的：
- en: '![](img/00024.jpeg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00024.jpeg)'
- en: Creating a crawler for extracting all the book titles
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个用于提取所有书名的爬虫
- en: 'Here, we can see `div` with a `class` of `book-block-title`, and then the title
    name. Keep this in mind or in a notebook, as that would be even better. We need
    this to define what we want to extract in our crawl process. Now, let''s get coding:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到`div`的`class`是`book-block-title`，然后是标题名称。记住这一点，或者在笔记本中记下来，那会更好。我们需要这样做来定义我们在爬行过程中想要提取的内容。现在，让我们开始编码：
- en: 'Let''s go back to our virtual machine and open a Terminal. In order to create
    a crawler, we''ll change to the `/Examples/Section-3` directory:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们回到我们的虚拟机并打开一个终端。为了创建一个爬虫，我们将切换到`/Examples/Section-3`目录：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Then, we need to create our project with the following Scrapy command:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们需要使用以下Scrapy命令创建我们的项目：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In our case, the name of the crawler is `basic_crawler`.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，爬虫的名称是`basic_crawler`。
- en: When we create a project, Scrapy automatically generates a folder with the basic
    structure of the crawler.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们创建一个项目时，Scrapy会自动生成一个具有爬虫基本结构的文件夹。
- en: 'Inside the `basic_crawler` directory, you will see another folder called `basic_crawler`.
    We are interested in working with the `items.py` file and the content of the `spiders`
    folder:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`basic_crawler`目录中，您会看到另一个名为`basic_crawler`的文件夹。我们对`items.py`文件和`spiders`文件夹中的内容感兴趣：
- en: '![](img/00025.jpeg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00025.jpeg)'
- en: These are the two files we'll work with.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将要处理的两个文件。
- en: So, we open the Atom editor, and add our project with Add Project Folder... under Examples
    | Section-3 | basic crawler.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们打开Atom编辑器，并通过`Examples | Section-3 | basic crawler`下的`Add Project Folder...`添加我们的项目。
- en: 'Now, we need to open `items.py` in the Atom editor:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要在Atom编辑器中打开`items.py`：
- en: '![](img/00026.jpeg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00026.jpeg)'
- en: When working with Scrapy, we need to specify what the things we're interested
    in getting are while crawling a website. These things are called items in Scrapy,
    and think about them as our data module.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Scrapy时，我们需要指定在爬取网站时我们感兴趣的内容。这些内容在Scrapy中称为items，并且可以将它们视为我们的数据模块。
- en: So, let's edit the `items.py` file and define our first item. We can see in
    the preceding screenshot that the `BasicCrawlerItem` class was created.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，让我们编辑`items.py`文件并定义我们的第一个项目。我们可以在前面的截图中看到`BasicCrawlerItem`类已创建。
- en: 'We''ll create a variable called `title`, and that will be an object of the
    class `Field`:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将创建一个名为`title`的变量，它将是`Field`类的对象：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can delete the remaining part of the code after `title = scrappy.Field()`
    as it is not used.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以删除`title = scrappy.Field()`之后的代码的其余部分，因为它没有被使用。
- en: This is all for now with this file.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 目前就是这些。
- en: Let's move onto our spider. For the spider, we'll work on the `spiderman.py` file,
    which is created for this exercise in order to save time.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们继续进行我们的爬虫。对于爬虫，我们将在为此练习创建的`spiderman.py`文件上进行操作，以节省时间。
- en: 'We first need to copy it from `Examples/Section-3/examples/spiders/spiderman-base.py`
    to `/Examples/Section-3/basic_crawler/basic_crawler/spiders/spiderman.py`:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先需要将其从`Examples/Section-3/examples/spiders/spiderman-base.py`复制到`/Examples/Section-3/basic_crawler/basic_crawler/spiders/spiderman.py`：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, open the file in the editor and we can see at the top of the file the
    imports needed for this to work. We have `BaseSpider`, which is the basic crawling
    class. Then, we have `Selector`, which will help us to extract data using cross
    path. `BasicCrawlerItem` is the model we created in the `items.py` file. Finally,
    find a `Request` that will perform the request to the website:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，打开编辑器中的文件，我们可以在文件顶部看到为其工作所需的导入。我们有`BaseSpider`，这是基本的爬取类。然后，我们有`Selector`，它将帮助我们使用交叉路径提取数据。`BasicCrawlerItem`是我们在`items.py`文件中创建的模型。最后，找到一个`Request`，它将执行对网站的请求：
- en: '![](img/00027.jpeg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00027.jpeg)'
- en: 'Then, we have the `class MySpider`, which has the following fields:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有`class MySpider`，它有以下字段：
- en: '`name`: This is the name of our spider, which is needed to invoke it later.
    In our case, it is `basic_crawler`.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`：这是我们爬虫的名称，以便以后调用它所需的名称。在我们的情况下，它是`basic_crawler`。'
- en: '`allowed_domains`: This is a list of domains that are allowed to be crawled.
    Basically, this is done to keep the crawler in bounds of the project; in this
    case, we''re using `packtpub.com`.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`allowed_domains`：这是允许被爬取的域名列表。基本上，这是为了将爬虫限制在项目的范围内；在这种情况下，我们使用`packtpub.com`。'
- en: '`start_urls`: This is a list that contains the starting URLs where the crawler
    will start with the process. In this case, it is `https://www.packtpub.com`.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_urls`：这是一个包含爬虫将开始处理的起始URL的列表。在这种情况下，它是`https://www.packtpub.com`。'
- en: '`parse`: As the name suggests, here is where the parsing of the results happens.
    We instantiate the `Selector`, parsing it with the `response` of the request.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parse`：顾名思义，这里是结果解析的地方。我们用请求的`response`实例化`Selector`，对其进行解析。'
- en: Then, we define the `book_titles` variable that will contain the results of
    executing the following cross path query. The cross path query is based on the
    analysis we performed at the beginning of the chapter. This will result in an
    array containing all of the book titles extracted with the defined cross path
    from the response content. Now, we need to loop that array and create books of
    the `BasicCrawlerItem` type, and assign the extracted book title to the title
    of the book.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义将包含执行以下交叉路径查询结果的`book_titles`变量。交叉路径查询是基于我们在本章开头进行的分析。这将导致一个包含使用响应内容中定义的交叉路径提取的所有书名的数组。现在，我们需要循环该数组，并创建`BasicCrawlerItem`类型的书籍，并将提取的书名分配给书的标题。
- en: That's all for our basic crawler. Let's go to the Terminal and change the directory
    to `basic_crawler` and then run the crawler with `scrapy crawl basic_crawler`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的基本爬虫。让我们去终端，将目录更改为`basic_crawler`，然后使用`scrapy crawl basic_crawler`运行爬虫。
- en: 'All the results are printed in the console and we can see the book titles being
    scraped correctly:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 所有结果都打印在控制台上，我们可以看到书名被正确地抓取出来了：
- en: '![](img/00028.jpeg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00028.jpeg)'
- en: 'Now, let''s save the output of the folder in a file by adding `-o books.json
    -t`, followed by the type of the file that is `json`:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过添加`-o books.json -t`，后跟文件类型`json`，将文件夹的输出保存到文件中：
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, run it. We'll open the `books.json` file with `vi books.json`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在运行它。我们将使用`vi books.json`打开`books.json`文件。
- en: 'We can see that the book titles being extracted as expected:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到书名被提取出来了：
- en: '![](img/00029.jpeg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00029.jpeg)'
- en: There are some extra tabs and spaces in the titles, but we got the name of the
    books. This will be the minimal structure needed to create a crawler, but you
    might be wondering that we are just scraping the index page. How do we make it
    recursively crawl a whole website? That is a great question, and we'll answer
    this in the next section.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 书名中有一些额外的制表符和空格，但我们已经得到了书名。这将是创建爬虫所需的最小结构，但您可能会想我们只是在抓取索引页面。我们如何使其递归地爬取整个网站？这是一个很好的问题，我们将在下一节中回答这个问题。
- en: Making our crawler recursive
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使我们的爬虫递归
- en: 'In this section, we''ll start learning how to extract links, and then we''ll
    use them to make the crawler recursive. Now that we have created the basic structure
    of a crawler, let''s add some functionality:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将开始学习如何提取链接，然后我们将使用它们来使爬虫递归。现在我们已经创建了爬虫的基本结构，让我们添加一些功能：
- en: First, let's copy the prepared `spiderman.py` file for this exercise. Copy it
    from `examples/spiders/spiderman-recursive.py` to `basic_crawler/basic_crawler/spiders/spiderman.py`.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们为这个练习复制准备好的`spiderman.py`文件。从`examples/spiders/spiderman-recursive.py`复制到`basic_crawler/basic_crawler/spiders/spiderman.py`。
- en: 'Then, go back to our editor. As we would like to make the crawler recursive,
    for this purpose, we will once again work the `spiderman.py` file and start with
    adding another extractor. However, this time we''ll add the links instead of titles,
    as highlighted in the following screenshot:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，回到我们的编辑器。由于我们想要使爬虫递归，为此目的，我们将再次处理`spiderman.py`文件，并开始添加另一个提取器。然而，这次我们将添加链接而不是标题，如下截图所示：
- en: '![](img/00030.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00030.jpeg)'
- en: 'Also, we need to make sure that the links are valid and complete, so we''ll
    create a regular expression that will validate links highlighted in the following
    screenshot:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，我们需要确保链接是有效和完整的，因此我们将创建一个正则表达式，用于验证以下截图中突出显示的链接：
- en: '![](img/00031.jpeg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00031.jpeg)'
- en: This regular expression should validate all HTTP and HTTPS absolute links. Now
    that we have the code to extract the links, we need an array to control the visited
    links, as we don't want to repeat links and waste resources.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个正则表达式应该验证所有的HTTP和HTTPS绝对链接。现在我们有了提取链接的代码，我们需要一个数组来控制已访问的链接，因为我们不想重复链接和浪费资源。
- en: 'Finally, we need to create a loop to iterate over the links found, and if the
    link is an absolute URL and has not been visited before, we `yield` a request
    with that URL to continue the process:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要创建一个循环来遍历找到的链接，如果链接是绝对URL并且以前没有被访问过，我们就`yield`一个带有该URL的请求来继续这个过程：
- en: '![](img/00032.jpeg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00032.jpeg)'
- en: If the link failed the validation, it means it is a relative URL. So, we will
    join that relative URL with the base URL, where this link was obtained from by
    creating a valid absolute URL. Then, we'll use the `yield` request.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果链接未通过验证，这意味着它是一个相对URL。因此，我们将通过将相对URL与获取该链接的基本URL结合来创建一个有效的绝对URL。然后，我们将使用`yield`请求。
- en: Save it and then go to the console.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存它，然后去控制台。
- en: Then, we change the directory to `basic_crawler`, run it with `scrapy crawl
    basic_crawler -t json -o test.json`, and then press *Enter*.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将目录更改为`basic_crawler`，用`scrapy crawl basic_crawler -t json -o test.json`运行它，然后按*Enter*。
- en: 'We can see that it is working now. We are recursively crawling and scraping
    all of the pages in the website:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到它现在正在工作。我们正在递归地爬行和抓取网站中的所有页面：
- en: '![](img/00033.jpeg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00033.jpeg)'
- en: This could take a long time, so we cancel by pressing *Ctrl* + *C* and we'll
    get the file with the results up to this point.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要很长时间，所以我们按*Ctrl* + *C*取消，然后我们将得到到目前为止的结果文件。
- en: Let's open the `test.json` file with the `vi test.json` command.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用`vi test.json`命令打开`test.json`文件。
- en: 'As we can see in the following screenshot, we have a lot of book titles for
    multiple pages:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在下面的截图中看到的，我们有很多书名，来自多个页面：
- en: '![](img/00034.jpeg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00034.jpeg)'
- en: Congratulations! We have built a web application crawler.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！我们已经建立了一个Web应用程序爬虫。
- en: Think about all the tasks you can automate now.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 想想现在你可以自动化的所有任务。
- en: Scraping interesting stuff
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抓取有趣的东西
- en: In this section, we'll take a look at how to extract other interesting information
    such as emails, forms, and comments that will be useful for our security analysis.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将看看如何提取其他有趣的信息，比如电子邮件、表单和评论，这些对我们的安全分析很有用。
- en: We've added recursive capabilities to our crawler, so now we are ready to add
    more features. In this case, we'll be adding some extraction capabilities for
    emails because it is always useful to have a valid account, which could be handy
    during our tests. Forms will be useful where there's information being submitted
    from the browser to the application. Comments could provide interesting information,
    which developers may have left in production without realizing.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为我们的爬虫添加了递归功能，所以现在我们准备添加更多功能。在这种情况下，我们将为电子邮件添加一些提取功能，因为拥有一个有效的账户总是很有用的，在我们的测试过程中可能会派上用场。表单将在从浏览器提交信息到应用程序的地方很有用。评论可能提供有趣的信息，开发人员可能在生产中留下了这些信息而没有意识到。
- en: 'There is more stuff that you can obtain from web applications but these are
    usually the most useful:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从Web应用程序中可以获得更多的东西，但这些通常是最有用的：
- en: 'First, let''s add these fields into our item. Open the `items.py` file in Atom
    and add the following code:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们将这些字段添加到我们的item中。在Atom中打开`items.py`文件并添加以下代码：
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will be used to indicate where the information was found.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这将用于指示信息的来源。
- en: 'So, let''s get back to the `spiderman.py` file. Let''s copy a prepared `spicderman.py`
    file. We''ll copy `examples/spiders/spiderman-c.py` to `basic_crawler/basic_crawler/spiders/spiderman.py`:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所以，让我们回到`spiderman.py`文件。我们将复制一个准备好的`spicderman.py`文件。我们将`examples/spiders/spiderman-c.py`复制到`basic_crawler/basic_crawler/spiders/spiderman.py`：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Let's go back to the editor.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们回到编辑器。
- en: 'In order to extract emails, we need to add the highlighted code to our `spiderman.py`
    file:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了提取电子邮件，我们需要将突出显示的代码添加到我们的`spiderman.py`文件中：
- en: '![](img/00035.jpeg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00035.jpeg)'
- en: This selector could yield some false positives as it will extract any word that
    contains an `@` symbol, as well as the loop to store the results detected by the
    selector into our items.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这个选择器可能会产生一些误报，因为它会提取任何包含`@`符号的单词，以及将选择器检测到的结果存储到我们的item中的循环。
- en: And that's it, with the code, we'll now extract all the email addresses we find
    while crawling.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样，有了这段代码，我们现在将提取我们在爬行过程中发现的所有电子邮件地址。
- en: 'Now, we need to do the same to extract the `forms` actions. The cross path
    will get the action attribute for the forms, which points to the page that will
    process the data submitted by the user. Then, we iterate over the findings and
    add it to the `items.py` file:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要做同样的事情来提取`forms`操作。交叉路径将获取表单的操作属性，该属性指向将处理用户提交的数据的页面。然后，我们遍历发现的内容并将其添加到`items.py`文件中：
- en: '![](img/00036.jpeg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00036.jpeg)'
- en: That's it for forms.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 表单就是这样。
- en: 'Now, let''s do the same for code `comments`. We''ll create the extractor, and
    again, iterate over the results and add it to the items. Now, we can run the crawler
    and see the results:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们对`comments`代码做同样的操作。我们将创建提取器，并再次迭代结果并将其添加到项目中。现在，我们可以运行爬虫并查看结果：
- en: '![](img/00037.jpeg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00037.jpeg)'
- en: Let's go back to the Terminal, and in `basic_crawler`, we'll type `scrapy crawl
    basic_crawler -o results.json -t json` and hit *Enter*.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到终端，在`basic_crawler`中，我们将输入`scrapy crawl basic_crawler -o results.json
    -t json`并按*Enter*。
- en: It will take a long time to finish the crawling. We'll stop it by pressing *CTRL*
    + *C* after a while.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 完成爬行将需要很长时间。过一会儿我们将按*CTRL* + *C*来停止它。
- en: 'Once it is finished, we can open up `results.json` with the Atom editor and
    inspect the results:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们可以用Atom编辑器打开`results.json`并检查结果：
- en: '![](img/00038.jpeg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00038.jpeg)'
- en: Congratulations! You've extended the crawler to extract interesting information
    about a website.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已经扩展了爬虫，以提取有关网站的有趣信息。
- en: You can see the results, form, comments, and so on. I suggest you look at other
    ways on how to deal with the results such as passing them or storing them into
    SQLite or MongoDB.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以查看结果、表单、评论等。我建议您查看其他处理结果的方法，例如传递它们或将它们存储到SQLite或MongoDB中。
- en: Congratulations! You have created your first web crawler using Python.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已经使用Python创建了您的第一个Web爬虫。
- en: Summary
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we saw what web application mapping is. We learned how to create
    a basic web application crawler. In this chapter, we added recursion capabilities
    and also learned how to make our crawler recursive.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了什么是Web应用程序映射。我们学会了如何创建基本的Web应用程序爬虫。在本章中，我们添加了递归功能，并学会了如何使我们的爬虫递归。
- en: Finally, we learned how to develop a web application crawler using Python and
    the Scrapy library. This will be useful for mapping the web application structure
    and to harvest interesting information such as forms, emails, and comments from
    the source code of the pages.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们学会了如何使用Python和Scrapy库开发Web应用程序爬虫。这对于映射Web应用程序结构和从页面源代码中收集表单、电子邮件和评论等有趣信息将非常有用。
- en: Now, we know how to map a web application using a crawler, but most of the applications
    have hidden resources. These resources are not accessible for all the users or
    are not linked by all. Luckily, we can use the brute force technique to discover
    directories, files, or parameters in order to find vulnerabilities or interesting
    information that we can use in our tests.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们知道如何使用爬虫映射Web应用程序，但大多数应用程序都有隐藏的资源。这些资源对所有用户不可访问，或者并非所有用户都链接。幸运的是，我们可以使用暴力攻击技术来发现目录、文件或参数，以找到我们可以在测试中使用的漏洞或有趣信息。
- en: In [Chapter 4](part0049.html#1ENBI0-5a228e2885234f4ba832bb786a6d0c80), *Resources
    Discovery*, we'll write a tool to perform brute force attacks in different parts
    of the web application.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](part0049.html#1ENBI0-5a228e2885234f4ba832bb786a6d0c80)中，*资源发现*，我们将编写一个工具，在Web应用程序的不同部分执行暴力攻击。
