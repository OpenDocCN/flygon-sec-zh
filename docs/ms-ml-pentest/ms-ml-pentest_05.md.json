["```py\n$ wget --no-check-certificate https://mcfp.felk.cvut.cz/publicDatasets/CTU-13-Dataset/CTU-13-Dataset.tar.bz2\n```", "```py\n# tar xvjf  CTU-13-Dataset.tar.bz2\n```", "```py\n>>> import pandas as pd\n>>> data = pd.read_csv(\"capture20110816-3.binetflow\")\n>>> data['Label'] = data.Label.str.contains(\"Botnet\")\n```", "```py\n>> data.columns\n```", "```py\nfrom __future__ import division\nimport os, sys\nimport threading\n```", "```py\nclass Prepare(threading.Thread): \ndef __init__(self, X, Y, XT, YT, accLabel=None):\n threading.Thread.__init__(self)\n self.X = X\n self.Y = Y\n self.XT=XT\n self.YT=YT\n self.accLabel= accLabel\n\ndef run(self):\n X = np.zeros(self.X.shape)\n Y = np.zeros(self.Y.shape)\n XT = np.zeros(self.XT.shape)\n YT = np.zeros(self.YT.shape)\n np.copyto(X, self.X)\n np.copyto(Y, self.Y)\n np.copyto(XT, self.XT)\n np.copyto(YT, self.YT)\n for i in range(9):\n X[:, i] = (X[:, i] - X[:, i].mean()) / (X[:, i].std())\n for i in range(9):\n XT[:, i] = (XT[:, i] - XT[:, i].mean()) / (XT[:, i].std())\n```", "```py\nimport LoadData\nimport DataPreparation\nimport pickle\nfile = open('flowdata.pickle', 'rb')\ndata  = pickle.load(file)\n```", "```py\nXdata = data[0]\nYdata =  data[1]\nXdataT = data[2]\nYdataT = data[3]\n```", "```py\nfrom sklearn.linear_model import *\nfrom sklearn.tree import *\nfrom sklearn.naive_bayes import *\nfrom sklearn.neighbors import *\n```", "```py\n>>> DataPreparation.Prepare(Xdata,Ydata,XdataT,YdataT)\n```", "```py\n>>> clf = DecisionTreeClassifier()\n>>> clf.fit(Xdata,Ydata)\n>>> Prediction = clf.predict(XdataT)\n>>> Score = clf.score(XdataT,YdataT)\n>>> print (\u201cThe Score of the Decision Tree Classifier is\u201d, Score * 100)\n```", "```py\n>>> clf = LogisticRegression(C=10000)\n>>> clf.fit(Xdata,Ydata)\n>>> Prediction = clf.predict(XdataT) >>> Score = clf.score(XdataT,YdataT)\n```", "```py\n>>> print (\"The Score of the Logistic Regression Classifier is\", Score * 100)\n```", "```py\n>>> clf = GaussianNB()\n>>> clf.fit(Xdata,Ydata)\n>>> Prediction = clf.predict(XdataT)\n>>> Score = clf.score(XdataT,YdataT)\n>>> print(\"The Score of the Gaussian Naive Bayes classifier is\", Score * 100)\n```", "```py\n>>> clf = KNeighborsClassifier()\n>>> clf.fit(Xdata,Ydata)\n>>> Prediction = clf.predict(XdataT)\n>>> Score = clf.score(XdataT,YdataT)\n>>> print(\"The Score of the K-Nearest Neighbours classifier is\", Score * 100)\n```", "```py\n>>> from keras.models import *\n>>> from keras.layers import Dense, Activation\n>>> from keras.optimizers import *\n\nmodel = Sequential()\nmodel.add(Dense(10, input_dim=9, activation=\"sigmoid\")) model.add(Dense(10, activation='sigmoid'))\nmodel.add(Dense(1))\nsgd = SGD(lr=0.01, decay=0.000001, momentum=0.9, nesterov=True) \nmodel.compile(optimizer=sgd, loss='mse')\nmodel.fit(Xdata, Ydata, nb_epoch=200, batch_size=100)\nScore = model.evaluate(XdataT, YdataT, verbose=0)\nPrint(\u201cThe Score of the Neural Network is\u201d, Score * 100  )\n```", "```py\n>>> import pandas as pd\n>>> import numpy as np\n>>> import seaborn\n```", "```py\n>>> data = pd.read_csv('training_data_2_csv_UTF.csv')\n>>> Bots = data[data.bot==1]\n>> NonBots = data[data.bot==0]\n```", "```py\n>>> data = np.random.multivariate_normal([0, 0], [[5, 2], [2, 2]], size=2000)\n>>> data = pd.DataFrame(data, columns=['x', 'y'])\n>>> for col in 'xy':\n... seaborn.kdeplot(data[col], shade=True)\n```", "```py\nmatplotlib.pyplot.figure(figsize=(10,6))\n seaborn.heatmap(data.isnull(), yticklabels=False, cbar=False, cmap='viridis')\n matplotlib.pyplot.tight_layout()\n```", "```py\nbag_of_words_bot = r'bot|b0t|cannabis|tweet me|mishear|follow me|updates every|gorilla|yes_ofc|forget' \\\nr'expos|kill|bbb|truthe|fake|anony|free|virus|funky|RNA|jargon' \\                 r'nerd|swag|jack|chick|prison|paper|pokem|xx|freak|ffd|dunia|clone|genie|bbb' \\                r'ffd|onlyman|emoji|joke|troll|droop|free|every|wow|cheese|yeah|bio|magic|wizard|face'\n```", "```py\ndata['screen_name_binary'] = data.screen_name.str.contains(bag_of_words_bot, case=False, na=False)\ndata['name_binary'] = data.name.str.contains(bag_of_words_bot, case=False, na=False)\ndata['description_binary'] = data.description.str.contains(bag_of_words_bot, case=False, na=False)\ndata['status_binary'] = data.status.str.contains(bag_of_words_bot, case=False, na=False)\n```", "```py\ndata['listed_count_binary'] = (data.listed_count>20000)==False\n features = ['screen_name_binary', 'name_binary', 'description_binary', 'status_binary', 'verified', 'followers_count', 'friends_count', 'statuses_count', 'listed_count_binary', 'bot']\n```", "```py\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\nfrom sklearn.model_selection import train_test_split\n```", "```py\n X = data[features].iloc[:,:-1]\n y = data[features].iloc[:,-1]\n```", "```py\nclf = DecisionTreeClassifier(criterion='entropy', min_samples_leaf=50, min_samples_split=10)\n```", "```py\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)\n```", "```py\nclf.fit(X_train, y_train)\ny_pred_train = clf.predict(X_train)\ny_pred_test = clf.predict(X_test)\n```", "```py\nprint(\"Training Accuracy: %.5f\" %accuracy_score(y_train, y_pred_train))\nprint(\"Test Accuracy: %.5f\" %accuracy_score(y_test, y_pred_test))\n```"]