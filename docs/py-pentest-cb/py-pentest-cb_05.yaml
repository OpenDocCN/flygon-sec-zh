- en: Web Scraping with Scrapy and BeautifulSoup
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scrapy和BeautifulSoup进行网络抓取
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Web spiders with Scrapy
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Scrapy的网络蜘蛛
- en: Scrapy shell
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scrapy shell
- en: Linking the extractor with Scrapy
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将提取器与Scrapy链接起来
- en: Scraping after logging into websites using Scrapy
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Scrapy登录网站后进行抓取
- en: Introduction
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: '**Scrapy** is one of the most powerful Python web-crawling frameworks, and
    it can help with a lot of basic functionalities for efficiently scraping web pages.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**Scrapy**是最强大的Python网络爬虫框架之一，它可以帮助高效地抓取网页的许多基本功能。'
- en: Web spiders with Scrapy
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scrapy的网络蜘蛛
- en: Web spidering starts with a URL or a list of URLs to visit, and when the spider
    gets a new page, it analyzes the page to identify all the hyperlinks, adding these
    links to the list of URLs to be crawled. This action continues recursively for
    as long as new data is found.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 网络蜘蛛从要访问的URL或URL列表开始，当蜘蛛获取新页面时，它会分析页面以识别所有超链接，并将这些链接添加到要爬行的URL列表中。只要发现新数据，这个动作就会递归地继续下去。
- en: A web spider can find new URLs and index them for crawling or download useful
    data from them. In the following recipe, we will use Scrapy to create a web spider.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 网络蜘蛛可以找到新的URL并对其进行索引以进行爬行，或者从中下载有用的数据。在下面的示例中，我们将使用Scrapy创建一个网络蜘蛛。
- en: Getting ready
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We can start by installing Scrapy. It can be installed from Python''s `pip`
    command:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从Python的“pip”命令安装Scrapy：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Make sure that you have the required permission for installing Scrapy. If any
    errors occur with the permission, use the `sudo` command.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您有安装Scrapy所需的权限。如果权限出现任何错误，请使用“sudo”命令。
- en: How to do it...
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s create a simple spider with the Scrapy:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用Scrapy创建一个简单的蜘蛛：
- en: 'For creating a new spider project, open the Terminal and go to the folder for
    our spider:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建一个新的蜘蛛项目，请打开终端并转到我们的蜘蛛所在的文件夹：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then run the following command to create a new spider project with `scrapy`:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后运行以下命令创建一个带有“scrapy”的新蜘蛛项目：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will create a project with the name `books` and some useful files for
    creating the crawler. Now you have a folder structure, as shown in the following
    screenshot:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个名为“books”的项目，并创建一些有用的文件来创建爬虫。现在你有了一个文件夹结构，如下面的截图所示：
- en: '![](img/00011.jpeg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00011.jpeg)'
- en: 'Now we can create a crawler with the following command:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以使用以下命令创建一个爬虫：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This will generate the code for the spider with the name `home`, as we are
    planning to spider the home page of `books.toscrape.com`. Now the folder structure
    inside the `spiders` folder will be as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成名为“home”的蜘蛛的代码，因为我们计划爬取“books.toscrape.com”的主页。现在“spiders”文件夹内的文件夹结构将如下所示：
- en: '![](img/00012.jpeg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00012.jpeg)'
- en: 'As you can see, there is a file named `home.py` inside the `spiders` folder.
    We can open the `home.py` and start editing it. The `home.py` files will have
    the following code:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如您所见，“spiders”文件夹内有一个名为“home.py”的文件。我们可以打开“home.py”并开始编辑它。“home.py”文件将包含以下代码：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`HomeSpider` is a subclass of `scrapy.spider`. The name is set as `home`, which
    we provided while generating the spider. The `allowed_domains` property defines
    the authorized domains for this crawler and `start_urls` defines the URLs for
    the crawler to start with.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`HomeSpider`是`scrapy.spider`的子类。名称设置为“home”，这是我们在生成蜘蛛时提供的。`allowed_domains`属性定义了此爬虫的授权域，`start_urls`定义了爬虫要开始的URL。'
- en: As the name suggests, the `parse` method parses the content of the accessed
    URLs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，“parse”方法解析了所访问的URL的内容。
- en: 'Try running the spider with the following command:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用以下命令运行蜘蛛：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now we can rewrite the spider to navigate through the pagination links:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以重写蜘蛛以浏览分页链接：
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: To navigate through many pages, we can use a subclass of `CrawlSpider`. Import
    the `CrawlSpider` and `Rule` module from `scrapy.spider`. For extracting links,
    we can use `LinkExtractor` from `scrapy.linkextractors`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要浏览多个页面，我们可以使用“CrawlSpider”的子类。从“scrapy.spider”导入“CrawlSpider”和“Rule”模块。对于提取链接，我们可以使用“scrapy.linkextractors”中的“LinkExtractor”。
- en: 'Then we have to set the `rules` variable, which is used to set the rule for
    navigating through the pages. Here, we used the `restrict_css` parameter to set
    the `css` class to get to the next page. The `css` class for the next page''s
    URL can be found by inspecting the web page from the browser, as shown in the
    following screenshot:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要设置“rules”变量，用于设置通过页面的规则。在这里，我们使用“restrict_css”参数来设置“css”类以到达下一页。可以通过在浏览器中检查网页来找到下一页URL的“css”类，如下面的截图所示：
- en: '![](img/00013.jpeg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00013.jpeg)'
- en: 'Now check the crawler by running it with the following command:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过以下命令运行爬虫来检查爬虫：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This will print all the URLs that the spider parsed.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出蜘蛛解析的所有URL。
- en: 'Let''s rewrite the script to get the book `title` and the `price`. For this,
    we have to create a class for our item, so inside the `book` project we will create
    another file with the name `item.py` and define our item to extract:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们重写脚本以获取书籍的“标题”和“价格”。为此，我们必须为我们的项目创建一个类，因此在“book”项目内，我们将创建另一个名为“item.py”的文件，并定义我们要提取的项目：
- en: '[PRE8]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here, we define a new class with the details we expect to extract with our
    spider. Now the folder structure will be as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了一个新类，其中包含我们希望从我们的蜘蛛中提取的细节。现在文件夹结构将如下所示：
- en: '![](img/00014.jpeg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00014.jpeg)'
- en: 'Then, update the `spider/home.py` file to extract the data:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，更新“spider/home.py”文件以提取数据：
- en: '[PRE9]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Update the `parse_page` method to extract the `title` and `price` details from
    each page. To extract the data from the page, we have to use a selector. Here,
    we used the `xpath` selector. XPath is a common syntax or language that is used
    to navigate through XML and HTML documents.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 更新“parse_page”方法以从每个页面提取“标题”和“价格”详情。要从页面中提取数据，我们必须使用选择器。在这里，我们使用了“xpath”选择器。XPath是一种常用的语法或语言，用于浏览XML和HTML文档。
- en: In the `parse_page` method, initially, we selected all the article tags in which
    the book details are placed on the website and iterated through each article tag
    to parse the titles and prices of the books.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在`parse_page`方法中，最初，我们选择了网站上放置书籍详细信息的所有文章标签，并遍历每个文章标签以解析书籍的标题和价格。
- en: 'To get the `xpath` selector for a tag, we can use the Google Chrome browser''s
    XPath tool as follows:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要获取标签的`xpath`选择器，我们可以使用谷歌Chrome浏览器的XPath工具，如下所示：
- en: '![](img/00015.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00015.jpeg)'
- en: 'We can use Firefox Inspector as following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Firefox Inspector如下：
- en: '![](img/00016.jpeg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00016.jpeg)'
- en: 'Now we can run the spider to extract the data to a `.csv` file:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以运行爬虫，将数据提取到`.csv`文件中：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This will create a file named `book-data.csv` in the current directory containing
    the extracted details.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在当前目录中创建一个名为`book-data.csv`的文件，其中包含提取的详细信息。
- en: You can learn more about selectors such as XPath and how to select details from
    a page at [https://doc.scrapy.org/en/latest/topics/selectors.html](https://doc.scrapy.org/en/latest/topics/selectors.html).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://doc.scrapy.org/en/latest/topics/selectors.html](https://doc.scrapy.org/en/latest/topics/selectors.html)了解有关选择器（如XPath）以及如何从页面中选择详细信息的更多信息。
- en: Scrapy shell
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scrapy shell
- en: Scrapy shell is a command-line interface that helps to debug scripts without
    running the entire crawler. We have to provide a URL, and Scrapy shell will open
    up an interface to interact with objects that the spider handles in its callbacks,
    such as a response object.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy shell是一个命令行界面，可帮助调试脚本而无需运行整个爬虫。我们必须提供一个URL，Scrapy shell将打开一个接口，与爬虫在其回调中处理的对象进行交互，例如响应对象。
- en: How to do it...
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We can go through some simple usage of Scrapy''s interactive shell. The steps
    are as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过一些简单的Scrapy交互式shell用法。步骤如下：
- en: 'Open up a Terminal window and type the following command:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个终端窗口，然后输入以下命令：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After loading the Scrapy shell, it will open up an interface to interact with
    the response object as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 加载Scrapy shell后，它将打开一个接口，与响应对象进行交互，如下所示：
- en: '![](img/00017.jpeg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00017.jpeg)'
- en: 'We can use this interface to debug the selectors for the `response` object:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用这个接口来调试`response`对象的选择器：
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This will print the selector output. With this, we can create and test the extraction
    rules for spiders.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印选择器输出。有了这个，我们可以创建和测试爬虫的提取规则。
- en: 'We can also open the Scrapy shell from the code for debugging errors in extraction
    rules. For that, we can use the `inspect_response` method:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以从代码中打开Scrapy shell以调试提取规则中的错误。为此，我们可以使用`inspect_response`方法：
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This will open up a shell interface if the condition fails. Here, we have imported
    `inspect_response` and used it to debug the spider from the code.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果条件失败，这将打开一个shell接口。在这里，我们已经导入了`inspect_response`并使用它来从代码中调试爬虫。
- en: Link extractor with Scrapy
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scrapy的链接提取器
- en: As their name indicates, link extractors are the objects that are used to extract
    links from the Scrapy response object. Scrapy has built-in link extractors, such
    as `scrapy.linkextractors`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正如它们的名称所示，链接提取器是用于从Scrapy响应对象中提取链接的对象。Scrapy具有内置的链接提取器，例如`scrapy.linkextractors`。
- en: How to do it...
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Let''s build a simple link extractor with Scrapy:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用Scrapy构建一个简单的链接提取器：
- en: As we did for the previous recipe, we have to create another spider for getting
    all the links.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与上一个示例一样，我们必须创建另一个spider来获取所有链接。
- en: 'In the new `spider` file, import the required modules:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在新的`spider`文件中，导入所需的模块：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Create a new `spider` class and initialize the variables:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的`spider`类并初始化变量：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we have to initialize the rule for crawling the URL:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们必须初始化爬取URL的规则：
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This rule orders the extraction of all unique and canonicalized links, and also
    instructs the program to follow those links and parse them using the `parse_page`
    method
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此规则命令提取所有唯一和规范化的链接，并指示程序跟随这些链接并使用`parse_page`方法解析它们
- en: 'Now we can start the spider using the list of URLs listed in the `start_urls`
    variable:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以使用`start_urls`变量中列出的URL列表启动spider：
- en: '[PRE17]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `start_requests()` method is called once when the spider is opened for scraping
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`start_requests()`方法在打开爬虫进行爬取时调用一次'
- en: 'Now we can write the method to parse the URLs:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以编写解析URL的方法：
- en: '[PRE18]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This method extracts all canonicalized and unique links with respect to the
    current response. It also verifies that the domain of the URL of the link is in
    one of the authorized domains.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法提取相对于当前响应的所有规范化和唯一的链接。它还验证链接的URL的域是否属于授权域中的一个。
- en: Scraping after logging into websites using Scrapy
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scrapy登录网站后进行爬取
- en: There are situations where we have to log into websites to access the data we
    are planning to extract. With Scrapy, we can handle the login forms and cookies
    easily. We can make use of Scrapy's `FormRequest` object; it will deal with the
    login form and try to log in with the credentials provided.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们必须登录网站才能访问我们计划提取的数据。使用Scrapy，我们可以轻松处理登录表单和cookies。我们可以利用Scrapy的`FormRequest`对象；它将处理登录表单并尝试使用提供的凭据登录。
- en: Getting ready
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: When we visit a website that has authentication, we need a username and password.
    In Scrapy, we need the same credentials to log in. So we need to get an account
    for the website that we plan to scrape.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们访问一个需要身份验证的网站时，我们需要用户名和密码。在Scrapy中，我们需要相同的凭据来登录。因此，我们需要为我们计划抓取的网站获取一个帐户。
- en: How to do it...
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Here is how we can use Scrapy to crawl websites which require logging in:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们如何使用Scrapy来爬取需要登录的网站：
- en: 'To use the `FormRequest` object, we can update the `parse_page` method as follows:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要使用`FormRequest`对象，我们可以按如下方式更新`parse_page`方法：
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Here, the response object is the HTTP response of the page where we have to
    fill in the login form. The `FormRequest` method includes the credentials that
    we need to log in and the `callback` method that is used to parse the page after
    login.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，响应对象是我们需要填写登录表单的页面的HTTP响应。`FormRequest`方法包括我们需要登录的凭据以及用于登录后解析页面的`callback`方法。
- en: To paginate after logging in while preserving the logged-in session, we can
    use the method we used in the previous recipe.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在保持登录会话的情况下进行分页，我们可以使用前面一篇食谱中使用的方法。
