["```py\npip install scrapy   \n```", "```py\n$ mkdir new-spider\n$ cd new-spider  \n```", "```py\n$ scrapy startproject books  \n```", "```py\n$ scrapy genspider home books.toscrape.com  \n```", "```py\n# -*- coding: utf-8 -*- \nimport scrapy \nclass HomeSpider(scrapy.Spider): \n    name = 'home' \n    allowed_domains = ['books.toscrape.com'] \n    start_urls = ['http://books.toscrape.com/'] \n    def parse(self, response): \n        pass \n```", "```py\n$ scrapy crawl home    \n```", "```py\nfrom scrapy.spiders import CrawlSpider, Rule \nfrom scrapy.linkextractors import LinkExtractor  \nclass HomeSpider(CrawlSpider): \n    name = 'home' \n    allowed_domains = ['books.toscrape.com'] \n    start_urls = ['http://books.toscrape.com/'] \n    rules = (Rule(LinkExtractor(allow=(), restrict_css=('.next',)), \n             callback=\"parse_page\", \n             follow=True),)  \n    def parse_page(self, response): \n        print(response.url) \n```", "```py\n$ scrapy crawl home  \n```", "```py\nfrom scrapy.item import Item, Field \nclass BookItem(Item): \n    title = Field() \n    price = Field() \n```", "```py\nfrom scrapy.spiders import CrawlSpider, Rule \nfrom scrapy.linkextractors import LinkExtractor \nfrom books.item import BookItem \nclass HomeSpider(CrawlSpider): \n    name = 'home' \n    allowed_domains = ['books.toscrape.com'] \n    start_urls = ['http://books.toscrape.com/'] \n    rules = (Rule(LinkExtractor(allow=(), restrict_css=('.next',)), \n             callback=\"parse_page\", \n             follow=True),) \n    def parse_page(self, response): \n        items = [] \n        books = response.xpath('//ol/li/article') \n        index = 0 \n        for book in books: \n            item = BookItem() \n            title = books.xpath('//h3/a/text()')[index].extract() \n            item['title'] = str(title).encode('utf-8').strip() \n            price = books.xpath('//article/div[contains(@class, \"product_price\")]/p[1]/text()')[index].extract() \n            item['price'] = str(price).encode('utf-8').strip() \n            items.append(item) \n            index += 1 \n            yield item \n```", "```py\n$ scrapy crawl home -o book-data.csv -t csv   \n```", "```py\n$ Scrapy shell http://books.toscrape.com/  \n```", "```py\n>>> response.xpath('//ol/li/article')  \n```", "```py\nfrom scrapy.spiders import CrawlSpider, Rule \nfrom scrapy.linkextractors import LinkExtractor \nfrom scrapy.shell import inspect_response  \nclass HomeSpider(CrawlSpider): \n    name = 'home' \n    allowed_domains = ['books.toscrape.com'] \n    start_urls = ['http://books.toscrape.com/'] \n    rules = (Rule(LinkExtractor(allow=(), restrict_css=('.next',)), \n             callback=\"parse_page\", \n             follow=True),)  \n    def parse_page(self, response): \n        if len(response.xpath('//ol/li/article')) < 5: \n            title = response.xpath('//h3/a/text()')[0].extract() \n            print(title) \n        else: \n            inspect_response(response, self) \n```", "```py\nimport scrapy \nfrom scrapy.linkextractor import LinkExtractor \nfrom scrapy.spiders import Rule, CrawlSpider  \n```", "```py\nclass HomeSpider2(CrawlSpider): \n    name = 'home2' \n    allowed_domains = ['books.toscrape.com'] \n    start_urls = ['http://books.toscrape.com/']\n```", "```py\nrules = [ \n    Rule( \n        LinkExtractor( \n            canonicalize=True, \n            unique=True \n        ), \n        follow=True, \n        callback=\"parse_page\" \n    ) \n]   \n```", "```py\ndef start_requests(self): \n    for url in self.start_urls: \n        yield scrapy.Request(url, callback=self.parse, dont_filter=True)  \n```", "```py\ndef parse_page(self, response): \n    links = LinkExtractor(canonicalize=True, unique=True).extract_links(response) \n        for link in links: \n            is_allowed = False \n            for allowed_domain in self.allowed_domains: \n                if allowed_domain in link.url: \n                    is_allowed = True \n            if is_allowed: \n                print link.url \n```", "```py\ndef parse(self, response): \n    return scrapy.FormRequest.from_response( \n        response, \n        formdata={'username': 'username', 'password': 'password'}, \n        callback=self.parse_after_login \n     ) \n```"]