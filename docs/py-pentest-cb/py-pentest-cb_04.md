# 第四章：使用 Python 进行数据解析

在本章中，我们将涵盖以下示例：

+   解析 HTML 表格

+   从 HTML 文档中提取数据

+   解析 XML 数据

# 介绍

由于我们已经在之前的示例中下载了网页，现在我们可以讨论如何处理这些文件并解析它们以获取所需的信息。

# 解析 HTML 表格

从服务器下载 HTML 页面后，我们必须从中提取所需的数据。Python 中有许多模块可以帮助我们做到这一点。在这里，我们可以使用 Python 包`BeautifulSoup`。

# 准备工作

和往常一样，确保你安装了所有必需的包。对于这个脚本，我们需要`BeautifulSoup`和`pandas`。你可以使用`pip`安装它们：

```py
pip install bs4 
pip install pandas  
```

`pandas`是 Python 中的一个开源数据分析库。

# 操作步骤...

我们可以从下载的页面中解析 HTML 表格，如下所示：

1.  和往常一样，我们必须导入脚本所需的模块。在这里，我们导入`BeautifulSoup`来解析 HTML 和`pandas`来处理解析的数据。此外，我们还必须导入`urllib`模块以从服务器获取网页：

```py
import urllib2 
import pandas as pd 
from bs4 import BeautifulSoup 
```

1.  现在我们可以从服务器获取 HTML 页面；为此，我们可以使用`urllib`模块：

```py
url = "https://www.w3schools.com/html/html_tables.asp" 
try: 
    page = urllib2.urlopen(url) 
except Exception as e: 
    print e 
    pass 
```

1.  然后，我们可以使用`BeautifulSoup`来解析 HTML 并从中获取`table`：

```py
soup = BeautifulSoup(page, "html.parser") 
table = soup.find_all('table')[0] 
```

在这里，它将获取网页上的第一个表格。

1.  现在我们可以使用`pandas`库为表格创建一个`DataFrame`：

```py
new_table = pd.DataFrame(columns=['Company', 'Contact', 'Country'], index=range(0, 7)) 
```

这将创建一个具有三列和六行的`DataFrame`。列将显示公司名称、联系方式和国家。

1.  现在我们必须解析数据并将其添加到`DataFrame`中：

```py
row_number = 0 
for row in table.find_all('tr'): 
    column_number = 0 
    columns = row.find_all('td') 
    for column in columns: 
        new_table.iat[row_number, columns_number] = column.get_text() 
        columns_number += 1 
    row_number += 1  
print new_table 
```

这将打印`DataFrame`。

`DataFrame`是一个二维的、带标签的数据结构，具有可能不同类型的列。它更像是`dict`的系列对象。

1.  这个脚本可以在 Python 3 中运行，需要做一些更改，如下所示：

```py
import urllib.request 
import pandas as pd 
from bs4 import BeautifulSoup  
url = "https://www.w3schools.com/html/html_tables.asp" 
try: 
    page = urllib.request.urlopen(url) 
except Exception as e: 
    print(e) 
    pass 
soup = BeautifulSoup(page, "html.parser")  
table = soup.find_all('table')[0]  
new_table = pd.DataFrame(columns=['Company', 'Contact', 'Country'], index=range(0, 7))  
row_number = 0 
for row in table.find_all('tr'): 
    column_number = 0 
    columns = row.find_all('td') 
    for column in columns: 
        new_table.iat[row_number, column_number] = column.get_text() 
        column_number += 1 
    row_number += 1  
print(new_table) 
```

主要的更改是对`urllib`模块和`print`语句的修改。

你可以在[`pandas.pydata.org/pandas-docs/stable/`](https://pandas.pydata.org/pandas-docs/stable/)了解更多关于`pandas`数据分析工具包的信息。

# 从 HTML 文档中提取数据

我们可以使用`pandas`库将解析的数据提取到.csv 或 Excel 格式。

# 准备工作

要使用`pandas`模块中导出解析数据到 Excel 的函数，我们需要另一个依赖模块`openpyxl`，所以请确保你使用`pip`安装了`openpyxl`：

```py
pip install openpyxl  
```

# 操作步骤...

我们可以将数据从 HTML 提取到.csv 或 Excel 文档中，如下所示：

1.  要创建一个.csv 文件，我们可以使用`pandas`中的`to_csv()`方法。我们可以将上一个示例重写如下：

```py
import urllib.request 
import pandas as pd 
from bs4 import BeautifulSoup  
url = "https://www.w3schools.com/html/html_tables.asp" 
try: 
    page = urllib.request.urlopen(url) 
except Exception as e: 
    print(e) 
    pass 
soup = BeautifulSoup(page, "html.parser")  
table = soup.find_all('table')[0]  
new_table = pd.DataFrame(columns=['Company', 'Contact', 'Country'], index=range(0, 7))  
row_number = 0 
for row in table.find_all('tr'): 
    column_number = 0 
    columns = row.find_all('td') 
    for column in columns: 
        new_table.iat[row_number, column_number] = column.get_text() 
        column_number += 1 
    row_number += 1  
new_table.to_csv('table.csv') 
```

这将创建一个名为`table.csv`的.csv 文件。

1.  同样地，我们可以使用`to_excel()`方法将数据导出到 Excel。

将上一个脚本的最后一行改为以下内容：

```py
new_table.to_excel('table.xlsx') 
```

# 解析 XML 数据

有时，我们会从服务器得到一个 XML 响应，我们需要解析 XML 以提取数据。我们可以使用`xml.etree.ElementTree`模块来解析 XML 文件。

# 准备工作

我们必须安装所需的模块，`xml`：

```py
pip install xml  
```

# 操作步骤...

以下是我们如何使用 XML 模块解析 XML 数据：

1.  首先导入所需的模块。由于这个脚本是在 Python 3 中，确保你导入了正确的模块：

```py
from urllib.request import urlopen 
from xml.etree.ElementTree import parse 
```

1.  现在使用`urllib`模块中的`urlopen`方法获取 XML 文件：

```py
url = urlopen('http://feeds.feedburner.com/TechCrunch/Google') 
```

1.  现在使用`xml.etree.ElementTree`模块中的`parse`方法解析 XML 文件：

```py
xmldoc = parse(url) 
```

1.  现在迭代并打印 XML 中的细节：

```py
for item in xmldoc.iterfind('channel/item'): 
    title = item.findtext('title') 
    desc = item.findtext('description') 
    date = item.findtext('pubDate') 
    link = item.findtext('link')  
    print(title) 
    print(desc) 
    print(date) 
    print(link) 
    print('---------') 
```

1.  这个脚本可以重写为 Python 2 中运行，如下所示：

```py
from urllib2 import urlopen 
from xml.etree.ElementTree import parse  
url = urlopen('http://feeds.feedburner.com/TechCrunch/Google') 
xmldoc = parse(url) 
xmldoc.write('output.xml') 
for item in xmldoc.iterfind('channel/item'): 
   title = item.findtext('title') 
   desc = item.findtext('description') 
   date = item.findtext('pubDate') 
   link = item.findtext('link')  
    print title 
    print desc 
    print date 
    print link 
    print '---------' 
```

这也可以导出到 Excel 或.csv，就像我们在之前的示例中所做的那样。
