["```py\n        import urllib \n\n        url = urllib.urlopen(\"http://packtpub.com/\") \n\n        data = url.read() \n\n        print data \n\n```", "```py\n        import urllib \n\n        fields = { \n          'name' : 'Sean', \n          'email' : 'Sean@example.com' \n        } \n\n        parms = urllib.urlencode(fields) \n        print parms \n\n```", "```py\n        import urllib \n        fields = { \n          'name' : 'Sean', \n          'email' : 'Sean@example.com' \n        } \n        parms = urllib.urlencode(fields) \n        u = urllib.urlopen(\"http://example.com/login?\"+parms) \n        data = u.read() \n\n        print data \n\n```", "```py\n        import urllib \n        fields = { \n          'name' : 'Sean', \n          'email' : 'Sean@example.com' \n        } \n        parms = urllib.urlencode(fields) \n        u = urllib.urlopen(\"http://example.com/login\", parms) \n        data = u.read() \n        print data \n\n```", "```py\n        u = urllib.urlopen(\"http://packtpub.com\", parms) \n        response_headers = u.info() \n        print response_headers \n\n```", "```py\n>>> print response_headers.keys() \n['via', 'x-country-code', 'age', 'expires', 'server',\n        'connection', 'cache-control', 'date', 'content-type']\n\n```", "```py\n>>>print response_headers['server'] \nnginx/1.4.5 \n\n```", "```py\n            u = urllib.urlopen(\"http://packtpub.com\", parms) \n            response_code = u.code \n            print response_code \n\n    ```", "```py\n        headers = { \n         'User-Agent' : 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64;\n        rv:41.0) Gecko/20100101 Firefox/41.0' \n        }\n        request = urllib2.Request(\"http://packtpub.com/\",\n         headers=headers)\n        url = urllib2.urlopen(request)\n        response = url.read()\n\n```", "```py\n        fields = {  \n        'name' : 'sean',  \n        'password' : 'password!',  \n        'login' : 'LogIn'  \n        }  \n\n        # Here we creates a custom opener with cookies enabled \n        opener = urllib2.build_opener(  \n        urllib2.HTTPCookieProcessor()  \n        )  \n\n        # creates request \n        request = urllib2.Request(  \n          \"http://example.com/login\",  \n          urllib.urlencode(fields))  \n\n        # Login request sending \n        url = opener.open(request)  \n        response = url.read()  \n\n        # Now we can access the private pages with the cookie  \n        # got from the above login request \n        url = opener.open(\"http://example.com/dashboard\")  \n        response = url.read() \n\n```", "```py\nimport requests \nresponse = requests.get(\"http://packtpub.com\", parms) \n\n# Response \nprint response.status_code # Response Code   \nprint response.headers # Response Headers   \nprint response.content # Response Content \n\n# Request \nprint response.request.headers # Headers we sent \n\n```", "```py\n         from bs4 import BeautifulSoup  \n\n         parse = BeautifulSoup('<html><head><title>Title of the\n         page</title></head><body><p id=\"para1\" \n         align=\"center\">This is a paragraph<b>one</b><a \n         href=\"http://example1.com\">Example Link 1</a> </p><p \n         id=\"para2\">This is a paragraph<b>two</b><a \n         href=\"http://example2.com\">Example Link 2</a></p></body>\n         </html>')  \n\n         print parse.prettify()  \n\n```", "```py\nparse.contents[0].name\n>>> u'html'\nparse.contents[0].contents[0].name\n>>> u'head'\nhead = soup.contents[0].contents[0]\nhead.parent.name\n>>> u'html'\nhead.next\n>>> <title>Page title</title>\nhead.nextSibling.name\n>>> u'body'\nhead.nextSibling.contents[0]\n>>> <p id=\"para1\" align=\"center\">This is a \n        paragraph<b>one</b><a href=\"http://example1.com\">Example \n        Link 1</a> </p>\nhead.nextSibling.contents[0].nextSibling\n>>> <p id=\"para2\">This is a paragraph<b>two</b><a \n        href=\"http://example2.com\">Example Link 2</a></p> \n\n```", "```py\nparse.find_all('a')\n>>> [<a href=\"http://example1.com\">Example Link 1</a>, <a\n        href=\"http://example2.com\">Example Link 2</a>]\nparse.find(id=\"para2\")\n>>> <p id=\"para2\">This is a paragraph<b>two</b><a \n        href=\"http://example2.com\">Example Link 2</a></p>\n\n```", "```py\n# Importing required modules \nimport requests   \nfrom bs4 import BeautifulSoup   \nimport urlparse #urlparse is renamed to urllib.parse in Python  \n\n# Get the page with the requests \nresponse = requests.get('http://www.freeimages.co.uk/galleries/food/breakfast/index.htm')   \n\n# Parse the page with BeautifulSoup \nparse = BeautifulSoup(response.text) \n\n# Get all image tags \nimage_tags = parse.find_all('img') \n\n# Get urls to the images \nimages = [ url.get('src') for url in image_tags] \n# If no images found in the page \n\nif not images:   \n    sys.exit(\"Found No Images\") \n# Convert relative urls to absolute urls if any \nimages = [urlparse.urljoin(response.url, url) for url in images]   \nprint 'Found %s images' % len(images) \n\n# Download images to downloaded folder \nfor url in images:   \n    r = requests.get(url) \n    f = open('downloaded/%s' % url.split('/')[-1], 'w') \n    f.write(r.content) \n    f.close() \n    print 'Downloaded %s' % url \n\n```", "```py\n#Importing modules \nfrom lxml import html \nimport requests \n\nresponse = requests.get('http://packtpub.com/') \ntree = html.fromstring(response.content) \n\n```", "```py\n<div class=\"book-block-title\" itemprop=\"name\">Book 1</div> \n\n```", "```py\n#Create the list of Books: \n\nbooks = tree.xpath('//div[@class=\"book-block-title\"]/text()') \n\n```", "```py\nprint books \n\n```", "```py\n $ scrapy startproject testSpider\n\n```", "```py\n $ cd testSpider\n\n```", "```py\n $ scrapy genspider pactpub pactpub.com\n\n```", "```py\nfrom scrapy.item import Item, Field \nclass TestspiderItem(Item): \n    # define the fields for your item here: \n    book = Field() \n\n```", "```py\nfrom scrapy.spiders import Spider \nfrom scrapy.selector import Selector \nfrom pprint import pprint \nfrom testSpider.items import TestspiderItem \n\n```", "```py\n# Extend  Spider Class \nclass PactpubSpider(Spider): \n    name = \"pactpub\" \n    allowed_domains = [\"pactpub.com\"] \n    start_urls = ( \n        'https://www.pactpub.com/all', \n    ) \n\n```", "```py\n    # Define parse \n    def parse(self, response): \n        res = Selector(response) \n        items = [] \n        for sel in res.xpath('//div[@class=\"book-block\"]'): \n            item = TestspiderItem() \n            item['book'] = sel.xpath('//div[@class=\"book-block-title\"]/text()').extract() \n            items.append(item) \n        return items \n\n```", "```py\n $ scrapy crawl pactpub --output results.json\n\n```", "```py\n# Importing Modules  \nfrom bs4 import BeautifulSoup \nimport requests \nimport requests.exceptions \nimport urlparse \nfrom collections import deque \nimport re \n\n```", "```py\n# List of urls to be crawled \nurls = deque(['https://www.packtpub.com/']) \n\n```", "```py\n# URLs that we have already crawled \nscraped_urls = set() \n\n```", "```py\n# Crawled emails \nemails = set() \n\n```", "```py\n# Scrape urls one by one queue is empty \nwhile len(urls): \n    # move next url from the queue to the set of Scraped urls \n    url = urls.popleft() \n    scrapped_urls.add(url) \n\n```", "```py\n    # Get  base url \n    parts = urlparse.urlsplit(url) \n    base_url = \"{0.scheme}://{0.netloc}\".format(parts) \n    path = url[:url.rfind('/')+1] if '/' in parts.path else url \n\n```", "```py\n    # get url's content \n    print(\"Scraping %s\" % url) \n    try: \n        response = requests.get(url) \n    except (requests.exceptions.MissingSchema, requests.exceptions.ConnectionError): \n        # ignore  errors \n        continue \n\n```", "```py\n    # Search e-mail addresses and add them into the output set \n    new_emails = set(re.findall(r\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\", response.text, re.I)) \n    emails.update(new_emails) \n\n```", "```py\n    # find and process all the anchors \n    for anchor in soup.find_all(\"a\"): \n        # extract link url \n        link = anchor.attrs[\"href\"] if \"href\" in anchor.attrs else '' \n        # resolve relative links \n        if link.startswith('/'): \n            link = base_url + link \n        elif not link.startswith('http'): \n            link = path + link \n        # add the new url to the queue \n\n        if not link in urls and not link in scraped_urls: \n            urls.append(link) \n\n```", "```py\n>>> from scrapy.all import *\n>>> ans,unans=srloop(IP(dst=\"192.168.1.123\")/TCP(dport=80,flags=\"S\"))\n\n```", "```py\n>>> temp = 0\n>>> for s,r in ans:\n...     temp = r[TCP].seq - temp\n...     print str(r[TCP].seq) + \"\\t+\" + str(temp)\n\n```", "```py\n>>> from scapy.all import *\n>>> from scapy.modules.nmap import *\n>>> conf.nmap_base =\"/usr/share/nmap/nmap-os-db\" \n>>> nmap_fp(\"192.168.1.123\")\n\n```", "```py\n>>> from scapy.all import *\n>>> from scapy.modules.pof import *\n>>> conf.p0f_base =\"/etc/p0f/p0f.fp\"\n>>> conf.p0fa_base =\"/etc/p0f/p0fa.fp\"\n>>> conf.p0fr_base =\"/etc/p0f/p0fr.fp\"\n>>> conf.p0fo_base =\"/etc/p0f/p0fo.fp\"\n>>> sniff(prn=prnp0f) \n\n```", "```py\nimport os,sys \n\nfrom PIL import Image \n\nfrom PIL.ExifTags import TAGS \n\nfor (i,j) in Image.open('image.jpg')._getexif().iteritems(): \n\n        print '%s = %s' % (TAGS.get(i), j) \n\n```"]