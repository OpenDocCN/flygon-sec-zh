- en: Chapter 3. Application Fingerprinting with Python
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。使用Python进行应用指纹识别
- en: One important step during web application security assessment is fingerprinting.
    As a security researcher/pentester, we have to be well-versed at fingerprinting,
    which gives lot of information about underlying technology like software or framework
    version, web server info, OS and many more. This helps us to discover all the
    well-known vulnerabilities that are affecting the application and server.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在Web应用程序安全评估期间的一个重要步骤是指纹识别。作为安全研究人员/渗透测试人员，我们必须精通指纹识别，这可以提供有关底层技术（如软件或框架版本、Web服务器信息、操作系统等）的大量信息。这有助于我们发现影响应用程序和服务器的所有众所周知的漏洞。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Web scraping
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络爬虫
- en: E-mail gathering
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电子邮件收集
- en: OS fingerprinting
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作系统指纹识别
- en: EXIF data extraction
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EXIF数据提取
- en: Application fingerprinting
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用指纹识别
- en: Web scraping
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网络爬虫
- en: Even though some sites offer APIs, most websites are designed mainly for human
    eyes and only provide HTML pages formatted for humans. If we want a program to
    fetch some data from such a website, we have to parse the markup to get the information
    we need. Web scraping is the method of using a computer program to analyze a web
    page and get the data needed.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些网站提供API，但大多数网站主要设计供人类使用，只提供为人类格式化的HTML页面。如果我们想要程序从这样的网站获取一些数据，我们必须解析标记以获取所需的信息。网络爬虫是使用计算机程序分析网页并获取所需数据的方法。
- en: 'There are many methods to fetch the content from the site with Python modules:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以使用Python模块从网站获取内容：
- en: Use `urllib`/`urllib2` to create an HTTP request that will fetch the webpage,
    and using `BeautifulSoup` to parse the HTML
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`urllib`/`urllib2`创建将获取网页的HTTP请求，并使用`BeautifulSoup`解析HTML
- en: To parse an entire website we can use Scrapy ([http://scrapy.org](http://scrapy.org)),
    which helps to create web spiders
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要解析整个网站，我们可以使用Scrapy（[http://scrapy.org](http://scrapy.org)），它有助于创建网络爬虫
- en: Use requests module to fetch and lxml to parse
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用requests模块获取并使用lxml解析
- en: urllib / urllib2 module
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: urllib / urllib2模块
- en: Urllib is a high-level module that allows us to script different services such
    as HTTP, HTTPS, and FTP.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Urllib是一个高级模块，允许我们脚本化不同的服务，如HTTP、HTTPS和FTP。
- en: Useful methods of urllib/urllib2
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: urllib/urllib2的有用方法
- en: 'Urllib/urllib2 provide methods that can be used for getting resources from
    URLs, which includes opening web pages, encoding arguments, manipulating and creating
    headers, and many more. We can go through some of those useful methods as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Urllib/urllib2提供了一些方法，可用于从URL获取资源，包括打开网页，编码参数，操作和创建标头等。我们可以按以下方式使用其中一些有用的方法：
- en: 'Open a web page using `urlopen()`. When we pass a URL to `urlopen()` method,
    it will return an object, we can use the `read()` attribute to get the data from
    this object in string format, as follows:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`urlopen()`打开网页。当我们将URL传递给`urlopen()`方法时，它将返回一个对象，我们可以使用`read()`属性以字符串格式从该对象获取数据，如下所示：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The next method is parameter encoding: `urlencode()`. It takes a dictionary
    of fields as input and creates a URL-encoded string of parameters:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个方法是参数编码：`urlencode()`。它接受字段字典作为输入，并创建参数的URL编码字符串：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The other method is sending requests with parameters, for example, using a
    GET request: URL is crafted by appending the URL-encoded parameters:'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种方法是使用参数发送请求，例如，使用GET请求：URL是通过附加URL编码的参数来构建的：
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Using the POST request method, the URL-encoded parameters are passed to the
    method `urlopen()` separately:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用POST请求方法，URL编码的参数分别传递给方法`urlopen()`：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If we use response headers then the HTTP response headers can be retrieved
    using the `info()` method, which will return a dictionary-like object:'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们使用响应头，那么可以使用`info()`方法检索HTTP响应头，它将返回类似字典的对象：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output will look as follows:'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Useful methods of urllib/urllib2](img/image_03_001.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![urllib/urllib2的有用方法](img/image_03_001.jpg)'
- en: 'We can also use `keys()` to get all the response header keys:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还可以使用`keys()`来获取所有响应头键：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can access each entry as follows:'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以按如下方式访问每个条目：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Urllib does not support cookies and authentication. Also, it only supports GET
    and POST requests. Urllib2 is built upon urllib and has many more features.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Urllib不支持cookies和身份验证。它只支持GET和POST请求。Urllib2是建立在urllib之上的，具有更多功能。
- en: 'We can get the status codes with the code method:'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用code方法获取状态码：
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can modify the request headers with `urllib2` as follows:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用`urllib2`修改请求头，如下所示：
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Cookies can be used as follows:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以如下使用cookies：
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Requests module
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 请求模块
- en: 'We can also use the requests module instead of `urllib`/`urllib2`, which is
    a better option as it supports a fully REST API and it simply takes a dictionary
    as an argument without any parameters encoded:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用requests模块而不是`urllib`/`urllib2`，这是一个更好的选择，因为它支持完全的REST API，并且它只需将字典作为参数而不需要任何编码的参数：
- en: '[PRE10]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Parsing HTML using BeautifulSoup
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用BeautifulSoup解析HTML
- en: The preceding modules are only useful to fetch files. If we want to parse HTML
    obtained via `urlopen`, we have to use the `BeautifulSoup` module. `BeautifulSoup`
    takes raw HTML and XML files from `urlopen` and pulls data out of it. To run a
    parser, we have to create a parser object and feed it some data. It will scan
    through the data and trigger the various handler methods. Beautiful Soup 4 works
    on both Python 2.6+ and Python 3.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的模块只能用于获取文件。如果我们想要解析通过`urlopen`获得的HTML，我们必须使用`BeautifulSoup`模块。`BeautifulSoup`接受来自`urlopen`的原始HTML和XML文件，并从中提取数据。要运行解析器，我们必须创建一个解析器对象并提供一些数据。它将扫描数据并触发各种处理程序方法。Beautiful
    Soup 4适用于Python 2.6+和Python 3。
- en: 'The following are some simple examples:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些简单的示例：
- en: 'To prettify the HTML, use the following code:'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要使HTML格式化，使用以下代码：
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output will be as follows:'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Parsing HTML using BeautifulSoup](img/image_03_004.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![使用BeautifulSoup解析HTML](img/image_03_004.jpg)'
- en: 'Some example ways to navigate through the HTML with `BeautifulSoup` are as
    follows:'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`BeautifulSoup`导航HTML的一些示例方法如下：
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Some ways to search through the HTML for tags and properties are as follows:'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索HTML标签和属性的一些方法如下：
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Download all images on a page
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 下载页面上的所有图像
- en: 'Now we can write a script to download all images on a page and save them in
    a specific location:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编写一个脚本来下载页面上的所有图像，并将它们保存在特定位置：
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Parsing HTML with lxml
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用lxml解析HTML
- en: Another powerful, fast, and flexible parser is the HTML Parser that comes with
    lxml. As lxml is an extensive library written for parsing both XML and HTML documents,
    it can handle messed up tags in the process.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个强大、快速、灵活的解析器是lxml附带的HTML解析器。由于lxml是一个用于解析XML和HTML文档的广泛库，它可以处理过程中混乱的标签。
- en: Let's start with an example.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个例子开始。
- en: 'Here, we will use the requests module to retrieve the web page and parse it
    with lxml:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用requests模块检索网页并用lxml解析它：
- en: '[PRE15]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now the whole HTML is saved to `tree` in a nice tree structure that we can
    inspect in two different ways: XPath or CSS Select. XPath is used to navigate
    through elements and attributes to find information in structured documents such
    as HTML or XML.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在整个HTML保存在`tree`中，以一种良好的树结构，我们可以用两种不同的方式来检查：XPath或CSS选择。XPath用于在结构化文档（如HTML或XML）中导航元素和属性以查找信息。
- en: 'We can use any of the page inspect tools, such as Firebug or Chrome developer
    tools, to get the XPath of an element:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用任何页面检查工具，如Firebug或Chrome开发者工具，来获取元素的XPath：
- en: '![Parsing HTML with lxml](img/image_03_007.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![使用lxml解析HTML](img/image_03_007.jpg)'
- en: If we want to get the book names and prices from the  list, find the following
    section in the source.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要从列表中获取书名和价格，找到源代码中的以下部分。
- en: '[PRE16]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'From this we can create Xpath as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 从中我们可以创建Xpath如下：
- en: '[PRE17]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then we can print the lists using the following code:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用以下代码打印列表：
- en: '[PRE18]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Learn more on lxml at [http://lxml.de](http://lxml.de).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在[lxml.de](http://lxml.de)上了解更多关于lxml的信息。
- en: Scrapy
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Scrapy
- en: Scrapy is an open-source framework for web scraping and web crawling. This can
    be used to parse the whole website. As a framework, this helps to build spiders
    for specific requirements. Other than Scrapy, we can use mechanize to write scripts
    that can fill and submit forms.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy是一个用于网页抓取和爬取的开源框架。这可以用来解析整个网站。作为一个框架，它有助于为特定需求构建蜘蛛。除了Scrapy，我们还可以使用mechanize编写可以填写和提交表单的脚本。
- en: We can utilize the command line interface of Scrapy to create the basic boilerplate
    for new spidering scripts. Scrapy can be installed with `pip`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用Scrapy的命令行界面来为新的爬虫脚本创建基本样板。Scrapy可以通过`pip`安装。
- en: 'To create a new spider, we have to run the following command in the terminal
    after installing Scrapy:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个新的蜘蛛，我们必须在安装Scrapy后在终端中运行以下命令：
- en: '[PRE19]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This will generate a project folder in the current working directory `testSpider`.
    This will also create a basic structure and files inside the folder for our spider:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在当前工作目录`testSpider`中生成一个项目文件夹。这也将在文件夹内创建一个基本结构和文件，用于我们的spider：
- en: '![Scrapy](img/image_03_010.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![Scrapy](img/image_03_010.jpg)'
- en: 'Scrapy has CLI commands to create a spider. To create a spider, we have to
    enter the folder generated by the `startproject` command:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy有CLI命令来创建一个蜘蛛。要创建一个蜘蛛，我们必须输入`startproject`命令生成的文件夹：
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then we have to enter the generate spider command:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们必须输入生成蜘蛛命令：
- en: '[PRE21]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This will generate another folder, named `spiders`, and create the required
    files inside that folder. Then, the folder structure will be as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成另一个名为`spiders`的文件夹，并在该文件夹内创建所需的文件。然后，文件夹结构将如下所示：
- en: '![Scrapy](img/image_03_011.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![Scrapy](img/image_03_011.jpg)'
- en: 'Now open the `items.py` file and define a new item in the subclass called `TestspiderItem`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在打开`items.py`文件，并在子类中定义一个新项目，名为`TestspiderItem`：
- en: '[PRE22]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Most of this crawling logic is given by Scrapy in the `pactpub` class inside
    the `spider` folder, so we can extend this to write our `spider`. To do this,
    we have to edit the `pactpub.py` file in the spider folder.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分爬取逻辑都是由Scrapy在`spider`文件夹内的`pactpub`类中提供的，所以我们可以扩展这个来编写我们的`spider`。为了做到这一点，我们必须编辑spider文件夹中的`pactpub.py`文件。
- en: 'Inside the `pactpub.py` file, first we import the required modules:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在`pactpub.py`文件中，首先我们导入所需的模块：
- en: '[PRE23]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then, we have to extend the spider class of the Scrapy to define our `pactpubSpider`
    class. Here we can define the domain and initial URLs for crawling:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须扩展Scrapy的spider类，以定义我们的`pactpubSpider`类。在这里，我们可以定义域和爬取的初始URL：
- en: '[PRE24]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: After that, we have to define the parse method, which will create an instance
    of `TestspiderItem()` that we defined in the `items.py` file, and assign this
    to the items variable.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们必须定义解析方法，它将创建我们在`items.py`文件中定义的`TestspiderItem()`的一个实例，并将其分配给项目变量。
- en: Then we can add the items to extract, which can be done with XPATH or CSS style
    selectors.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以添加要提取的项目，可以使用XPATH或CSS样式选择器。
- en: 'Here, we are using XPATH selector:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用XPATH选择器：
- en: '[PRE25]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now we are ready to run the `spider`. We can run it using the following command:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备运行`spider`。我们可以使用以下命令运行它：
- en: '[PRE26]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This will start Scrapy with the URLs we defined and the crawled URLs will be
    passed to the `testspiderItems` and a new instance is created for each item.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用我们定义的URL启动Scrapy，并且爬取的URL将传递给`testspiderItems`，并为每个项目创建一个新实例。
- en: E-mail gathering
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 电子邮件收集
- en: Using the Python modules discussed previously, we can gather e-mails and other
    information from the web.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前讨论的Python模块，我们可以从网页中收集电子邮件和其他信息。
- en: To get e-mail IDs from a website, we may have to write customized scraping scripts.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要从网站获取电子邮件ID，我们可能需要编写定制的抓取脚本。
- en: Here, we discuss a common method of extracting e-mails from a web page with
    Python.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们讨论了一种从网页中提取电子邮件的常见方法。
- en: 'Let''s go through an example. Here, we are using `BeautifulSoup` and the requests
    module:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子。在这里，我们使用`BeautifulSoup`和requests模块：
- en: '[PRE27]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we will provide the list of URLs to crawl:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将提供要爬取的URL列表：
- en: '[PRE28]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we store the processed URLs in a set so as not to process them twice:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将处理过的URL存储在一个集合中，以便不重复处理它们：
- en: '[PRE29]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Collected e-mails are also stored in a set:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 收集的电子邮件也存储在一个集合中：
- en: '[PRE30]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'When we start scraping, we will take a URL from the queue and process it, and
    add it to the processed URLs. Also, we will do it until the queue is empty:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始抓取时，我们将从队列中获取一个URL并处理它，并将其添加到已处理的URL中。此外，我们将一直这样做，直到队列为空为止：
- en: '[PRE31]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'With the `urlparse` module we will get the base URL. This will be used to convert
    relative links to absolute links:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`urlparse`模块，我们将获得基本URL。这将用于将相对链接转换为绝对链接：
- en: '[PRE32]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The content of the URL will be available from try-catch. In case of error,
    it will go to the next URL:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: URL的内容将在try-catch中可用。如果出现错误，它将转到下一个URL：
- en: '[PRE33]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Inside the response, we will search for the e-mails and add the e-mails found
    to the e-mails set:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在响应中，我们将搜索电子邮件并将找到的电子邮件添加到电子邮件集合中：
- en: '[PRE34]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'After scraping the page, we will get all the links to other pages and update
    the URL queue:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在抓取页面后，我们将获取所有链接到其他页面的链接并更新URL队列：
- en: '[PRE35]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: OS fingerprinting
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: OS指纹识别
- en: 'A common process in pentesting is to identify the operating system used by
    the host. Usually, this involves tools like hping or Nmap, and in most cases these
    tools are quite aggressive to obtain such information and may generate alarms
    on the target host. OS fingerprinting mainly falls into two categories: active
    OS fingerprinting and passive OS fingerprinting.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 渗透测试中的常见过程是识别主机使用的操作系统。通常，这涉及到像hping或Nmap这样的工具，在大多数情况下，这些工具为了获取这样的信息而相当激进，并可能在目标主机上引发警报。OS指纹主要分为两类：主动OS指纹和被动OS指纹。
- en: Active fingerprinting is the method of sending packets to a remote host and
    analyzing corresponding responses. In passive fingerprinting, it analyzes packets
    from a host, so it does not send any traffic to the host and acts as a sniffer.
    In passive fingerprinting, it sniffs TCP/IP ports, so it avoids detection or being
    stopped by a firewall. Passive fingerprinting determines the target OS by analyzing
    the initial **Time to Live** (**TTL**) in IP headers packets, and with the TCP
    window size in the first packet of a TCP session. The first packet of TCP session
    is usually either a SYN (synchronize) or SYN/ACK (synchronize and acknowledge)
    packet.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 主动指纹识别是发送数据包到远程主机并分析相应响应的方法。在被动指纹识别中，它分析来自主机的数据包，因此不会向主机发送任何流量，并充当嗅探器。在被动指纹识别中，它嗅探TCP/IP端口，因此可以避免被防火墙检测或停止。被动指纹识别通过分析IP头数据包中的初始**生存时间**（**TTL**）以及TCP会话的第一个数据包中的TCP窗口大小来确定目标OS。TCP会话的第一个数据包通常是SYN（同步）或SYN/ACK（同步和确认）数据包。
- en: 'The following are the normal packet specifications for some operating systems:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些操作系统的正常数据包规格：
- en: '| **OS** | **Initial TTL** | **TCP window size** |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| **OS** | **初始TTL** | **TCP窗口大小** |'
- en: '| Linux kernel 2.x | 64 milliseconds | 5,840 kilobytes |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Linux内核2.x | 64毫秒 | 5,840千字节 |'
- en: '| Android / Chrome OS | 64 milliseconds | 5,720 kilobytes |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Android / Chrome OS | 64毫秒 | 5,720千字节 |'
- en: '| Windows XP | 128 milliseconds | 65,535 kilobytes |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Windows XP | 128毫秒 | 65,535千字节 |'
- en: '| Windows 7/ Server 2008 | 128 milliseconds | 8,192 kilobytes |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Windows 7/ Server 2008 | 128毫秒 | 8,192千字节 |'
- en: '| Cisco routers (IOS 12.4) | 255 milliseconds | 4,128 kilobytes |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Cisco路由器（IOS 12.4）| 255毫秒 | 4,128千字节 |'
- en: '| FreeBSD | 64 milliseconds | 65,535 kilobytes |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| FreeBSD | 64毫秒 | 65,535千字节 |'
- en: Passive OS fingerprinting is less accurate than the active method, but it helps
    the penetration tester avoid detection.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 被动OS指纹识别不如主动方法准确，但它有助于渗透测试人员避免被检测到。
- en: Another field that is interesting when fingerprinting systems is the **Initial
    Sequence Number** (**ISN**). In TCP, the members of a conversation keep track
    of what data has been seen and what data is to be sent next by using ISN. When
    establishing a connection, each member will select an ISN, and the following packets
    will be numbered by adding one to that number.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在指纹系统时另一个有趣的领域是**初始序列号**（**ISN**）。在TCP中，对话的成员通过使用ISN来跟踪已看到的数据和下一个要发送的数据。在建立连接时，每个成员都将选择一个ISN，随后的数据包将通过将该数字加一来编号。
- en: Scrapy can be used to analyze ISN increments to discover vulnerable systems.
    For that, we will collect responses from the target by sending a number of SYN
    packets in a loop.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Scrapy可用于分析ISN增量以发现易受攻击的系统。为此，我们将通过在循环中发送一定数量的SYN数据包来收集来自目标的响应。
- en: 'Start the interactive Python interpreter with `sudo` permission and import
    Scrapy:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sudo`权限启动交互式Python解释器并导入Scrapy：
- en: '[PRE36]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'After collecting some responses, we can print the data for analysis:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集了一些响应后，我们可以打印数据进行分析：
- en: '[PRE37]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This will print out the ISN values for analysis.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出用于分析的ISN值。
- en: 'If we have installed Nmap, we can use the active fingerprinting database of
    Nmap with Scapy as follows; make sure we have configured the fingerprinting database
    of Nmap `conf.nmap_base`:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们安装了Nmap，我们可以使用Nmap的主动指纹数据库与Scapy一起使用，方法如下；确保我们已经配置了Nmap的指纹数据库`conf.nmap_base`：
- en: '[PRE38]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Also, we can use `p0f` if it''s installed on our system to guess the OS with
    Scapy:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们的系统上安装了`p0f`，我们还可以使用它来猜测Scapy的OS：
- en: '[PRE39]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Get the EXIF data of an image
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取图像的EXIF数据
- en: 'We can find a lot of information from an image posted online. For every photo
    we took with our smartphone or camera, it records the date, time, shutter speed,
    aperture setting, ISO setting, whether the flash was used, the focal length, and
    lots more. This is stored with the photo, and is referred to as *EXIF* data. When
    we copy an image, the EXIF data is copied as well, as a part of the image. It
    can pose a privacy issue. For instance, a photo taken with a GPS-enabled phone,
    it can reveal the location and time it was taken, as well as the unique ID number
    of the device:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从在线发布的图像中找到大量信息。对于我们用智能手机或相机拍摄的每张照片，它都记录了日期、时间、快门速度、光圈设置、ISO设置、是否使用了闪光灯、焦距等等。这些信息存储在照片中，被称为*EXIF*数据。当我们复制一张图像时，EXIF数据也会被复制，作为图像的一部分。这可能会带来隐私问题。例如，使用GPS启用的手机拍摄的照片，可以显示拍摄的位置和时间，以及设备的唯一ID号：
- en: '[PRE40]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: First we imported the modules `PIL` image and `PIL TAGS`. `PIL` is an image
    processing module in Python. It supports many file formats and has a powerful
    image-processing capability. Then we iterate through the results and print the
    values.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入了`PIL`图像和`PIL TAGS`模块。`PIL`是Python中的图像处理模块。它支持许多文件格式，并具有强大的图像处理能力。然后我们遍历结果并打印数值。
- en: There are many other modules which support EXIF data extraction, like `ExifRead`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他模块支持EXIF数据提取，比如`ExifRead`。
- en: Web application fingerprinting
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Web应用指纹识别
- en: Web application fingerprinting is the main part of the information gathering
    stage in security assessment. It helps us to accurately identify an application
    and to pinpoint known vulnerabilities. This also allows us to customize payload
    or exploitation techniques based on the information. The simplest method is to
    open the site in the browser and look at its source code for specific keywords.
    Similarly, with Python, we can download the page and then run some basic regular
    expressions, which can give you the results.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Web应用指纹识别是安全评估信息收集阶段的主要部分。它帮助我们准确识别应用程序并找出已知的漏洞。这也允许我们根据信息定制有效载荷或利用技术。最简单的方法是在浏览器中打开网站并查看其特定关键字的源代码。同样，使用Python，我们可以下载页面然后运行一些基本的正则表达式，这可以给你结果。
- en: We can download the website with the `urllib`/`requests` module in combination
    with BeautifulSoup or lxml, as we discussed in this chapter.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`urllib`/`requests`模块与BeautifulSoup或lxml结合下载网站，就像我们在本章讨论的那样。
- en: Summary
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we discussed the possible methods of downloading and parsing
    a website. Using the basic methods discussed in this chapter, we can build our
    own scanners and web scrapers.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了下载和解析网站的可能方法。使用本章讨论的基本方法，我们可以构建自己的扫描器和网络爬虫。
- en: In the next chapter we will discuss more attack scripting techniques with Python.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论更多使用Python的攻击脚本技术。
