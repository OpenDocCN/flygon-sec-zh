["```py\nimport re\nimport random\nimport urllib\nurl1 = raw_input(\"Enter the URL \")\nu = chr(random.randint(97,122))\nurl2 = url1+u\nhttp_r = urllib.urlopen(url2)\n\ncontent= http_r.read()flag =0\ni=0\nlist1 = []\na_tag = \"<*address>\"\nfile_text = open(\"result.txt\",'a')\n\nwhile flag ==0:\n  if http_r.code == 404:\n    file_text.write(\"--------------\")\n    file_text.write(url1)\n    file_text.write(\"--------------n\")\n\n    file_text.write(content)\n    for match in re.finditer(a_tag,content):\n\n      i=i+1\n      s= match.start()\n      e= match.end()\n      list1.append(s)\n      list1.append(e)\n    if (i>0):\n      print \"Coding is not good\"\n    if len(list1)>0:\n      a= list1[1]\n      b= list1[2]\n\n      print content[a:b]\n    else:\n      print \"error handling seems ok\"\n    flag =1\n  elif http_r.code == 200:\n    print \"Web page is using custom Error page\"\n    break\n```", "```py\nflag =0\ni=0\nlist1 = []\na_tag = \"<*address>\"\nfile_text = open(\"result.txt\",'a')\n```", "```py\nfile_text.write(\"--------------\")\nfile_text.write(url1)\nfile_text.write(\"--------------n\")\n\nfile_text.write(content)\n```", "```py\nimport urllib\nurl1 = raw_input(\"Enter the URL \")\nhttp_r = urllib.urlopen(url1)\nif http_r.code == 200:\n  print http_r.headers\n```", "```py\n >>> import urllib\n  >>> http_r = urllib.urlopen(\"http://192.168.0.5/\")\n  >>> dir(http_r.headers)\n  ['__contains__', '__delitem__', '__doc__', '__getitem__', '__init__', '__iter__', '__len__', \n '__module__', '__setitem__', '__str__', 'addcontinue', 'addheader', 'dict', 'encodingheader', 'fp', \n 'get',  'getaddr', 'getaddrlist', 'getallmatchingheaders', 'getdate', 'getdate_tz', 'getencoding', \n 'getfirstmatchingheader', 'getheader', 'getheaders', 'getmaintype', 'getparam', 'getparamnames', \n 'getplist', 'getrawheader', 'getsubtype', 'gettype', 'has_key', 'headers', 'iscomment', 'isheader', \n 'islast', 'items', 'keys', 'maintype', 'parseplist', 'parsetype', 'plist', 'plisttext', 'readheaders', \n 'rewindbody', 'seekable', 'setdefault', 'startofbody', 'startofheaders', 'status', 'subtype', 'type', \n 'typeheader', 'unixfrom', 'values']\n  >>> \n  >>> http_r.headers.type\n  'text/html'\n  >>> http_r.headers.typeheader\n  'text/html; charset=UTF-8'\n >>>\n```", "```py\n      from lxml.html import fromstring\n      import requests\n```", "```py\n      domain = raw_input(\"Enter the domain : \")\n      url = 'http://whois.domaintools.com/'+domain\n      user_agent='wswp'\n      headers = {'User-Agent': user_agent}\n      resp = requests.get(url, headers=headers)\n      html = resp.text\n```", "```py\n      tree = fromstring(html)\n      ip= tree.xpath('//*[@id=\"stats\"]//table/tbody/tr//text()')\n```", "```py\n      list1 = []\n      for each in ip:\n        each = each.strip()\n        if each ==\"\":\n          continue\n        list1.append(each.strip(\"\\n\"))\n```", "```py\n      ip_index = list1.index('IP Address')\n      print \"IP address \", list1[ip_index+1]\n```", "```py\n      loc1 = list1.index('IP Location')\n      loc2 = list1.index('ASN')\n      print 'Location : ', \"\".join(list1[loc1+1:loc2])\n```", "```py\nimport urllib\nimport re\nfrom bs4 import BeautifulSoup\nurl = raw_input(\"Enter the URL \")\nht= urllib.urlopen(url)\nhtml_page = ht.read()\nemail_pattern=re.compile(r'\\b[\\w.-]+?@\\w+?\\.\\w+?\\b')\nfor match in re.findall(email_pattern,html_page ):\n  print match\n```", "```py\nimport socket\nimport struct\nimport binascii\ns = socket.socket(socket.PF_PACKET, socket.SOCK_RAW, socket.ntohs(0x0800))\nwhile True:\n\n  pkt  = s.recvfrom(2048)\n  banner = pkt[0][54:533]\n  print banner\n  print \"--\"*40\n```"]