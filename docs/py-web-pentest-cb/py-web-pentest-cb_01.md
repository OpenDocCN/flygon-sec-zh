# 第一章：收集开源情报

在本章中，我们将涵盖以下主题：

+   使用 Shodan API 收集信息

+   编写 Google+ API 搜索脚本

+   使用 Google+ API 下载个人资料图片

+   使用 Google+ API 分页收集额外结果

+   使用 QtWebKit 获取网站的屏幕截图

+   基于端口列表的屏幕截图

+   爬取网站

# 介绍

**开源情报**（**OSINT**）是从开放（公开）来源收集信息的过程。当涉及测试 Web 应用程序时，这可能看起来很奇怪。然而，在甚至触及网站之前，可以了解到关于特定网站的大量信息。您可能能够找出网站是用什么服务器端语言编写的，底层框架，甚至其凭据。学会使用 API 并编写这些任务可以使收集阶段的大部分工作变得更容易。

在本章中，我们将看一下我们可以使用 Python 利用 API 的几种方式来获得有关目标的洞察力。

# 使用 Shodan API 收集信息

Shodan 本质上是一个漏洞搜索引擎。通过提供名称、IP 地址甚至端口，它返回与其数据库中匹配的所有系统。这使得它成为基础设施情报的最有效来源之一。它就像互联网连接设备的谷歌。Shodan 不断扫描互联网并将结果保存到公共数据库中。虽然可以从 Shodan 网站（[`www.shodan.io`](https://www.shodan.io)）搜索此数据库，但结果和报告的服务是有限的，除非通过**应用程序编程接口**（**API**）访问。

本节的任务是通过使用 Shodan API 获取有关 Packt Publishing 网站的信息。

## 准备工作

在撰写本文时，Shodan 会员费为 49 美元，这是需要获取 API 密钥的。如果您对安全性很认真，访问 Shodan 是非常宝贵的。

如果您还没有 Shodan 的 API 密钥，请访问[www.shodan.io/store/member](http://www.shodan.io/store/member)并注册。Shodan 有一个非常好的 Python 库，也在[`shodan.readthedocs.org/en/latest/`](https://shodan.readthedocs.org/en/latest/)上有很好的文档。

要设置 Python 环境以与 Shodan 一起工作，您只需要使用`cheeseshop`安装库：

```py
$ easy_install shodan

```

## 如何做…

以下是我们将用于此任务的脚本：

```py
import shodan
import requests

SHODAN_API_KEY = "{Insert your Shodan API key}" 
api = shodan.Shodan(SHODAN_API_KEY)

target = 'www.packtpub.com'

dnsResolve = 'https://api.shodan.io/dns/resolve?hostnames=' + target + '&key=' + SHODAN_API_KEY

try:
    # First we need to resolve our targets domain to an IP
    resolved = requests.get(dnsResolve)
    hostIP = resolved.json()[target]

    # Then we need to do a Shodan search on that IP
    host = api.host(hostIP)
    print "IP: %s" % host['ip_str']
    print "Organization: %s" % host.get('org', 'n/a')
    print "Operating System: %s" % host.get('os', 'n/a')

    # Print all banners
    for item in host['data']:
        print "Port: %s" % item['port']
        print "Banner: %s" % item['data']

    # Print vuln information
    for item in host['vulns']:
        CVE = item.replace('!','')
        print 'Vulns: %s' % item
        exploits = api.exploits.search(CVE)
        for item in exploits['matches']:
            if item.get('cve')[0] == CVE:
                print item.get('description')
except:
    'An error occured'
```

上述脚本应该产生类似以下的输出：

```py
IP: 83.166.169.231
Organization: Node4 Limited
Operating System: None

Port: 443
Banner: HTTP/1.0 200 OK

Server: nginx/1.4.5

Date: Thu, 05 Feb 2015 15:29:35 GMT

Content-Type: text/html; charset=utf-8

Transfer-Encoding: chunked

Connection: keep-alive

Expires: Sun, 19 Nov 1978 05:00:00 GMT

Cache-Control: public, s-maxage=172800

Age: 1765

Via: 1.1 varnish

X-Country-Code: US

Port: 80
Banner: HTTP/1.0 301 https://www.packtpub.com/

Location: https://www.packtpub.com/

Accept-Ranges: bytes

Date: Fri, 09 Jan 2015 12:08:05 GMT

Age: 0

Via: 1.1 varnish

Connection: close

X-Country-Code: US

Server: packt

Vulns: !CVE-2014-0160
The (1) TLS and (2) DTLS implementations in OpenSSL 1.0.1 before 1.0.1g do not properly handle Heartbeat Extension packets, which allows remote attackers to obtain sensitive information from process memory via crafted packets that trigger a buffer over-read, as demonstrated by reading private keys, related to d1_both.c and t1_lib.c, aka the Heartbleed bug.

```

我只选择了 Shodan 返回的一些可用数据项，但您可以看到我们得到了相当多的信息。在这种特定情况下，我们可以看到存在潜在的漏洞。我们还看到这台服务器正在端口`80`和`443`上监听，并且根据横幅信息，它似乎正在运行`nginx`作为 HTTP 服务器。

## 工作原理…

1.  首先，在代码中设置我们的静态字符串；这包括我们的 API 密钥：

```py
SHODAN_API_KEY = "{Insert your Shodan API key}" 
target = 'www.packtpub.com'

dnsResolve = 'https://api.shodan.io/dns/resolve?hostnames=' + target + '&key=' + SHODAN_API_KEY
```

1.  下一步是创建我们的 API 对象：

```py
api = shodan.Shodan(SHODAN_API_KEY)
```

1.  为了使用 API 搜索主机的信息，我们需要知道主机的 IP 地址。Shodan 有一个 DNS 解析器，但它没有包含在 Python 库中。要使用 Shodan 的 DNS 解析器，我们只需向 Shodan DNS 解析器 URL 发出 GET 请求，并传递我们感兴趣的域（或域）：

```py
resolved = requests.get(dnsResolve)
hostIP = resolved.json()[target] 
```

1.  返回的 JSON 数据将是一个域到 IP 地址的字典；在我们的情况下，我们只有一个目标，我们可以简单地使用`target`字符串作为字典的键来提取我们主机的 IP 地址。如果您正在搜索多个域，您可能希望遍历此列表以获取所有 IP 地址。

1.  现在，我们有了主机的 IP 地址，我们可以使用 Shodan 库的`host`函数来获取有关我们的主机的信息。返回的 JSON 数据包含大量关于主机的信息，尽管在我们的情况下，我们只会提取 IP 地址、组织，如果可能的话，正在运行的操作系统。然后，我们将循环遍历找到的所有打开端口及其各自的横幅：

```py
    host = api.host(hostIP)
    print "IP: %s" % host['ip_str']
    print "Organization: %s" % host.get('org', 'n/a')
    print "Operating System: %s" % host.get('os', 'n/a')

    # Print all banners
    for item in host['data']:
        print "Port: %s" % item['port']
        print "Banner: %s" % item['data']
```

1.  返回的数据还可能包含 Shodan 认为服务器可能容易受到的**通用漏洞和暴露**（**CVE**）编号。这对我们可能非常有益，因此我们将遍历这些列表（如果有的话），并使用 Shodan 库的另一个函数获取有关利用的信息：

```py
for item in host['vulns']:
        CVE = item.replace('!','')
        print 'Vulns: %s' % item
        exploits = api.exploits.search(CVE)
        for item in exploits['matches']:
            if item.get('cve')[0] == CVE:
                print item.get('description')
```

这就是我们的脚本。尝试针对您自己的服务器运行它。

## 还有更多...

我们只是真正开始了 Shodan Python 库的使用。值得阅读 Shodan API 参考文档，并尝试使用其他搜索选项。您可以根据“facets”筛选结果以缩小搜索范围。您甚至可以使用其他用户使用“tags”搜索保存的搜索。

### 提示

**下载示例代码**

您可以从[`www.packtpub.com`](http://www.packtpub.com)的帐户中下载示例代码文件，以获取您购买的所有 Packt Publishing 图书。如果您在其他地方购买了本书，您可以访问[`www.packtpub.com/support`](http://www.packtpub.com/support)并注册，以便直接通过电子邮件接收文件。

# 编写 Google+ API 搜索脚本

社交媒体是收集有关目标公司或个人信息的好方法。在这里，我们将向您展示如何编写一个 Google+ API 搜索脚本，以在 Google+社交网站中查找公司的联系信息。

## 准备工作

一些 Google API 需要授权才能访问，但是如果您有 Google 帐户，获取 API 密钥很容易。只需转到[`console.developers.google.com`](https://console.developers.google.com)，创建一个新项目。单击**API 和身份验证** | **凭据**。单击**创建新密钥**，然后**服务器密钥**。可选择输入您的 IP，或者只需单击**创建**。您的 API 密钥将显示并准备好复制并粘贴到以下示例中。

## 如何做...

这是一个简单的查询 Google+ API 的脚本：

```py
import urllib2

GOOGLE_API_KEY = "{Insert your Google API key}" 
target = "packtpub.com"
api_response = urllib2.urlopen("https://www.googleapis.com/plus/v1/people? query="+target+"&key="+GOOGLE_API_KEY).read()
api_response = api_response.split("\n")
for line in api_response:
    if "displayName" in line:
        print line
```

## 工作原理...

前面的代码向 Google+搜索 API 发出请求（使用您的 API 密钥进行身份验证），并搜索与目标`packtpub.com`匹配的帐户。与前面的 Shodan 脚本类似，我们设置了静态字符串，包括 API 密钥和目标：

```py
GOOGLE_API_KEY = "{Insert your Google API key}" 
target = "packtpub.com"
```

下一步有两个作用：首先，它向 API 服务器发送 HTTP`GET`请求，然后读取响应并将输出存储到一个`api_response`变量中：

```py
api_response = urllib2.urlopen("https://www.googleapis.com/plus/v1/people? query="+target+"&key="+GOOGLE_API_KEY).read()
```

此请求返回 JSON 格式的响应；这里显示了结果的一个示例片段：

![工作原理...](img/B04044_01_01.jpg)

在我们的脚本中，我们将响应转换为列表，以便更容易解析：

```py
api_response = api_response.split("\n")
```

代码的最后一部分循环遍历列表，并仅打印包含`displayName`的行，如下所示：

![工作原理...](img/B04044_01_02.jpg)

## 另请参阅...

在下一个示例中，*使用 Google+ API 下载个人资料图片*，我们将看到如何改进这些结果的格式。

## 还有更多...

通过从一个简单的脚本开始查询 Google+ API，我们可以扩展它以提高效率，并利用返回的更多数据。Google+平台的另一个关键方面是用户可能还在 Google 的其他服务上拥有匹配的帐户，这意味着您可以交叉引用帐户。大多数 Google 产品都提供给开发人员的 API，因此一个很好的起点是[`developers.google.com/products/`](https://developers.google.com/products/)。获取 API 密钥并将上一个脚本的输出插入其中。

# 使用 Google+ API 下载个人资料图片

现在我们已经确定了如何使用 Google+ API，我们可以设计一个脚本来下载图片。这里的目标是从网页中获取姓名的照片。我们将通过 URL 向 API 发送请求，通过 JSON 处理响应，并在脚本的工作目录中创建图片文件。

## 如何做到

以下是一个使用 Google+ API 下载个人资料图片的简单脚本：

```py
import urllib2
import json

GOOGLE_API_KEY = "{Insert your Google API key}"
target = "packtpub.com"
api_response = urllib2.urlopen("https://www.googleapis.com/plus/v1/people? query="+target+"&key="+GOOGLE_API_KEY).read()

json_response = json.loads(api_response)
for result in json_response['items']:
      name = result['displayName']
      print name
      image = result['image']['url'].split('?')[0]
  f = open(name+'.jpg','wb+')
  f.write(urllib2.urlopen(image).read())
  f.close()
```

## 它是如何工作的

第一个更改是将显示名称存储到变量中，因为稍后会重复使用它：

```py
      name = result['displayName']
      print name
```

接下来，我们从 JSON 响应中获取图像 URL：

```py
image = result['image']['url'].split('?')[0]
```

代码的最后部分在三行简单的代码中做了很多事情：首先，它在本地磁盘上打开一个文件，文件名设置为`name`变量。这里的`wb+`标志指示操作系统，如果文件不存在，则应创建文件，并以原始二进制格式写入数据。第二行向图像 URL（存储在`image`变量中）发出 HTTP `GET`请求，并将响应写入文件。最后，关闭文件以释放用于存储文件内容的系统内存：

```py
  f = open(name+'.jpg','wb+')
  f.write(urllib2.urlopen(image).read())
  f.close()
```

脚本运行后，控制台输出将与以前相同，显示名称也会显示。但是，您的本地目录现在还将包含所有个人资料图片，保存为 JPEG 文件。

# 使用分页从 Google+ API 中获取额外的结果

默认情况下，Google+ API 返回最多 25 个结果，但我们可以通过增加最大值并通过分页收集更多结果来扩展先前的脚本。与以前一样，我们将通过 URL 和`urllib`库与 Google+ API 进行通信。我们将创建任意数字，随着请求的进行而增加，这样我们就可以跨页面移动并收集更多结果。

## 如何做到

以下脚本显示了如何从 Google+ API 中获取额外的结果：

```py
import urllib2
import json

GOOGLE_API_KEY = "{Insert your Google API key}"
target = "packtpub.com"
token = ""
loops = 0

while loops < 10:
  api_response = urllib2.urlopen("https://www.googleapis.com/plus/v1/people? query="+target+"&key="+GOOGLE_API_KEY+"&maxResults=50& pageToken="+token).read()

  json_response = json.loads(api_response)
  token = json_response['nextPageToken']

  if len(json_response['items']) == 0:
    break

  for result in json_response['items']:
        name = result['displayName']
        print name
        image = result['image']['url'].split('?')[0]
    f = open(name+'.jpg','wb+')
    f.write(urllib2.urlopen(image).read())
  loops+=1
```

## 它是如何工作的

这个脚本中的第一个重大变化是主要代码已经移入了一个`while`循环中：

```py
token = ""
loops = 0

while loops < 10:
```

在这里，循环的次数设置为最多 10 次，以避免向 API 服务器发送过多请求。当然，这个值可以更改为任何正整数。下一个变化是请求 URL 本身；它现在包含了两个额外的尾部参数`maxResults`和`pageToken`。来自 Google+ API 的每个响应都包含一个`pageToken`值，它是指向下一组结果的指针。请注意，如果没有更多结果，仍然会返回一个`pageToken`值。`maxResults`参数是不言自明的，但最多只能增加到 50：

```py
  api_response = urllib2.urlopen("https://www.googleapis.com/plus/v1/people? query="+target+"&key="+GOOGLE_API_KEY+"&maxResults=50& pageToken="+token).read()
```

下一部分在 JSON 响应中读取与以前相同，但这次它还提取了`nextPageToken`的值：

```py
  json_response = json.loads(api_response)
  token = json_response['nextPageToken']
```

主`while`循环如果`loops`变量增加到 10，就会停止，但有时您可能只会得到一页结果。代码中的下一部分检查返回了多少结果；如果没有结果，它会过早地退出循环：

```py
  if len(json_response['items']) == 0:
    break
```

最后，我们确保每次增加`loops`整数的值。一个常见的编码错误是忽略这一点，这意味着循环将永远继续：

```py
  loops+=1
```

# 使用 QtWebKit 获取网站的屏幕截图

他们说一张图片价值千言。有时，在情报收集阶段获取网站的屏幕截图是很有用的。我们可能想要扫描一个 IP 范围，并了解哪些 IP 正在提供网页，更重要的是它们的样子。这可以帮助我们挑选出有趣的网站进行关注，我们也可能想要快速扫描特定 IP 地址上的端口，出于同样的原因。我们将看看如何使用`QtWebKit` Python 库来实现这一点。

## 准备工作

QtWebKit 安装起来有点麻烦。最简单的方法是从[`www.riverbankcomputing.com/software/pyqt/download`](http://www.riverbankcomputing.com/software/pyqt/download)获取二进制文件。对于 Windows 用户，请确保选择适合你的`python/arch`路径的二进制文件。例如，我将使用`PyQt4-4.11.3-gpl-Py2.7-Qt4.8.6-x32.exe`二进制文件在我的安装了 Python 2.7 的 Windows 32 位虚拟机上安装 Qt4。如果你打算从源文件编译 Qt4，请确保你已经安装了`SIP`。

## 如何做…

一旦你安装了 PyQt4，你基本上就可以开始了。下面的脚本是我们将用作截图类的基础：

```py
import sys
import time
from PyQt4.QtCore import *
from PyQt4.QtGui import *
from PyQt4.QtWebKit import *

class Screenshot(QWebView):
    def __init__(self):
        self.app = QApplication(sys.argv)
        QWebView.__init__(self)
        self._loaded = False
        self.loadFinished.connect(self._loadFinished)

    def wait_load(self, delay=0):
        while not self._loaded:
            self.app.processEvents()
            time.sleep(delay)
        self._loaded = False

    def _loadFinished(self, result):
        self._loaded = True

    def get_image(self, url):
        self.load(QUrl(url))
        self.wait_load()

        frame = self.page().mainFrame()
        self.page().setViewportSize(frame.contentsSize())

        image = QImage(self.page().viewportSize(), QImage.Format_ARGB32)
        painter = QPainter(image)
        frame.render(painter)
        painter.end()
        return image
```

创建前面的脚本并将其保存在 Python 的`Lib`文件夹中。然后我们可以在我们的脚本中将其作为导入引用。

## 工作原理…

该脚本利用`QWebView`加载 URL，然后使用 QPainter 创建图像。`get_image`函数接受一个参数：我们的目标。有了这个，我们可以简单地将其导入到另一个脚本中并扩展功能。

让我们分解脚本，看看它是如何工作的。

首先，我们设置我们的导入：

```py
import sys
import time
from PyQt4.QtCore import *
from PyQt4.QtGui import *
from PyQt4.QtWebKit import *
```

然后，我们创建我们的类定义；我们正在创建的类通过继承从`QWebView`继承：

```py
class Screenshot(QWebView):
```

接下来，我们创建我们的初始化方法：

```py
def __init__(self):
        self.app = QApplication(sys.argv)
        QWebView.__init__(self)
        self._loaded = False
        self.loadFinished.connect(self._loadFinished)

def wait_load(self, delay=0):
        while not self._loaded:
            self.app.processEvents()
            time.sleep(delay)
        self._loaded = False

def _loadFinished(self, result):
        self._loaded = True
```

初始化方法设置了`self.__loaded`属性。这与`__loadFinished`和`wait_load`函数一起用于检查应用程序运行时的状态。它会等到站点加载完成后再截图。实际的截图代码包含在`get_image`函数中：

```py
def get_image(self, url):
        self.load(QUrl(url))
        self.wait_load()

        frame = self.page().mainFrame()
        self.page().setViewportSize(frame.contentsSize())

        image = QImage(self.page().viewportSize(), QImage.Format_ARGB32)
        painter = QPainter(image)
        frame.render(painter)
        painter.end()
        return image
```

在这个`get_image`函数中，我们将视口的大小设置为主框架中内容的大小。然后设置图像格式，将图像分配给绘图对象，然后使用绘图器渲染框架。最后，我们返回处理过的图像。

## 还有更多…

要使用我们刚刚创建的类，我们只需将其导入到另一个脚本中。例如，如果我们只想保存我们收到的图像，我们可以做如下操作：

```py
import screenshot
s = screenshot.Screenshot()
image = s.get_image('http://www.packtpub.com')
image.save('website.png')
```

就是这样。在下一个脚本中，我们将创建一些更有用的东西。

# 基于端口列表的截图

在上一个脚本中，我们创建了一个基本函数来返回 URL 的图像。现在我们将扩展该功能，循环遍历与基于 Web 的管理门户常见相关的端口列表。这将允许我们将脚本指向一个 IP，并自动运行可能与 Web 服务器相关的可能端口。这是用于在我们不知道服务器上开放了哪些端口的情况下使用，而不是在我们指定端口和域时使用。

## 准备工作

为了使这个脚本工作，我们需要在*使用 QtWeb Kit 获取网站截图*配方中创建脚本。这应该保存在`Pythonxx/Lib`文件夹中，并命名为清晰和易记的名称。在这里，我们将该脚本命名为`screenshot.py`。你的脚本的命名特别重要，因为我们会用一个重要的声明引用它。

## 如何做…

这是我们将要使用的脚本：

```py
import screenshot
import requests

portList = [80,443,2082,2083,2086,2087,2095,2096,8080,8880,8443,9998,4643, 9001,4489]

IP = '127.0.0.1'

http = 'http://'
https = 'https://'

def testAndSave(protocol, portNumber):
    url = protocol + IP + ':' + str(portNumber)
    try:
        r = requests.get(url,timeout=1)

        if r.status_code == 200:
            print 'Found site on ' + url 
            s = screenshot.Screenshot()
            image = s.get_image(url)
            image.save(str(portNumber) + '.png')
    except:
        pass

for port in portList:
    testAndSave(http, port)
    testAndSave(https, port)
```

## 工作原理…

我们首先创建我们的导入声明。在这个脚本中，我们使用了之前创建的`screenshot`脚本，还有`requests`库。`requests`库用于我们在尝试将其转换为图像之前检查请求的状态。我们不想浪费时间尝试转换不存在的站点。

接下来，我们导入我们的库：

```py
import screenshot
import requests
```

下一步是设置我们将要迭代的常见端口号数组。我们还设置了一个包含我们将要使用的 IP 地址的字符串：

```py
portList = [80,443,2082,2083,2086,2087,2095,2096,8080,8880,8443,9998,4643, 9001,4489]

IP = '127.0.0.1'
```

接下来，我们创建字符串来保存我们稍后将构建的 URL 的协议部分；这只是为了稍后的代码更加整洁：

```py
http = 'http://'
https = 'https://'
```

接下来，我们创建我们的方法，它将负责构建 URL 字符串的工作。创建 URL 后，我们检查我们的`get`请求是否返回`200`响应代码。如果请求成功，我们将返回的网页转换为图像，并以成功的端口号作为文件名保存。代码包裹在`try`块中，因为如果我们发出请求时网站不存在，它将抛出一个错误：

```py
def testAndSave(protocol, portNumber):
    url = protocol + IP + ':' + str(portNumber)
    try:
        r = requests.get(url,timeout=1)

        if r.status_code == 200:
            print 'Found site on ' + url 
            s = screenshot.Screenshot()
            image = s.get_image(url)
            image.save(str(portNumber) + '.png')
    except:
        pass
```

现在我们的方法已经准备好了，我们只需遍历端口列表中的每个端口，并调用我们的方法。我们先对 HTTP 协议进行一次，然后对 HTTPS 进行一次：

```py
for port in portList:
    testAndSave(http, port)
    testAndSave(https, port)
```

就是这样。只需运行脚本，它就会将图像保存在与脚本相同的位置。

## 还有更多...

你可能会注意到脚本运行起来需要一些时间。这是因为它必须依次检查每个端口。实际上，你可能希望将这个脚本改成多线程脚本，这样它就可以同时检查多个 URL。让我们快速看一下如何修改代码来实现这一点。

首先，我们需要几个额外的导入声明：

```py
import Queue
import threading
```

接下来，我们需要创建一个名为`threader`的新函数。这个新函数将处理将我们的`testAndSave`函数放入队列中：

```py
def threader(q, port):
    q.put(testAndSave(http, port))
    q.put(testAndSave(https, port))
```

现在我们有了新的函数，我们只需要设置一个新的`Queue`对象，并进行一些线程调用。我们将从我们对`portList`变量的`FOR`循环中取出`testAndSave`调用，并用这段代码替换它：

```py
q = Queue.Queue()

for port in portList:
    t = threading.Thread(target=threader, args=(q, port))
    t.deamon = True
    t.start()

s = q.get()
```

因此，我们的新脚本现在总共看起来是这样的：

```py
import Queue
import threading
import screenshot
import requests

portList = [80,443,2082,2083,2086,2087,2095,2096,8080,8880,8443,9998,4643, 9001,4489]

IP = '127.0.0.1'

http = 'http://'
https = 'https://'

def testAndSave(protocol, portNumber):
    url = protocol + IP + ':' + str(portNumber)
    try:
        r = requests.get(url,timeout=1)

        if r.status_code == 200:
            print 'Found site on ' + url 
            s = screenshot.Screenshot()
            image = s.get_image(url)
            image.save(str(portNumber) + '.png')
    except:
        pass

def threader(q, port):
    q.put(testAndSave(http, port))
    q.put(testAndSave(https, port))

q = Queue.Queue()

for port in portList:
    t = threading.Thread(target=threader, args=(q, port))
    t.deamon = True
    t.start()

s = q.get()
```

如果我们现在运行这个脚本，我们将更快地执行我们的代码，因为 Web 请求现在是并行执行的。

你可以尝试进一步扩展脚本，使其适用于一系列 IP 地址；当你测试内部网络范围时，这可能会很方便。

# 爬取网站

许多工具提供了绘制网站地图的功能，但通常你只能限制输出样式或提供结果的位置。这个爬虫脚本的基础版本允许你快速绘制网站地图，并且可以根据需要进行修改。

## 准备工作

为了使这个脚本工作，你需要`BeautifulSoup`库，可以通过`apt`命令安装，使用`apt-get install python-bs4`，或者使用`pip install beautifulsoup4`。就是这么简单。

## 如何做...

这是我们将要使用的脚本：

```py
import urllib2 
from bs4 import BeautifulSoup
import sys
urls = []
urls2 = []

tarurl = sys.argv[1] 

url = urllib2.urlopen(tarurl).read()
soup = BeautifulSoup(url)
for line in soup.find_all('a'):
    newline = line.get('href')
    try: 
        if newline[:4] == "http": 
            if tarurl in newline: 
            urls.append(str(newline)) 
        elif newline[:1] == "/": 
            combline = tarurl+newline urls.append(str(combline)) except: 
               pass

    for uurl in urls: 
        url = urllib2.urlopen(uurl).read() 
        soup = BeautifulSoup(url) 
        for line in soup.find_all('a'): 
            newline = line.get('href') 
            try: 
                if newline[:4] == "http": 
                    if tarurl in newline:
                        urls2.append(str(newline)) 
                elif newline[:1] == "/": 
                    combline = tarurl+newline 
                    urls2.append(str(combline)) 
                    except: 
                pass 
            urls3 = set(urls2) 
    for value in urls3: 
    print value
```

## 它是如何工作的...

首先导入必要的库，并创建两个名为`urls`和`urls2`的空列表。这将允许我们对爬虫过程进行两次运行。接下来，我们设置输入，作为脚本的附录添加到命令行中运行。它将运行如下：

```py
$ python spider.py http://www.packtpub.com

```

然后，我们打开提供的`url`变量，并将其传递给`beautifulsoup`工具：

```py
url = urllib2.urlopen(tarurl).read() 
soup = BeautifulSoup(url) 
```

`beautifulsoup`工具将内容分成部分，并允许我们只提取我们想要的部分：

```py
for line in soup.find_all('a'): 
newline = line.get('href') 
```

然后，我们提取在 HTML 中标记为标签的所有内容，并抓取标记指定为`href`的元素。这允许我们抓取页面中列出的所有 URL。

接下来的部分处理相对链接和绝对链接。如果一个链接是相对的，它以斜杠开头，表示它是一个托管在 Web 服务器本地的页面。如果一个链接是绝对的，它包含完整的地址，包括域名。我们在下面的代码中所做的是确保我们作为外部用户可以打开我们找到的所有链接并将它们列为绝对链接：

```py
if newline[:4] == "http": 
if tarurl in newline: 
urls.append(str(newline)) 
  elif newline[:1] == "/": 
combline = tarurl+newline urls.append(str(combline))
```

然后，我们再次使用从该页面识别出的`urls`列表重复这个过程，通过遍历原始`url`列表中的每个元素：

```py
for uurl in urls:
```

除了引用列表和变量的更改，代码保持不变。

我们合并这两个列表，最后，为了方便输出，我们将`urls`列表的完整列表转换为一个集合。这将从列表中删除重复项，并允许我们整齐地输出它。我们遍历集合中的值，并逐个输出它们。

## 还有更多...

这个工具可以与本书中早期和后期展示的任何功能相结合。它可以与*使用 QtWeb Kit 获取网站截图*结合，允许您对每个页面进行截图。您可以将其与第二章中的电子邮件地址查找器*枚举*结合，从每个页面获取电子邮件地址，或者您可以找到另一种用途来映射网页的简单技术。

该脚本可以很容易地更改，以添加深度级别，从当前的 2 个链接深度到系统参数设置的任何值。输出可以更改以添加每个页面上存在的 URL，或将其转换为 CSV，以便您可以将漏洞映射到页面进行简单的注释。
