- en: Machine Learning and Cybersecurity
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习和网络安全
- en: These days, **Machine Learning** (**ML**) is a term we come across quite often.
    In this chapter, we are going to look at an overview of what exactly ML is, what
    kinds of problems it solves, and finally what kinds of applications it can have
    in the cyber security ecosystem. We are also going to look at the various different
    kinds of ML models, and which models we can use in which circumstances. It should
    be noted that the scope of this book is not to cover ML in detail, but instead
    to provide a solid understanding of ML and its applications in the cyber security
    domain.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，**机器学习**（**ML**）是一个我们经常遇到的术语。在本章中，我们将概述机器学习的确切含义，它解决的问题类型，以及它在网络安全生态系统中的应用类型。我们还将研究各种不同类型的机器学习模型，以及在哪些情况下可以使用哪些模型。值得注意的是，本书的范围不是详细介绍机器学习，而是提供对机器学习及其在网络安全领域的应用的扎实理解。
- en: 'The following topics will be covered in this chapter in detail:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将详细介绍以下主题：
- en: Machine Learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习
- en: Regression-based Machine Learning models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于回归的机器学习模型
- en: Classification models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类模型
- en: Natural language processing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: Machine Learning
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习
- en: Let's start with a basic question: *what is machine learning, and why should
    we use it?*
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个基本问题开始：*什么是机器学习，为什么我们要使用它？*
- en: We can define ML as a branch of data science that can efficiently solve prediction
    problems. Let's assume that we have data on the customers of an e-commerce website
    over the last three months, and that data contains the purchase history of a particular
    product (`c_id`, `p_id`**,** `age`, `gender`, `nationality`, `purchased[yes/no]`).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将机器学习定义为数据科学的一个分支，可以有效解决预测问题。假设我们有过去三个月电子商务网站客户的数据，数据包含特定产品的购买历史（`c_id`，`p_id`，`age`，`gender`，`nationality`，`purchased[yes/no]`）。
- en: Our objective is to use the dataset to identify a customer who would be likely
    to purchase the product, based on their purchase history. We might think that
    a good idea would be to take the purchase column into account and to assume that
    those who have purchased the product previously would be most likely to purchase
    it again. However, a better business solution would take all parameters into account,
    including the region from which the most purchases happen, the age group of the
    customer, and their gender as well. Based upon the permutation of all of these
    fields, a business owner can get a better idea of the kind of customer who is
    most influenced by the product and the marketing team can therefore design more
    specific, targeted campaigns.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是利用数据集来识别可能购买产品的客户，基于他们的购买历史。我们可能认为一个好主意是考虑购买列，并假设那些先前购买产品的人最有可能再次购买。然而，一个更好的业务解决方案将考虑所有参数，包括发生最多购买的地区，客户的年龄组和性别等。基于所有这些领域的排列，业务所有者可以更好地了解受产品影响最大的客户类型，因此营销团队可以设计更具体、有针对性的活动。
- en: We can do this in two different ways. The first solution would be to write software
    in the programming language of our choice and to write logic that gives a specific
    weight to each of the parameters discussed. The logic would then be able to tell
    us who all the potential buyers are. The downside of this approach, however, is
    that a significant amount of time would be required to draft the logic and if
    new parameters are added (such as the profession of the customer), the logic would
    need to change. Furthermore, the logic written would solve only one specific business
    problem. This is the traditional approach adopted before machine learning was
    developed, and is still used by various businesses.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过两种不同的方式来做到这一点。第一种解决方案是使用我们选择的编程语言编写软件，并编写逻辑，给每个讨论的参数赋予特定的权重。然后逻辑将能够告诉我们所有潜在的买家是谁。然而，这种方法的缺点是需要大量时间来起草逻辑，如果添加了新的参数（例如客户的职业），逻辑将需要更改。此外，编写的逻辑只能解决一个特定的业务问题。这是在机器学习开发之前采用的传统方法，目前仍被各种企业使用。
- en: The second solution would be to use ML. Based on the customer dataset, we can
    train an ML model and make it predict whether a customer is a potential buyer
    or not. Training the model involves feeding all the training data to an ML library
    that would take all the parameters into account and learn which are the common
    attributes of customers who purchased the product, and which are the attributes
    of the customers who didn't purchase the product. Whatever is learned by the model
    is persisted in the memory and the obtained model is said to be trained. If the
    model is presented with the data of a new customer, it would use its training
    and make a prediction based upon the learned attributes that usually lead to a
    purchase. The same business problem that used to have to be solved with a computer
    program and hardcoded logic is now solved with a mathematical ML model. This is
    one of the many cases in which we can use ML.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种解决方案是使用机器学习。基于客户数据集，我们可以训练一个机器学习模型，并让其预测客户是否是潜在的买家。训练模型涉及将所有训练数据提供给一个机器学习库，该库将考虑所有参数并学习购买产品的客户的共同属性，以及未购买产品的客户的属性。模型学到的内容将被保存在内存中，获得的模型被称为经过训练的。如果模型被提供新客户的数据，它将使用其训练并基于学到的通常导致购买的属性进行预测。以前必须用计算机程序和硬编码逻辑解决的同样的业务问题现在用数学机器学习模型解决。这是我们可以使用机器学习的许多案例之一。
- en: It is important to remember that if the problem at hand is a prediction problem,
    ML can be applied to obtain a good prediction. However, if the objective of the
    problem is to automate a manual task, ML would not be helpful; we would need to
    use a traditional programming approach. ML solves prediction problems by using
    mathematical models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，如果手头的问题是一个预测问题，机器学习可以应用来获得良好的预测。然而，如果问题的目标是自动化手动任务，机器学习将无济于事；我们需要使用传统的编程方法。机器学习通过使用数学模型来解决预测问题。
- en: '**Artificial intelligence** (**AI**) is another word that we are likely to
    come across very often. Let''s now try to answer another question: What is artificial
    intelligence and how is it different than machine learning?'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工智能**（**AI**）是另一个我们经常会遇到的词。现在让我们试着回答另一个问题：**什么是人工智能，它和机器学习有什么不同？**'
- en: Setting up a Machine Learning environment in Kali Linux
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Kali Linux中设置机器学习环境
- en: 'All ML libraries come packaged within a package called `anaconda`. This will
    install Python 3.5 or the latest Python version available. To run ML code, we
    require Python 3 or higher:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的机器学习库都打包在一个叫做`anaconda`的包中。这将安装Python 3.5或最新版本的Python。要运行机器学习代码，我们需要Python
    3或更高版本：
- en: 'Download anaconda from the following URL: [https://conda.io/miniconda.html](https://conda.io/miniconda.html).'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从以下网址下载anaconda：[https://conda.io/miniconda.html](https://conda.io/miniconda.html)。
- en: Install all the packages by running `bash Anaconda-latest-Linux-x86_64.sh.>`
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行`bash Anaconda-latest-Linux-x86_64.sh.>`来安装所有的包。
- en: 'For more details, refer to the following URL: [https://conda.io/docs/user-guide/install/linux.html](https://conda.io/docs/user-guide/install/linux.html).'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有关更多详细信息，请参考以下网址：[https://conda.io/docs/user-guide/install/linux.html](https://conda.io/docs/user-guide/install/linux.html)。
- en: Regression-based machine learning models
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于回归的机器学习模型
- en: We make use of regression models when we have to predict a continuous value
    rather than a discrete one. For example, let's say that a dataset contains the
    number of years of experience of an employee and the employee's salary. Based
    upon these two values, this model is trained and expected to make a prediction
    on the employee's salary based on their *years of experience*. Since the salary
    is a continuous number, we can make use of regression-based machine learning models
    to solve this kind of problem.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们需要预测连续值而不是离散值时，我们使用回归模型。例如，假设数据集包含员工的工作经验年限和工资。基于这两个值，这个模型被训练并期望根据他们的*工作经验年限*来预测员工的工资。由于工资是一个连续的数字，我们可以使用基于回归的机器学习模型来解决这种问题。
- en: 'The various regression models we will discuss are as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的各种回归模型如下：
- en: Simple linear regression
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单线性回归
- en: Multiple linear regression
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多元线性回归
- en: Polynomial regression
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式回归
- en: Support vector regression
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量回归
- en: Decision tree regression
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树回归
- en: Random forest regression
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林回归
- en: Simple linear regression
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单线性回归
- en: '**Simple linear regression** (**SLR**) takes linear data and applies feature
    scaling to it if required. **Feature scaling** is a method used to balance the
    effects of various attributes. All machine learning models are mathematical in
    nature, so before training the model with the data, we need to apply a few steps
    to make sure the predictions made are not biased.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**简单线性回归**（**SLR**）对线性数据进行特征缩放，如果需要的话。**特征缩放**是一种用来平衡各种属性影响的方法。所有的机器学习模型都是数学性质的，所以在用数据训练模型之前，我们需要应用一些步骤来确保所做的预测不会有偏差。'
- en: For example, if the dataset contains three attributes (`age`, `salary`, and `item_purchased[0/1]`),
    we as humans know that the age group that is likely to visit shops is between
    10 and 70, and the salary can range between 10,000 and 100,000 or higher. When
    making the prediction, we want to take both parameters into consideration, to
    know which age group with what salary is most likely to purchase the product.
    However, if we train the model without scaling the age and the salary to the same
    level, the value of the salary will overshadow the effect of age due to the large
    numeric difference between them. To make sure this does not happen, we apply feature
    scaling to the dataset to balance them out.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果数据集包含三个属性（`age`，`salary`，和`item_purchased[0/1]`），我们作为人类知道，可能会去商店的年龄段在10到70之间，工资可能在10,000到100,000或更高之间。在进行预测时，我们希望同时考虑这两个参数，知道哪个年龄段的人在什么工资水平下最有可能购买产品。然而，如果我们在不将年龄和工资缩放到相同水平的情况下训练模型，工资的值会因为它们之间的巨大数值差异而掩盖年龄的影响。为了确保这种情况不会发生，我们对数据集应用特征缩放来平衡它们。
- en: 'Another step required is data encoding, using a **one-hot encoder**. For example,
    if the dataset has a country attribute, this a categorical value, which, let''s
    say, has three categories: Russia, US, and UK. These words do not make sense to
    a mathematical model. Using a one-hot encoder, we transform the dataset so it
    reads (`id`, `age`, `salary`, `Russia`, `UK`, `USA`, `item_purchased`). Now, all
    the customers who have purchased the product and are from Russia would have the
    number 1 under the column named Russia, and the number 0 under the USA and UK
    columns.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个必需的步骤是数据编码，使用**独热编码器**。例如，如果数据集有一个`国家`属性，这是一个分类值，假设有三个类别：俄罗斯、美国和英国。这些词对数学模型来说没有意义。使用独热编码器，我们将数据集转换成（`id`，`age`，`salary`，`Russia`，`UK`，`USA`，`item_purchased`）。现在，所有购买产品并来自俄罗斯的顾客在名为俄罗斯的列下会有数字1，在美国和英国的列下会有数字0。
- en: 'As an example, let''s say the data initially looks as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设数据最初如下所示：
- en: '| **ID** | **Country** | **Age** | **Salary** | **Purchased** |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| **ID** | **国家** | **年龄** | **工资** | **购买** |'
- en: '| 1 | USA | 32 | 70 K | 1 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 美国 | 32 | 70 K | 1 |'
- en: '| 2 | Russia | 26 | 40 K | 1 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 俄罗斯 | 26 | 40 K | 1 |'
- en: '| 3 | UK | 32 | 80 K | 0 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 英国 | 32 | 80 K | 0 |'
- en: 'After performing the data transformations, we would get the following dataset:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 进行数据转换后，我们会得到以下数据集：
- en: '| **ID** | **Russia** | **USA** | **UK** | **Age** | **Salary** | **Purchased**
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| **ID** | **俄罗斯** | **美国** | **英国** | **年龄** | **工资** | **购买** |'
- en: '| 1 | 0 | 1 | - | 0.5 | 0.7 | 1 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 1 | - | 0.5 | 0.7 | 1 |'
- en: '| 2 | 1 | 0 | 0 | 0.4 | 0.4 | 1 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | 0 | 0 | 0.4 | 0.4 | 1 |'
- en: '| 3 | 0 | 0 | 1 | 0.5 | 0.8 | 0 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | 0 | 1 | 0.5 | 0.8 | 0 |'
- en: It can be seen that the dataset obtained is purely mathematical and so we can
    now give it to our regression model to learn from and then make predictions.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到得到的数据集是纯数学的，所以我们现在可以把它交给我们的回归模型来学习，然后进行预测。
- en: It should be noted that the input variables that help to make the prediction
    are called independent variables. In the preceding example, `country`, `age`,
    and `salary` are the independent variables. The output variable that defines the
    prediction is called the dependent variable, which is the `Purchased` column in
    our case.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，帮助进行预测的输入变量被称为自变量。在前面的例子中，`country`、`age`和`salary`是自变量。定义预测的输出变量被称为因变量，在我们的例子中是`Purchased`列。
- en: How does the regression model work?
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归模型如何工作？
- en: Our objective is to train a machine learning model on the dataset and then ask
    the model to make predictions in order to establish the salary that should be
    given to an employee based on their years of experience.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是在数据集上训练一个机器学习模型，然后要求模型进行预测，以确定应根据员工的工作经验给予的薪资。
- en: 'The example that we are considering is based on an Excel sheet. Basically,
    we have data from a company where we have a salary structure based on years of
    experience. We want our machine learning model to derive the correlation between
    the years of experience and the salary given. From the derived correlation, we
    want the model to provide future predictions and specify the modeled salary. The
    machine does this through simple linear regression. In simple linear regression,
    various lines are drawn through the given scattered data (trend lines). The idea
    of the trend line is it should best-fit (cut across) all the scattered data. After
    that, the best trend line is chosen by computing the modeled differences. This
    can be further explained as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑的例子是基于Excel表的。基本上，我们有一家公司的数据，其中薪资结构是基于工作经验年限的。我们希望我们的机器学习模型能够推导出工作经验年限和给定薪资之间的相关性。根据推导出的相关性，我们希望模型能够提供未来的预测并指定建模薪资。机器通过简单线性回归来实现这一点。在简单线性回归中，通过给定的散点数据绘制各种线条（趋势线）。趋势线的理念是它应该最佳拟合（穿过）所有的散点数据。之后，通过计算建模差异来选择最佳的趋势线。这可以进一步解释如下：
- en: '![](img/4065a88b-4f56-47b7-8862-add0c9322521.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4065a88b-4f56-47b7-8862-add0c9322521.png)'
- en: Continuing with the same example, let's take the case of an employee "e" who
    is earning a salary of 100,000 after 10 years of experience in their actual job.
    According to the model, however, the employee should be earning a little less
    than what he is actually earning, as shown by the green  `+` and the line beneath
    the green `+` is actually less than the line followed by the organization (the
    modeled salary). The green dotted line represents the difference between the actual
    salary and the modeled salary (`~=80K`). It is given by *yi -yi^*, where *yi*
    is actual salary and *yi^* is the mode.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用相同的例子，让我们以员工“e”为例，他在实际工作中拥有10年的经验，薪资为100,000。然而，根据模型，员工实际上应该获得的薪资要低一些，如绿色`+`所示，绿色`+`下方的线实际上低于组织所遵循的线（建模薪资）。绿色虚线表示实际薪资和建模薪资之间的差异（约80K）。它由*yi
    -yi^*给出，其中*yi*是实际薪资，*yi^*是模式。
- en: SLR draws all possible trend lines through your data, then computes the sum
    *(y-y^)**²* for the whole line. It then finds the minimum of the computed squares.
    The line with the minimum sum of the squares is considered to be the one that
    would best fit the data. This method is called the **least squares method** or
    the **Euclidean distance method**. The least squares method is a form of mathematical
    regression analysis that finds the line of best fit for a dataset, providing a
    visual demonstration of the relationship between the data points.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: SLR通过数据绘制所有可能的趋势线，然后计算整条线的*（y-y^）*²*的和。然后找到计算平方的最小值。具有最小平方和的线被认为是最适合数据的线。这种方法称为**最小二乘法**或**欧几里得距离法**。最小二乘法是一种数学回归分析方法，它为数据集找到最佳拟合线，提供了数据点之间关系的可视化演示。
- en: 'The following screenshot represents the various prediction lines drawn by a
    regression model:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下屏幕截图表示回归模型绘制的各种预测线：
- en: '![](img/c2866745-289f-447b-9c1a-6605e57a305b.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c2866745-289f-447b-9c1a-6605e57a305b.png)'
- en: 'Based on the sum of squares method, the best fitting line is chosen, as shown
    here:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 基于平方和方法，选择了最佳拟合线，如下所示：
- en: '![](img/59bca6d8-6d99-404d-8429-e775a15af39e.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/59bca6d8-6d99-404d-8429-e775a15af39e.png)'
- en: 'Basically, the data points plotted are not in a line, but the the actual dots
    are plotted symmetrically either side of the straight line, as shown here:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，绘制的数据点不在一条直线上，而是在直线的两侧对称绘制，如下所示：
- en: '![](img/f05273ca-db2c-4379-a092-95e7ed7a331f.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f05273ca-db2c-4379-a092-95e7ed7a331f.png)'
- en: 'The following section represents the code to implement SLR:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分代表了实现SLR的代码：
- en: '![](img/625a4c93-18bf-4808-89c7-8de1604e1995.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/625a4c93-18bf-4808-89c7-8de1604e1995.png)'
- en: Multiple linear regression
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多元线性回归
- en: SLR works on datasets that have one independent and one dependent variable.
    It plots both in *XY* dimensional space, draws trend lines based on the dataset,
    and finally makes a prediction by choosing the best fitting line. However, we
    now need to think about what would happen if the number of dependent variables
    is more than *one*. This is where multiple linear regression comes into the picture.
    **Multiple linear regression** (**MLR**) takes multiple independent variables
    and plots them over n-dimensions in order to make a prediction.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: SLR适用于具有一个自变量和一个因变量的数据集。它在*XY*维度空间中绘制两者，根据数据集绘制趋势线，最后通过选择最佳拟合线进行预测。然而，现在我们需要考虑的是如果因变量的数量超过*一个*会发生什么。这就是多元线性回归出现的地方。**多元线性回归**（**MLR**）使用多个自变量并在n维空间中绘制它们以进行预测。
- en: We will now be working on a different dataset that contains information relating
    to 50 startup companies. The data essentially consists of expenditure made on
    various verticals of the company such as R&D, administration, and marketing. It
    also indicates the state in which the company is located and the net profit made
    by each verticals . Clearly, profit is the dependent variable and the other factors
    are the independent variables.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将处理一个包含与50家初创公司相关信息的不同数据集。数据基本上包括公司在各种垂直领域（如研发、行政和营销）上的支出。它还指示了公司所在的州以及每个垂直领域的净利润。显然，利润是因变量，其他因素是自变量。
- en: 'Here, we will be acting from the perspective of an investor who wants to analyze
    various parameters and predict which verticals more revenue should be spent on,
    and in which state, in order to maximize profit. For example, there may be states
    in which spending more on R&D provides better results, or others in which spending
    more on marketing is more profitable. The model should be able to predict which
    verticals to invest in, shown as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将从投资者的角度进行分析，他想要分析各种参数，并预测应该在哪些垂直领域投入更多的收入，并在哪个州，以最大化利润。例如，可能有一些州在其中更多地投入研发会带来更好的结果，或者其他一些州在其中更多地投入营销更有利可图。该模型应该能够预测应该投资哪些垂直领域，如下所示：
- en: '![](img/f49826c0-9324-4325-a2e5-5da36f4187d5.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/f49826c0-9324-4325-a2e5-5da36f4187d5.png)
- en: 'Given that we have multiple independent variables, as shown here, it is also
    important for us to identify those that are actually useful and those that aren''t:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们有多个自变量，如下所示，对于我们来说，识别哪些是实际有用的，哪些是无用的也很重要：
- en: '![](img/82204bed-9eeb-48f0-b68f-04c601b37613.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/82204bed-9eeb-48f0-b68f-04c601b37613.png)
- en: 'While some independent variables may have an impact on the final dependent
    variable, others might not. To improve the model''s accuracy, we must eliminate
    all the variables that have a minimal impact on the dependent variable. There
    are five ways to eliminate such variables, shown in the following figure, but
    the most reliable one is **backward elimination**:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然一些自变量可能会对最终的因变量产生影响，但其他一些可能不会。为了提高模型的准确性，我们必须消除对因变量影响较小的所有变量。有五种方法可以消除这些变量，如下图所示，但最可靠的是**向后消除**： '
- en: '![](img/38446cdc-c2a3-44bf-9cbd-ea35b7243dae.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/38446cdc-c2a3-44bf-9cbd-ea35b7243dae.png)
- en: 'The working principles of backward elimination are shown here:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 向后消除的工作原理如下所示：
- en: '![](img/35ac9100-ec27-4f8e-a22f-9adfd8722ff8.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/35ac9100-ec27-4f8e-a22f-9adfd8722ff8.png)
- en: What we mean by significance level in the previous method is the minimum threshold
    value that would signify that the variable under examination is crucial to the
    dependent variable or final prediction.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面的方法中所说的显著水平是指能够表明正在检查的变量对因变量或最终预测至关重要的最低阈值值。
- en: The **P value** is the probability that determines whether the relation between
    dependent and independent variables is random. For any given variable, if the
    computed P value is equal to 0.9, this would suggest that the relation between
    that independent variable and final dependent variable is 90% random, so any change
    to the independent variable may not have a direct impact on the dependent one.
    On the other hand, if the P value for a different variable is 0.1, this means
    that the relation between this variable and the dependent one is not random in
    nature, and a change to this variable would have a direct impact on the output.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**P值**是确定因变量和自变量之间关系是否随机的概率。对于任何给定的变量，如果计算得到的P值等于0.9，这将表明该自变量与最终因变量之间的关系是90%随机的，因此对自变量的任何改变可能不会直接影响因变量。另一方面，如果另一个变量的P值为0.1，这意味着该变量与因变量之间的关系并非是随机的，对该变量的改变将直接影响输出。'
- en: 'We should start by analyzing the dataset to figure out the independent variables
    that are significant for the prediction. We must train our data model only on
    those variables. The following code snippet represents the implementation of backward
    elimination, which will give us an idea about which variables to take out and
    which to leave in:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该从分析数据集开始，找出对预测有重要意义的自变量。我们必须只在这些变量上训练我们的数据模型。以下代码片段表示了向后消除的实现，这将让我们了解哪些变量应该被排除，哪些应该被保留：
- en: '![](img/035d5bc5-1d47-4097-bfc1-2c07b2c491d1.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/035d5bc5-1d47-4097-bfc1-2c07b2c491d1.png)
- en: 'The following is the explanation for the main functions used in the preceding
    code snippet:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面代码片段中使用的主要函数的解释：
- en: '`X[:,[0,1,2,3,4,5]]` indicates that we pass all rows and columns from 90 to
    5 to the backward elimination function'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X[:,[0,1,2,3,4,5]]`表示我们将所有行和从90到5的所有列传递给向后消除函数'
- en: '`sm.OLS` is an internal Python library that helps in P value computation'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sm.OLS`是一个内部Python库，用于P值计算'
- en: '`regressor_OLS.summary()` will display a summary on the console that will help
    us decide which data variables to keep and which to leave out'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`regressor_OLS.summary()`将在控制台上显示一个摘要，帮助我们决定哪些数据变量要保留，哪些要排除'
- en: 'In the following example, we are training the model over all the variables.
    It is recommended, however, to use `X_Modeled`, as obtained before, instead of
    `X`:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的示例中，我们正在对所有变量进行模型训练。但是建议使用之前获得的`X_Modeled`，而不是`X`：
- en: '![](img/8d93203b-2aed-462a-a4cc-539647e36472.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/8d93203b-2aed-462a-a4cc-539647e36472.png)
- en: 'It should be noted that in MLR, the prediction is also made based upon the
    best fitting line, but in this case the best fitting line is plotted over multiple
    dimensions. The following screenshot gives an idea of how the dataset will be
    plotted in n-dimensional space:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在MLR中，应该注意的是，预测也是基于最佳拟合线进行的，但在这种情况下，最佳拟合线是在多个维度上绘制的。以下屏幕截图给出了数据集在n维空间中的绘制方式：
- en: '![](img/83debebc-309c-4c0a-9bad-1bf7706ccca2.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: ！[](img/83debebc-309c-4c0a-9bad-1bf7706ccca2.png)
- en: There are various other regression models that work for other types of datasets,
    but to cover them all is beyond the scope of this book. However, the two models
    mentioned should have given us an idea about how regression models work. In the
    next section, we are going to discuss **classification models**. We will look
    at one classification model in greater detail and see how we can use it in natural
    language processing to apply ML in the penetration testing ecosystem.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他各种回归模型适用于其他类型的数据集，但涵盖它们都超出了本书的范围。然而，提到的两个模型应该让我们了解回归模型的工作原理。在下一节中，我们将讨论**分类模型**。我们将更详细地研究一个分类模型，并看看我们如何在自然语言处理中使用它来应用ML在渗透测试生态系统中。
- en: Classification models
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类模型
- en: 'Unlike regression models, where the model predicts a continuous number, classification
    models are used to predict a category among a given list of categories. The business
    problem discussed previously, where we have data related to customers of an e-commerce
    website over the last three months containing the purchase history of a particular
    product as (`c_id`, `p_id`, `age`, `gender`, `nationality`, `salary`, `purchased[yes/no]`).
    Our objective, as before, is to identify a customer who would be likely to purchase
    the product based upon their purchase history. Based on the permutation of all
    independent variables (`age`, `gender`, `nationality`, `salary`), a classification
    model can make a prediction in terms of 1 and 0, 1 being the prediction that a
    given customer will purchase the product, and 0 being that they won''t. In this
    particular case, there are two categories (0 and 1). However, depending upon the
    business problem, the number of output categories may vary. The different classification
    models that are commonly used are shown here:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 与回归模型不同，回归模型预测连续数字，分类模型用于预测给定类别列表中的类别。之前讨论的业务问题，我们在过去三个月内有关电子商务网站客户的数据，其中包含特定产品的购买历史（`c_id`，`p_id`，`age`，`gender`，`nationality`，`salary`，`purchased[yes/no]`）。我们的目标与之前一样，是根据他们的购买历史来识别可能购买产品的客户。根据所有独立变量（`age`，`gender`，`nationality`，`salary`）的排列组合，分类模型可以进行1和0的预测，1表示给定客户将购买产品的预测，0表示他们不会。在这种情况下，有两个类别（0和1）。然而，根据业务问题，输出类别的数量可能会有所不同。常用的不同分类模型如下所示：
- en: Naive Bayes
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Logistic regression
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: K-nearest neighbors
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K最近邻
- en: Support vector machines
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Kernel SVM
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核SVM
- en: Decision tree classifier
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树分类器
- en: Random forest classifier
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林分类器
- en: Naive Bayes classifier
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯分类器
- en: Let's try to understand how classification models work with the help of a Naive
    Bayes classifier. In order to understand Naive Bayes classifiers, we need to understand
    the Bayes theorem. The **Bayes theorem** is the theorem we studied in probability,
    and can be explained with the help of an example.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试通过朴素贝叶斯分类器来理解分类模型的工作原理。为了理解朴素贝叶斯分类器，我们需要理解贝叶斯定理。贝叶斯定理是我们在概率中学习的定理，并可以通过一个例子来解释。
- en: Let's say that we have two machines, both of which produce spanners. The spanners
    are marked with which machine has produced them. M1 is the label for machine 1
    and M2 is the label for machine 2.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两台机器，两台机器都生产扳手。扳手上标有生产它们的机器。M1是机器1的标签，M2是机器2的标签。
- en: 'Let''s say that one spanner is defective and we want to find the probability
    that the defective spanner was produced by machine 2\. The probability of event
    A happening provided B has already occurred is determined by the Naive Bayes theorem. We
    therefore make use of the Bayes theorem as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一个扳手是有缺陷的，我们想找到有缺陷的扳手是由机器2生产的概率。提供B已经发生的情况下A发生的概率由朴素贝叶斯定理确定。因此，我们使用贝叶斯定理如下：
- en: '![](img/df7a805a-b276-42e2-993c-e7896f01041a.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df7a805a-b276-42e2-993c-e7896f01041a.png)'
- en: P(A) represents the probability of an event happening.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(A)代表事件发生的概率。
- en: p(B/A) represents the probability of B given A (the probability of B happening
    assuming that A has already happened).
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: p(B/A)代表A发生的情况下B发生的概率。
- en: P(B) represents the probability of B happening.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(B)代表B发生的概率。
- en: p(A/B) represents the probability of A given B (the probability of A happening,
    assuming that B has already happened).
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: p(A/B)代表B发生的情况下A发生的概率（假设B已经发生的情况下A发生的概率）。
- en: 'If we put the data in terms of probability, we get the following:'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们用概率来表示数据，我们得到以下结果：
- en: '| ![](img/b12e6835-1d4d-4e64-b138-5c5c9ac71843.png) | ![](img/5cdc2f99-b110-44d9-81cd-a87f6fc44aae.png)
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/b12e6835-1d4d-4e64-b138-5c5c9ac71843.png) | ![](img/5cdc2f99-b110-44d9-81cd-a87f6fc44aae.png)
    |'
- en: 'Let''s say we have a dataset of people, of whom some walk to work and some
    drive to work, depending upon the age category they fall into:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个人的数据集，有些人步行上班，有些人开车上班，这取决于他们所属的年龄类别：
- en: '| ![](img/f0956274-0f90-4867-a8f2-4dd7a3523508.png) |                     ![](img/7afadd61-6a01-4735-8e3f-114e18432885.png)
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| ![](img/f0956274-0f90-4867-a8f2-4dd7a3523508.png) |                     ![](img/7afadd61-6a01-4735-8e3f-114e18432885.png)
    |'
- en: If a new data point is added, we should be able to say whether that person drives
    to work or walks to work. This is supervised learning; we are training the machine
    on a dataset and deriving a learned model from that. We will apply Bayes theorem
    to determine the probability of the new data point belonging to the walking category
    and the driving category.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果添加了一个新的数据点，我们应该能够判断那个人是开车上班还是步行上班。这是监督学习；我们正在对数据集进行机器训练，并从中得出一个学习模型。我们将应用贝叶斯定理来确定新数据点属于步行类别和驾驶类别的概率。
- en: 'To calculate the probability of the new data point belonging to the walking
    category, we calculate *P(Walk/X).* Here, *X* represents the features of the given
    person, including their age and their salary:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算新数据点属于步行类别的概率，我们计算*P(Walk/X)*。这里，*X*代表给定人的特征，包括他们的年龄和工资：
- en: '![](img/21e1f515-fb99-4bc9-b9fd-5b76d37e936d.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21e1f515-fb99-4bc9-b9fd-5b76d37e936d.png)'
- en: 'To calculate the probability of the new data point belonging to the driving
    category, we calculate *P(Drives/X)* as shown in the following:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算新数据点属于驾驶类别的概率，我们计算*P(Drives/X)*如下所示：
- en: '![](img/f96b5bbf-4908-41d9-b8b5-7200a42d8b36.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f96b5bbf-4908-41d9-b8b5-7200a42d8b36.png)'
- en: Finally, we will compare *P(Walks/X)* and *P(Drives/X). *Based on this comparison,
    we will establish where to put the new data point (in the category in which the
    probability is higher). The initial plotting happens over n-dimensional space,
    depending upon the values of independent variables.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将比较*P(Walks/X)*和*P(Drives/X)。*基于这个比较，我们将确定在哪个类别中放置新数据点（在概率更高的类别中）。最初的绘图发生在n维空间中，取决于独立变量的值。
- en: 'Next, we compute the marginal likelihood, as shown in the following figure,
    which is P(X):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算边际似然，如下图所示，即P(X)：
- en: '![](img/68ba163a-89e4-432b-8094-7c2ca7eb70f2.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/68ba163a-89e4-432b-8094-7c2ca7eb70f2.png)'
- en: '*P(X)* actually refers to the probability of adding the new data point to a
    place that has data points with similar features. The algorithm divides or makes
    a circle around the data points that it finds are similar in features to the one
    it is about to add. Then, the probability of the features is computed as *P(X)
    =number of similar observations/Total observations*.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*P(X)*实际上是指将新数据点添加到具有相似特征数据点的概率。该算法将划分或在发现具有与即将添加的数据点相似特征的数据点周围画一个圆。然后，计算特征的概率为*P(X)
    =相似观察的数量/总观察数量*。'
- en: 'The radius of the circle is an important parameter in this case. This radius
    is given as an input parameter to the algorithm:'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 圆的半径在这种情况下是一个重要的参数。这个半径作为算法的输入参数给出：
- en: '![](img/dad55bff-e896-4df6-a2a8-2d5cb5f3efdf.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dad55bff-e896-4df6-a2a8-2d5cb5f3efdf.png)'
- en: 'In this example, all the points inside the circle are assumed to have similar
    features to the data point that is to be added. Let''s say that the data point
    that we are adding relates to someone who is 35 years old and has a salary of
    $40,000\. In this case, everybody within the bracket $25-40K would be selected
    in the circle:'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个例子中，圆内的所有点被假定具有与要添加的数据点相似的特征。假设我们要添加的数据点与一个35岁，年薪40,000美元的人相关。在这种情况下，圆内的所有人都会被选中：
- en: '![](img/91023a2b-3bab-4adc-b518-c30bdbe5fe93.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/91023a2b-3bab-4adc-b518-c30bdbe5fe93.png)'
- en: 'Next, we need to compute the likelihood, which means the probability that someone
    chosen randomly who walks contains the features of X. The following will determine
    *P(X/walks)*:'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们需要计算似然，即随机选择一个步行者包含X的特征的概率。以下将确定*P(X/walks)*：
- en: '![](img/f67595a3-a52a-4c9d-84b3-a03673837041.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f67595a3-a52a-4c9d-84b3-a03673837041.png)'
- en: We will be doing the same to derive the probability of the data point belonging
    to the driving section given that it has features identical to people who walk
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将使用相同的方法来推导数据点属于驾驶部分的概率，假设它具有与步行者相同的特征
- en: In this case, P(X) is equal to the number of similar observations that fall
    in the circle shown before, divided by the total number of observations . P(X)
    =4/30 = 0.133
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这种情况下，P(X)等于落在之前所示圆内的相似观察的数量，除以总观察数量。P(X) = 4/30 = 0.133
- en: P(drives)= P(# who drive) /(#total) =20/30 = 0.666
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(drives) = P(# who drive) / (#total) = 20/30 = 0.666
- en: P(X|Drivers) = P (similar observations that are drivers) /total drivers = 1/20
    =0.05
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P(X|Drivers) = P(相似的驾驶员观察) / 总驾驶员 = 1/20 = 0.05
- en: Applying the values we get P(Drivers|X) =0.05 *0.666 /0.133 =0.25 =&gt;25
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用我们得到的值，得到P(Drivers|X) = 0.05 * 0.666 / 0.133 = 0.25 => 25
- en: For the given problem, we will assume that the data point will belong to the
    set of walkers.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的问题，我们将假设数据点属于步行者的集合。
- en: Summarizing the Naive Bayes classifier
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结朴素贝叶斯分类器
- en: 'The following bullet points put all of the concepts discussed so far into perspective,
    to summarize what we have learned about the Naive Bayes classifier:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下项目将迄今讨论的所有概念整合起来，总结了我们对朴素贝叶斯分类器的学习：
- en: It should be noted that the Naive Bayes classifier does not have a computed
    model that is obtained after training. In fact, at the time of prediction, all
    the data points are simply labeled according to which class they belong to.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该注意的是，朴素贝叶斯分类器在训练后并没有一个计算出的模型。事实上，在预测时，所有数据点只是根据它们属于哪个类别进行简单的标记。
- en: At the time of prediction, based on the values of the independent variables,
    a data point would be computed and plotted at a particular place in the n-dimensional
    space. The aim is to predict which class a data point belongs to among N classes.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预测时，根据独立变量的值，数据点将在n维空间中的特定位置计算并绘制。目标是预测数据点在N个类别中属于哪个类别。
- en: Based on the independent variables, the data point will be plotted in vector
    space in close proximity to data points of similar features. However, this still
    does not determine which class the data point belongs to.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于独立变量，数据点将在接近具有相似特征的数据点的向量空间中绘制。然而，这仍然不能确定数据点属于哪个类别。
- en: Based on the initially chosen optimal value of the radius, a circle would be
    drawn around that data point, encapsulating a few other points within the proximity
    of the radius of the circle.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据最初选择的最佳半径值，将在该数据点周围画一个圆，将圆的半径内的一些其他点包围起来。
- en: Let's say we have two classes, A and B, and we need to determine the class for
    the new data point X. Bayes theorem will be used to determine the probability
    of X belonging to class A and the probability of X belonging to class B. The one
    that has the higher probability is the class in which the data point is predicted
    to belong.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设我们有两个类别，A和B，我们需要确定新数据点X的类别。贝叶斯定理将用于确定X属于类A的概率和X属于类B的概率。具有更高概率的那个类别就是预测数据点所属的类别。
- en: Implementation code
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现代码
- en: 'Let''s assume that we have a car company X that holds some data on people,
    which contains their age, their salary, and other information. It also has details
    about whether the person has purchased an SUV that the company has launched at
    a very expensive price. This data is used to help them understand who buys their
    cars:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一家名为X的汽车公司，拥有一些关于人们的数据，包括他们的年龄、薪水和其他信息。它还包括关于这些人是否购买了公司以非常昂贵的价格推出的SUV的详细信息。这些数据用于帮助他们了解谁购买了他们的汽车：
- en: '![](img/124e9018-1f28-4010-9421-bda54f403750.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/124e9018-1f28-4010-9421-bda54f403750.png)'
- en: 'We will use the same data to train our model so that it can predict whether
    a person will buy a car, given their `age`, `salary`, and `gender`:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用相同的数据来训练我们的模型，以便它可以预测一个人是否会购买一辆汽车，给定他们的`年龄`、`薪水`和`性别`：
- en: '![](img/db67f62a-5f20-48e8-b463-bf9599ded9c7.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/db67f62a-5f20-48e8-b463-bf9599ded9c7.png)'
- en: 'The following screenshot shows the difference between `y_pred` and `y_test`
    for the first 12 data points:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了前12个数据点的`y_pred`和`y_test`之间的差异：
- en: '| ![](img/e31d4c8a-3e8d-4908-96f0-2995a35ed172.png) |                                                  
    ![](img/fcc87af1-e5dd-423e-986a-75ad337a02e6.png)The previous screenshot represents
    the output of the confusion matrix.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '| ![](img/e31d4c8a-3e8d-4908-96f0-2995a35ed172.png) | ![](img/fcc87af1-e5dd-423e-986a-75ad337a02e6.png)前面的截图代表了混淆矩阵的输出。'
- en: The cell [0,0] represents the total cases where the output was 0 and was predicted
    as 0.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元格[0,0]代表了输出为0且被预测为0的总案例。
- en: The cell [0,1] represents the total cases where the output was 0 but was predicted
    as 1.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元格[0,1]代表了输出为0但被预测为1的总案例。
- en: The cell [1,0] represents the total cases where the output was 1 but was predicted
    as 0.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元格[1,0]代表了输出为1但被预测为0的总案例。
- en: The cell [1,1] represents the total cases where the output was 1 and was predicted
    as 1.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单元格[1,1]代表了输出为1且被预测为1的总案例。
- en: If we take the statistics from the previous dataset, we can see that out of
    the 100 predictions, 90 were correct and 10 were wrong, giving us an accuracy
    of 90%.                      |
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从先前的数据集中获取统计数据，我们可以看到在100次预测中，有90次是正确的，10次是错误的，给出了90%的准确率。
- en: Natural language processing
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理
- en: '**Natural language processing** (**NLP**) is about analyzing text, articles
    and involves carrying out predictive analysis on textual data. The algorithm we
    make will address a simple problem, but the same concept is applicable to any
    text. We can also predict the genre of a book with NLP.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**自然语言处理**（**NLP**）是关于分析文本、文章并进行对文本数据的预测分析。我们将制作的算法将解决一个简单的问题，但相同的概念适用于任何文本。我们也可以使用NLP来预测一本书的类型。'
- en: 'Consider the following Tab Separated Values (TSV), which is a tab-delimited
    dataset for us to apply NLP to and see how it works:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下的Tab分隔值（TSV），这是一个用于应用NLP并查看其工作原理的制表符分隔的数据集：
- en: '![](img/3dfcd7e2-6f90-4a5b-8d50-b606750de3fe.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3dfcd7e2-6f90-4a5b-8d50-b606750de3fe.png)'
- en: This is a small portion of the data we will be working on. In this case, the
    data represents customer reviews about a restaurant. The reviews are given as
    text, and they have a rating, which is 0 or 1 to indicate whether the customer
    liked the restaurant or not. 1 would mean the review is positive and 0 would indicate
    that it's not positive.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们将要处理的数据的一小部分。在这种情况下，数据代表了关于餐厅的顾客评论。评论以文本形式给出，并且有一个评分，即0或1，表示顾客是否喜欢这家餐厅。1表示评论是积极的，0表示不是积极的。
- en: Usually, we would use a CSV file. Here, however, we are using a TSV file where
    the delimiter is a tab because we are working on text-based data, so we may have
    commas that don't indicate a separator. If we take the 14^(th) record, for example,
    we can see a comma in the text. Had this been a CSV file, Python would have taken
    the first half of the sentence as the review and the second half as a rating,
    while the `1` would have been taken as a new review. This would mess up the whole
    model.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们会使用CSV文件。然而，在这里，我们使用的是TSV文件，分隔符是制表符，因为我们正在处理基于文本的数据，所以可能会有逗号，这些逗号并不表示分隔符。例如，如果我们看第14条记录，我们可以看到文本中有一个逗号。如果这是一个CSV文件，Python会将句子的前半部分作为评论，后半部分作为评分，而`1`会被视为一个新的评论。这将破坏整个模型。
- en: 'The dataset has got around 1,000 reviews and has been labeled manually. Since
    we are importing a TSV file, some parameters of `pandas.read_csv` will need to
    change. First of all, we specify that the delimiters are tab separated, using /t.
    We should also ignore double quotes, which can be done by specifying parameter
    quoting=3:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集大约有1,000条评论，并且已经被手动标记。由于我们正在导入一个TSV文件，`pandas.read_csv`的一些参数需要更改。首先，我们指定分隔符是制表符分隔的，使用/t。我们还应该忽略双引号，可以通过指定参数quoting=3来实现：
- en: '![](img/df6b9cce-a6df-4b7b-a18b-b20e9854ad23.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/df6b9cce-a6df-4b7b-a18b-b20e9854ad23.png)'
- en: 'The imported dataset is shown here:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 导入的数据集如下所示：
- en: '![](img/d1cc2af3-48f2-4eb9-b875-e35a85069898.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d1cc2af3-48f2-4eb9-b875-e35a85069898.png)'
- en: We can see that the 1,000 reviews have been imported successfully. All the reviews
    are in the review column and all the ratings are in the **Liked** column. In NLP,
    we have to clean text-based data before we use it. This is because NLP algorithms
    work using the bag of words concept, which means that only the words that lead
    to a prediction are maintained. The bag of words actually contains only the relevant
    words that impact the prediction. Words such as `a`, `the`, `on`, and so on are
    considered to be irrelevant in this context. We also get rid of dots and numbers
    unless numbers are needed, and apply stemming on the words. An example of stemming
    would be taking the word `love` in place of `loved`. The reason why we apply stemming
    is because we don't want to have too many words in the end, and also to regroup
    words such as `loving` and `loved` to one word, `love`. We also remove the capital
    letters and have everything in lowercase. To apply our bag-of-words model, we
    need to apply tokenization. After we do this, we will have different words, because
    the pre-processing will have got rid of those that are irrelevant.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到成功导入了1,000条评论。所有评论都在评论列中，所有评分都在**Liked**列中。在NLP中，我们必须在使用文本数据之前对其进行清理。这是因为NLP算法使用词袋概念工作，这意味着只保留导致预测的单词。词袋实际上只包含影响预测的相关单词。例如`a`，`the`，`on`等单词在这种情况下被认为是不相关的。我们还摆脱点和数字，除非需要数字，并对单词进行词干处理。词干处理的一个例子是用`love`代替`loved`。我们应用词干处理的原因是因为我们不希望最终有太多的单词，并且还要将`loving`和`loved`等单词重新组合成一个单词`love`。我们还去掉大写字母，并将所有内容转换为小写。要应用我们的词袋模型，我们需要应用标记化。这样做后，我们将有不同的单词，因为预处理将消除不相关的单词。
- en: Then, we take all the words of the different reviews and make one column for
    each word. There are likely to be many columns as there may be many different
    words in the reviews. Then, for each review, each column would contain a number
    that indicates the number of times that word has occurred in that specific review.
    This kind of matrix is called a sparse matrix, as there is likely to be lots of
    zeros in the dataset.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们取出不同评论的所有单词，并为每个单词创建一列。可能会有许多列，因为评论中可能有许多不同的单词。然后，对于每条评论，每个列将包含一个数字，指示该特定评论中该单词出现的次数。这种类型的矩阵称为稀疏矩阵，因为数据集中可能有许多零。
- en: 'The `dataset[''Review''][0]` command will give us the first review:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`dataset[''Review''][0]`命令将给出我们的第一条评论：'
- en: '![](img/d3908299-bf7b-4646-935b-bab54b2ad262.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3908299-bf7b-4646-935b-bab54b2ad262.png)'
- en: 'We use a sub module of regular expressions, as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用正则表达式的一个子模块，如下所示：
- en: '![](img/b6819ab8-f6ae-4d4f-b8a4-4bf010baa0fd.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b6819ab8-f6ae-4d4f-b8a4-4bf010baa0fd.png)'
- en: 'The sub module we are using is called a subtract function. This subtracts specified
    characters from our input string. It can also club words together and replace
    the specified characters with a character of your choice. The characters to be
    replaced can either be input as a string or in regular expression format. In regular
    expression format shown in the previous example, the ^ sign means not and [a-zA-Z]
    means everything other then a-z and A-Z should be replaced by a single space `''
    ''`. In the given string, the dots will be removed and replaced by spaces, producing
    this output: `Wow Loved this place`.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用的子模块称为减法函数。这将从我们的输入字符串中减去指定的字符。它还可以将单词组合在一起，并用您选择的字符替换指定的字符。要替换的字符可以输入为字符串，也可以输入为正则表达式格式。在前面的示例中，正则表达式格式中的^符号表示不，[a-zA-Z]表示除a-z和A-Z之外的所有内容应该被一个空格'
    '替换。在给定的字符串中，点将被移除并替换为空格，产生以下输出：`Wow Loved this place`。
- en: 'We now remove all non-significant words such as `the`, `a`, `this`, and so
    on. To do this, we will use the `nltk` library (natural language toolkit). This
    has a sub module called stopwords, which contains all the words (generic words
    ) that are mostly irrelevant with regard to fetching the meaning of the sentence.
    To download stopwords, we use the following command:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在删除所有不重要的单词，例如`the`，`a`，`this`等。为此，我们将使用`nltk`库（自然语言工具包）。它有一个名为stopwords的子模块，其中包含所有与句子意义获取无关的单词（通用单词）。要下载停用词，我们使用以下命令：
- en: '![](img/49314ee0-a4a3-4823-948d-d286336a9418.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/49314ee0-a4a3-4823-948d-d286336a9418.png)'
- en: 'This downloads stop words to the current path from where they can be used directly. First,
    we break the reviews into a list of words and then we move through the different
    words and compare them with the downloaded stopwords, removing those that are
    unnecessary:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从当前路径下载停用词，然后可以直接使用它们。首先，我们将评论分成单词列表，然后我们遍历不同的单词，并将它们与下载的停用词进行比较，删除那些不必要的单词：
- en: '![](img/15bff5a3-c53b-4677-a0bd-7eac69eff2da.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15bff5a3-c53b-4677-a0bd-7eac69eff2da.png)'
- en: In the previous code snippet, we are using a for loop. Declaring the `[]` sign
    in front of review signifies that the list will contain the words that will be
    returned from the for loop, which are the stopwords in this case.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们正在使用一个for循环。在review前面声明`[]`符号表示列表将包含从for循环返回的单词，这些单词在这种情况下是停用词。
- en: The code preceding the `for` loop indicates that we should assign the string
    word, and update the list with new words every time that word is present in the
    review list and not present in the `stopwords.words('English')` list. Note that
    we are making use of the `set()` function to actually convert the given stop word
    list to a set, because in Python the search operation over sets is much faster
    than over lists. Finally, the review will hold the string with our irrelevant
    words. In this case, for the first review, it will hold [`wov`, `loved`, `place`].
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在`for`循环之前的代码表示我们应该分配字符串单词，并且每次单词出现在评论列表中并且不出现在`stopwords.words('English')`列表中时，更新列表中的新单词。请注意，我们正在使用`set()`函数将给定的停用词列表实际转换为集合，因为在Python中，集合上的搜索操作比列表快得多。最后，评论将包含我们的无关紧要的单词。在这种情况下，对于第一条评论，它将包含[`wov`，`loved`，`place`]。
- en: The next step is to perform stemming. The reason why we apply stemming is to
    avoid sparsity, which occurs when we have lots and lots of zeros in our matrix
    (known as a sparse matrix). To reduce sparsity, we need to reduce the proportion
    of zeros in the matrix.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是进行词干提取。我们应用词干提取的原因是为了避免稀疏性，即在我们的矩阵中有大量的零（称为稀疏矩阵）时发生的情况。为了减少稀疏性，我们需要减少矩阵中零的比例。
- en: 'We will use the portstemmer library to apply stemming to each word:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用portstemmer库对每个单词应用词干提取：
- en: '![](img/ec31b7c4-d334-4a2f-9abb-efd9a0340a7b.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ec31b7c4-d334-4a2f-9abb-efd9a0340a7b.png)'
- en: Now, the review will hold [`wov`, `love`, `place`].
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，评论将包含[`wov`, `love`, `place`]。
- en: In this step, we will join the transformed string review from the list back
    to a string by calling `join`. We will put a space as the `delimiter` `' '.join(review)` to
    join all the words in the review list together and then we use `' '` as a separator
    to separate the words.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们将通过调用`join`将列表中转换后的字符串评论连接成一个字符串。我们将使用空格作为`delimiter` `' '.join(review)`将评论列表中的所有单词连接在一起，然后我们使用`'
    '`作为分隔符来分隔单词。
- en: 'The review is now a string of relevant words all in lowercase:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在评论是一个包含所有小写相关单词的字符串：
- en: '![](img/30639f06-fc93-482a-a023-5c550e3188dc.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/30639f06-fc93-482a-a023-5c550e3188dc.png)'
- en: 'After executing the code, if we compare the original dataset and the obtained
    corpus list, we will obtain the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码后，如果我们比较原始数据集和获得的语料库列表，我们将得到以下结果：
- en: '![](img/b5544e88-4137-4d4a-b4bc-ee9ac777d7e7.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b5544e88-4137-4d4a-b4bc-ee9ac777d7e7.png)'
- en: Since the stopword list also had the word `Not`, the string at index 1, `Crust
    is not good` (which had a `Liked` rating of 0), became `crust good`. We need to
    make sure that this does not happen. Likewise, `would not go back` became `would
    go back`. One of the ways to handle it would be to use a stop word list as `set(stopwords.words('english'))]`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于停用词列表中也有单词`Not`，索引1处的字符串`Crust is not good`（`Liked`评分为0）变为了`crust good`。我们需要确保这不会发生。同样，`would
    not go back`变成了`would go back`。处理它的一种方法是使用一个停用词列表，如`set(stopwords.words('english'))]`。
- en: Next, we will create a bag of words model. Here, the different words from the
    obtained corpus (list of sentences) would be taken, and a column would be made
    for each distinct word. None of the words will be repeated.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个词袋模型。在这里，将使用获得的语料库（句子列表）中的不同单词，并为每个不同的单词创建一列。不会重复任何单词。
- en: Thus, words such as `wov love place`, `crust good`, `tasti textur nasti`, and
    so on will be taken and a column will be made for each. Each column will correspond
    to a different word. We will also have the review comment and an entry number,
    specifying how many times the word has existed in that specific review.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，诸如`wov love place`，`crust good`，`tasti textur nasti`等单词将被取出，并为每个单词创建一列。每一列将对应一个不同的单词。我们还将有评论和一个条目编号，指定该特定评论中单词存在的次数。
- en: With this kind of setup, there would be many zeros in our table because there
    may be words that do not appear frequently. The objective should always be to
    keep sparsity to a minimum, such that only the relevant words point to a prediction.
    This will yield a better model. The sparse matrix we have just created will be
    our bag of words model, and it works just like our classification model. We have
    some independent variables that take some values (in this case, the independent
    variables are the review words) and, based on the values of the independent variables,
    we will predict the dependent variables, which is if the review is positive or
    not. To create our bag of words model, we will apply a classification model to
    predict whether each new review is positive or negative. We will create a bag
    of words model with the help of tokenization and a tool called **CountVectoriser**.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这种设置，我们的表中会有很多零，因为可能有一些单词并不经常出现。目标应该始终是将稀疏性保持到最低，这样只有相关的单词才能指向预测。这将产生一个更好的模型。我们刚刚创建的稀疏矩阵将成为我们的词袋模型，并且它的工作方式就像我们的分类模型一样。我们有一些独立变量取一些值（在这种情况下，独立变量是评论单词），并且根据独立变量的值，我们将预测依赖变量，即评论是积极的还是否定的。为了创建我们的词袋模型，我们将应用一个分类模型来预测每个新评论是积极的还是消极的。我们将使用标记化和一个名为**CountVectoriser**的工具来创建一个词袋模型。
- en: 'We will use the following code to use this library:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下代码来使用这个库：
- en: '[PRE0]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will create an instance of this class. The parameters take stop words
    as one of the arguments, but since we have already applied stop words to our dataset,
    we do not need to do that again. This class also allows us to control the case
    and the token pattern. We could have chosen to perform all the steps before with
    this class as well, but doing it separately gives better granular control:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建这个类的一个实例。参数中有一个停用词作为其中一个参数，但由于我们已经将停用词应用到我们的数据集中，我们不需要再次这样做。这个类还允许我们控制大小写和标记模式。我们也可以选择使用这个类来执行之前的所有步骤，但是分开执行可以更好地进行细粒度控制。
- en: '![](img/cb89493e-113d-4c2b-8dd6-233afab1ec52.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb89493e-113d-4c2b-8dd6-233afab1ec52.png)'
- en: Note that the line `cv.fit_transform` will actually fit the sparse matrix to
    cv and return  a matrix of features that has all the words of the corpus.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，行`cv.fit_transform`实际上会将稀疏矩阵拟合到cv，并返回一个具有语料库中所有单词的特征矩阵。
- en: Up until now, we have made our bag of words, or sparse matrix, a matrix of independent
    variables. The next step is to use a classification model and train the model
    over a part of the bag of words, -X, and the dependent variable over the same
    indexes, -Y. The dependent variable in this case is the `Liked` column.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经制作了我们的词袋，或者稀疏矩阵，一个独立变量的矩阵。下一步是使用分类模型，并在词袋的一部分-X上训练模型，以及在相同索引上的依赖变量-Y。在这种情况下，依赖变量是`Liked`列。
- en: Executing the preceding code will create a matrix of features with around 1,565
    features (different columns). If the number of distinct features come out to be
    very large, we can limit the max features and specify a maximum threshold number.
    Let's say that if we specify the threshold number to be 1,500, then only 1,500
    features or distinct words will be taken in the sparse matrix and those that are
    less frequent as compared to the first 1,500 would get removed. This would make
    a better correlation between the independent and dependent variables, further
    reducing sparsity.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述代码将创建一个包含大约1565个特征（不同列）的特征矩阵。如果不同特征的数量非常大，我们可以限制最大特征并指定最大阈值。假设我们将阈值数指定为1500，那么只有1500个特征或不同的单词将被纳入稀疏矩阵，那些与前1500个相比较少的将被移除。这将更好地相关独立和因变量，进一步减少稀疏性。
- en: 'We now need to train our classification model on the bag of model words and
    the dependent variables:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要在词袋模型单词和因变量上训练我们的分类模型：
- en: 'Extract the dependent variable as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 提取因变量如下：
- en: '![](img/5db0ad4c-bc11-414b-9d54-43e3d95031dd.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5db0ad4c-bc11-414b-9d54-43e3d95031dd.png)'
- en: '`X` and `Y` would look as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`X`和`Y`如下所示：'
- en: '![](img/9d2394b1-f70c-4465-9d54-a54adc8e28f1.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d2394b1-f70c-4465-9d54-a54adc8e28f1.png)'
- en: 'Note that in the previous case, each index (0-1499) corresponds to a word in
    the original corpus list. We now have exactly what we had in the classification
    model: a metric of independent variables and a result, 0 for a negative review
    and 1 for a positive review. However, we have still got a significant amount of
    sparsity.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的情况下，每个索引（0-1499）对应于原始语料库列表中的一个单词。我们现在拥有了分类模型中的内容：独立变量和结果的度量，负面评价为0，正面评价为1。然而，我们仍然有相当多的稀疏性。
- en: 'The next step for us is to make use of a classification model for training. There
    are two ways to use classifications models. One way is to test all the classification
    models against our dataset and determine false positives and false negatives,
    and the other method is based on experience and past experiments. The most common
    models used alongside NLP are Naive Bayes and decision trees or random forest
    classification. In this tutorial, we will be using a Naive Bayes model:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的下一步是利用分类模型进行训练。有两种使用分类模型的方法。一种方法是测试所有分类模型并确定假阳性和假阴性，另一种方法是基于经验和过去的实验。在NLP中最常用的模型是朴素贝叶斯和决策树或随机森林分类。在本教程中，我们将使用朴素贝叶斯模型：
- en: '![](img/5cd1b943-ad4e-49b2-822a-965ab3b6baae.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5cd1b943-ad4e-49b2-822a-965ab3b6baae.png)'
- en: 'The whole code is shown here:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 整个代码如下所示：
- en: '![](img/f1d38bb1-4220-4957-8226-b5f71e06b262.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f1d38bb1-4220-4957-8226-b5f71e06b262.png)'
- en: 'From the preceding code, we can see that we are splitting the train and test
    sets as 80% and 20%. We will give 800 observations to the training set and 200
    observations to the test set, and see how our model will behave. The value of
    the confusion metric after the execution is given as following:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 从上述代码中，我们可以看到我们将训练集和测试集分为80%和20%。我们将给训练集800个观察值，测试集200个观察值，并查看我们的模型将如何表现。执行后的混淆矩阵的值如下：
- en: '![](img/13c1413a-f21b-41c4-97cb-8b840c48a641.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13c1413a-f21b-41c4-97cb-8b840c48a641.png)'
- en: There are 55 correct predictions for negative reviews and 91 correct predictions
    for positive reviews. There are 42 incorrect predictions for negative reviews
    and 12 incorrect predictions for positive reviews. Therefore, out of 200 predictions,
    there are 146 total correct predictions, which is equal to 73%.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 负面评价有55个正确预测，正面评价有91个正确预测。负面评价有42个错误预测，正面评价有12个错误预测。因此，在200次预测中，有146次正确预测，相当于73%。
- en: Using natural language processing with penetration testing reports
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自然语言处理处理渗透测试报告
- en: One of the applications of ML in the cyber security space that I have experimented
    with is automating the task of report analysis to find vulnerabilities. We now
    know how the vulnerability scanner that we built in the last chapter works, but
    the amount of data produced by all the integrated scripts and tools is enormous
    and we need to deal with it or analyze it manually. What happens in typical scanners
    such as Nessus or Qualys is that the plugins are actually scripts. Since they
    are developed in-house by Nessus and Qualys, the scripts are designed to find
    flaws and report them in a manner that can be easily understood. However, in our
    case, we are integrating many open source scripts and tool sets, and the output
    produced is not integrated. In order to automate this task and get an overview
    of the vulnerabilities, we need to figure out the output the script or tool produces,
    in a scenario where it flags a vulnerability, and also in a scenario where the
    results returned are safe. Based on our understanding and the expected output
    patterns of each script, we have to draft our Python code logic to discover which
    plugin produced unsafe check results and which returned safe checks. This requires
    a huge amount of effort. Any time we increase the number of integrated scripts,
    the logic of our code also needs to be updated, so it is up to you whether you
    want to follow this path.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我在网络安全领域中使用ML的一个应用是自动化报告分析以发现漏洞。我们现在知道上一章中构建的漏洞扫描器是如何工作的，但所有集成脚本和工具产生的数据量巨大，我们需要手动处理或分析它。在Typical
    scanners如Nessus或Qualys中，插件实际上是脚本。由于它们是由Nessus和Qualys内部开发的，这些脚本旨在发现缺陷并以易于理解的方式报告它们。然而，在我们的情况下，我们正在集成许多开源脚本和工具集，并且产生的输出并不是集成的。为了自动化这项任务并获得漏洞的概述，我们需要弄清楚脚本或工具产生的输出，在标记漏洞的情况下，以及在返回安全检查的情况下。根据我们的理解和每个脚本的预期输出模式，我们必须起草我们的Python代码逻辑，以发现哪个插件产生了不安全的检查结果，哪个返回了安全检查。这需要大量的工作。每当我们增加集成脚本的数量时，我们的代码逻辑也需要更新，所以你可以选择是否要走这条路。
- en: The other method we have at hand is to make use of machine learning and NLP.
    Since there is a huge pool of historic pentesting data that is available to us,
    why not feed it to a machine learning model and train it to understand what is
    unsafe and what is safe? Thanks to the historic penetration testing reports that
    we have performed with our vulnerability scanner, we have an awful lot of data
    in our database tables. We can try to reuse this data to automate manual report
    analysis using machine learning and NLP. We are talking about supervised learning,
    which requires a one-time effort to tag the data appropriately. Let's say that
    we take the  historic data of the last 10 penetration tests we conducted, with
    an average of three IPs to be tested in each. Let's also assume that on average
    we executed 100 scripts per IP (depending on the number of open ports). This means
    that we have the data of 3,000 scripts.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们手头还有另一种方法，那就是利用机器学习和NLP。由于我们可以获得大量的历史渗透测试数据，为什么不将其提供给机器学习模型，并训练它理解什么是不安全的，什么是安全的呢？多亏了我们使用漏洞扫描器执行的历史渗透测试报告，我们的数据库表中有大量数据。我们可以尝试重用这些数据，利用机器学习和NLP自动化手动报告分析。我们谈论的是监督学习，它需要一次性的工作来适当地标记数据。假设我们拿过去进行的最后10次渗透测试的历史数据，每次测试平均有3个IP。我们还假设每个IP平均执行100个脚本（取决于开放端口的数量）。这意味着我们有3000个脚本的数据。
- en: We would need to tag the results manually. Alternatively, if the tester is presented
    with the data in a user interface, while testing, the tester can select **vulnerable**/**not
    vulnerable** with the help of a checkbox, which will act as a tag to the data
    presented. Let's say that we are able to tag all the result data with 1 where
    the test case or check resulted as safe, and 0 where the test case resulted as
    unsafe. We would then have tagged data that will be pre-processed and given to
    our NLP model, which will receive training on it. Once the model is trained, we
    persist the model. Finally, during live scanning, we pass the results of the test
    case to our trained model, making it carry out predictions for test cases that
    result as vulnerable against the ones that don't. The tester then only needs to focus
    on the vulnerable test cases and prepare their exploitation steps.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要手动标记结果。或者，如果测试人员在用户界面中呈现数据时，可以通过复选框选择**易受攻击**/**不易受攻击**，这将作为数据的标记。假设我们能够将所有结果数据标记为1，表示测试用例或检查结果安全，标记为0，表示测试用例结果不安全。然后我们将得到标记的数据进行预处理，并提供给我们的NLP模型进行训练。一旦模型训练完成，我们就会持久化模型。最后，在实时扫描期间，我们将测试用例的结果传递给我们训练好的模型，让它对结果易受攻击的测试用例进行预测。测试人员只需要专注于易受攻击的测试用例，并准备其利用步骤。
- en: For us to demonstrate a POC for this concept, let's take the results from one
    project, and consider only the scripts that ran for `ssl` and `http`. Let's see
    the code in action.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示这个概念的POC，让我们拿一个项目的结果，并只考虑运行`ssl`和`http`的脚本。让我们看看代码的运行情况。
- en: Step 1– tagging the raw data
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1步-标记原始数据
- en: 'The following is the output of the `ssl` and `http` checks we did on one of
    the projects we scanned with our vulnerability scanner. The data is obtained from
    the backend IPexploits table and is tagged with 0 where the check was not vulnerable
    and 1 where the test was unsafe. We can see this in the following screenshot.
    This is a TSV file with the schema (`command_id`, `recored_id`, `service_result`,
    `vul[0/1]`):'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们使用漏洞扫描器扫描的一个项目上进行的`ssl`和`http`检查的输出。数据是从后端IPexploits表中获取的，并且标记为0表示检查不容易受攻击，标记为1表示测试不安全。我们可以在以下截图中看到这一点。这是一个带有模式（`command_id`，`recored_id`，`service_result`，`vul[0/1]`）的TSV文件：
- en: '![](img/96902653-c1ab-4eba-86c5-fa3b73031522.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/96902653-c1ab-4eba-86c5-fa3b73031522.png)'
- en: Now that we have tagged the data, let's process and clean it. After that, we
    will train our NLP model with it. We will be using a Naive Bayes classifier with
    NLP. I have had decent success with this model for the current dataset. It would
    be a good exercise to test various other models and see whether we can achieve
    a better prediction success rate.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经标记了数据，让我们处理和清理它。之后，我们将用它来训练我们的NLP模型。我们将使用NLP的朴素贝叶斯分类器。我在当前数据集上使用这个模型取得了不错的成功。测试各种其他模型并看看是否能够获得更好的预测成功率将是一个很好的练习。
- en: Step 2–writing the code to train and test our model
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2步-编写训练和测试模型的代码
- en: 'The following code is identical to what we discussed in the section on NLP,
    with a few additions where we are using `pickle.dump` to save the trained model
    in a file. We also use `pickle.load` to load the saved model:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码与我们在NLP部分讨论的内容完全相同，只是在使用`pickle.dump`将训练好的模型保存到文件中时添加了一些内容。我们还使用`pickle.load`来加载保存的模型：
- en: '![](img/ee1568fe-3ca7-4213-937c-b9d793dee0b1.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee1568fe-3ca7-4213-937c-b9d793dee0b1.png)'
- en: '![](img/29912ad7-0f20-4edd-9828-08b7e55b8c10.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/29912ad7-0f20-4edd-9828-08b7e55b8c10.png)'
- en: '![](img/ae164868-e89d-4e97-864b-f07ef3f48571.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae164868-e89d-4e97-864b-f07ef3f48571.png)'
- en: 'The following screenshot shows the results, in the form of a confusion matrix
    given by our trained model for the dataset. We trained the model on 80% of our
    dataset, specified by 0.8, and tested it on 20%, specified by 0.2\. The result
    set obtained suggests that we have a 92% accuracy rate with the model prediction.
    It should be noted that the accuracy may vary for a larger dataset. The idea here
    was to give you an understanding of how NLP can be used with penetration testing
    reports. We can improve the processing to give cleaner data and change the choice
    of model to arrive at better results:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了我们训练模型为数据集提供的混淆矩阵的结果。我们在80%的数据集上训练了模型，并在20%的数据集上进行了测试。得到的结果表明，我们的模型预测准确率为92%。需要注意的是，对于更大的数据集，准确性可能会有所不同。这里的想法是让您了解NLP如何与渗透测试报告一起使用。我们可以改进处理以提供更干净的数据，并改变模型选择以获得更好的结果：
- en: '![](img/9f2f6f9c-797b-4166-b014-6d0004572b07.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9f2f6f9c-797b-4166-b014-6d0004572b07.png)'
- en: Summary
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed using ML with Python and how we can apply it to
    the cyber security domain. There are many other wonderful applications of data
    science and ML in the cyber security space related to log analysis, traffic monitoring,
    anomaly detection, data exfiltration, URL analysis, spam detection, and so on.
    Modern SIEM solutions are mostly built on top of machine learning, and a big data
    engine is used to reduce human analysis in monitoring. Refer to the further reading
    section to see the various other use cases of machine learning with cyber security.
    It must also be noted that it is important for pen testers to have an understanding
    of machine learning, in order to find vulnerabilities.  In the next chapter, the
    user is going to understand how they can use Python to automate various web application
    attack categories, which include SQLI, XSS, CSRF, and clickjacking.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了如何使用Python进行机器学习，以及如何将其应用于网络安全领域。在网络安全领域中，数据科学和机器学习有许多其他精彩的应用，涉及日志分析、流量监控、异常检测、数据外泄、URL分析、垃圾邮件检测等。现代SIEM解决方案大多建立在机器学习之上，并且使用大数据引擎来减少人工分析。请参考进一步阅读部分，了解机器学习在网络安全中的其他用例。还必须注意的是，渗透测试人员有必要了解机器学习，以便发现漏洞。在下一章中，用户将了解如何使用Python自动化各种网络应用攻击类型，包括SQLI、XSS、CSRF和点击劫持。
- en: Questions
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What are the various vulnerabilities associated with machine learning?
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与机器学习相关的各种漏洞是什么？
- en: What is big data and what is an example of a big data product with known vulnerabilities?
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是大数据，有哪些已知漏洞的大数据产品示例？
- en: What is the difference between machine learning and artificial intelligence?
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 机器学习和人工智能之间有什么区别？
- en: Which pentesting tools use machine learning and why?
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪些渗透测试工具使用机器学习，以及原因？
- en: Further reading
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Detecting phishing websites with machine learning: [https://github.com/abhishekdid/detecting-phishing-websites](https://github.com/abhishekdid/detecting-phishing-websites)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习检测钓鱼网站：[https://github.com/abhishekdid/detecting-phishing-websites](https://github.com/abhishekdid/detecting-phishing-websites)
- en: 'Using machine learning for log analysis: [https://github.com/logpai](https://github.com/logpai)'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习进行日志分析：[https://github.com/logpai](https://github.com/logpai)
- en: 'NLP for cyber security: [https://www.recordedfuture.com/machine-learning-](https://www.recordedfuture.com/machine-learning-cybersecurity-applications/)[cybersecurity-applications/](https://www.recordedfuture.com/machine-learning-cybersecurity-applications/)'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络安全的自然语言处理：[https://www.recordedfuture.com/machine-learning-cybersecurity-applications/](https://www.recordedfuture.com/machine-learning-cybersecurity-applications/)
- en: 'Spam detection using machine learning: [https://github.com/Meenapintu/Spam-Detection](https://github.com/Meenapintu/Spam-Detection)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习进行垃圾邮件检测：[https://github.com/Meenapintu/Spam-Detection](https://github.com/Meenapintu/Spam-Detection)
- en: 'Deep learning with Python: [https://www.manning.com/books/deep-learning-with-python](https://www.manning.com/books/deep-learning-with-python)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python深度学习：[https://www.manning.com/books/deep-learning-with-python](https://www.manning.com/books/deep-learning-with-python)
